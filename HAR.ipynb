{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Human Activity Recognition using Inertial sensors and Neural Networks\n",
        "\n",
        "Elia Bonetto, Filippo Rigotto. \n",
        "\n",
        "Deptartment of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_swJwzcxFYUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePk1tSoDS8ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "!ls /content/drive/My\\ Drive/hda-project\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (16,10)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D, MaxPooling1D, Dropout\n",
        "#from tensorflow.keras.layers import Conv2D, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "#logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "from dataset import ars_ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxB85blVHppF",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocessing"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGNOXPGZcqfc",
        "colab_type": "text"
      },
      "source": [
        "### Load dataset\n",
        "\n",
        "Note that `num_classes = 16` instead of 17 because there are no elements assigned to the last 'TRANSIT' class."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR8-2uUvH2m8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ds = ars_ds.ARSDataset('dataset/ARS_DLR.npz')\n",
        "imu,labels = ds.get()\n",
        "\n",
        "num = imu.shape[0]\n",
        "num_classes = np.unique(labels).size\n",
        "print(\"Num classes: \" + str(num_classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqbWQsOPcu9E",
        "colab_type": "text"
      },
      "source": [
        "Temporary reassign labels (until definitive): group walking, jumping and transitions.\n",
        "\n",
        "DO NOT TRUST `get_label_encode/decode_map()` after this passage.\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gwb10lEuSAb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dmap = ds.get_label_decode_map()\n",
        "newmap = {\n",
        "    0:0, 1:1, 2:2, 3:3, 4:4, 5:5, 6:6, # unaltered\n",
        "    7:1,   8:1, # = walking\n",
        "    9:2,  10:2, 11:2, # = jumping\n",
        "    12:7, 13:7, 14:7, 15:7, 16:7 # = transitions\n",
        "}\n",
        "pprint(dmap)\n",
        "print(newmap)\n",
        "\n",
        "labels = np.array([newmap[i] for i in labels])\n",
        "num_classes = np.unique(labels).size\n",
        "print(\"Num classes: \" + str(num_classes))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKanqxbAdA1M",
        "colab_type": "text"
      },
      "source": [
        "### Train-test split\n",
        "\n",
        "`random_state` is the seed of the PRNG."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXz4oqm2UpNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(imu, labels, test_size=0.2, random_state=1)\n",
        "\n",
        "print(\"IMU shape:     \" + str(imu.shape))\n",
        "print(\"Labels shape:  \" + str(labels.shape))\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(Y_test.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sZUwwNG2sYBg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_datasets(X_train, X_test, Y_train, Y_test, batch_size):\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((X_train, Y_train))\n",
        "    train_dataset = train_dataset.shuffle(len(X_train))\n",
        "    #train_dataset = train_dataset.repeat()\n",
        "    train_dataset = train_dataset.map(lambda data, label: (tf.expand_dims(data, 1), label))\n",
        "    train_dataset = train_dataset.batch(batch_size=batch_size)\n",
        "    train_dataset = train_dataset.prefetch(buffer_size=1)\n",
        "\n",
        "    test_dataset = tf.data.Dataset.from_tensor_slices((X_test, Y_test))\n",
        "    #test_dataset = test_dataset.repeat()\n",
        "    test_dataset = test_dataset.map(lambda data, label: (tf.expand_dims(data, 1), label))\n",
        "    test_dataset = test_dataset.batch(batch_size=batch_size)\n",
        "    test_dataset = test_dataset.prefetch(buffer_size=1)\n",
        "    return train_dataset, test_dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of3HRFQtHtoO",
        "colab_type": "text"
      },
      "source": [
        "## Models"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KJxIvGs7GucF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv1model(input_shape, num_classes):\n",
        "    # Define the input placeholder as a tensor with shape input_shape. Think of this as your input image!\n",
        "    X_input = Input(shape=input_shape)\n",
        "\n",
        "    # Zero-Padding: pads the border of X_input with zeroes\n",
        "    # X = ZeroPadding2D((3, 3))(X_input)\n",
        "\n",
        "    # CONV -> Batch Normalization -> ReLU Block applied to X\n",
        "    X = Conv1D(32, 5, name='conv0')(X_input)\n",
        "    X = BatchNormalization(axis=1, name='bn0')(X)\n",
        "    X = Activation('relu')(X)\n",
        "\n",
        "    # MAXPOOL\n",
        "    X = MaxPooling1D(2, name='max_pool0')(X)\n",
        "\n",
        "    # FLATTEN X (means convert it to a vector) + FULLYCONNECTED\n",
        "    X = Flatten()(X)\n",
        "    #X = Dense(128, activation='relu', name='fc')(X)\n",
        "    X = Dense(num_classes, activation='softmax', name='softmax')(X)\n",
        "\n",
        "    model = Model(inputs = X_input, outputs = X, name='Conv1Model')\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lxhfx_7Hw4u",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4im4NdAR-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, train_dataset, test_dataset, config):\n",
        "    \"\"\"Generic method to build a model, train and evaluate performances.\"\"\"\n",
        "\n",
        "    out_folder = os.path.join('output', datetime.now().strftime('%Y%m%d-%H%M%S')+'_'+model.name)\n",
        "    if not os.path.exists(out_folder):\n",
        "        os.mkdir(out_folder)\n",
        "    \n",
        "    # print and save model summary\n",
        "    model.summary()\n",
        "    with open(os.path.join(out_folder, 'summary.txt'),'w') as sfile:\n",
        "        model.summary(print_fn=lambda x: sfile.write(x+'\\n'))\n",
        "\n",
        "    # use and save config\n",
        "    with open(os.path.join(out_folder, 'config.json'),'w') as cfile:\n",
        "        json.dump(config, cfile, indent=2)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=config['optimizer'], loss=config['loss'], metrics=['accuracy'])\n",
        "\n",
        "    # train model, save final state and history\n",
        "    history = model.fit(train_dataset, epochs=config['epochs'], validation_data=test_dataset)\n",
        "    model.save(os.path.join(out_folder, 'model.h5'))\n",
        "    with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        json.dump(history.history, hfile, indent=2)\n",
        "\n",
        "    # evaluate model, save results\n",
        "    preds = model.evaluate(test_dataset)\n",
        "    print (\"Loss = \" + str(preds[0]))\n",
        "    print (\"Test Accuracy = \" + str(preds[1]) + \" = \" + str(preds[1]*100))\n",
        "    with open(os.path.join(out_folder, 'evaluation.json'),'w') as efile:\n",
        "        json.dump({'loss':preds[0], 'accuracy':preds[1]}, efile, indent=2)\n",
        "\n",
        "    # plot and save loss and accuracy\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Train loss')\n",
        "    plt.plot(history.history['val_loss'], label='Val loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-loss')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='Train loss')\n",
        "    plt.plot(history.history['val_acc'], label='Val loss')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuzqoxhheZu",
        "colab_type": "text"
      },
      "source": [
        "Define here models and config files to test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2FY72HB7uzp8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 32\n",
        "train_dataset, test_dataset = create_datasets(X_train, X_test, Y_train, Y_test, batch_size)\n",
        "\n",
        "input_shape = X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKaBl4gUvIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 1,\n",
        "    'batch_size': batch_size, # inferred from dataset\n",
        "    'batch_per_epoch': int(np.ceil(len(X_train)/batch_size)) # use only with repeat() in dataset\n",
        "}\n",
        "model = conv1model(input_shape, num_classes)\n",
        "run_model(model, train_dataset, test_dataset, config)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
