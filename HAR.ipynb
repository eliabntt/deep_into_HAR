{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Going deep into Human Activity Recognition\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 2 - Training of DL models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vo-eQWFE-aB",
        "colab_type": "text"
      },
      "source": [
        "TEST TO DO:\n",
        "- augmented\n",
        "- not-normalized\n",
        "\n",
        "TO DO:\n",
        "- plot f1 vs cum probability (see hammerla paper)\n",
        "- test various loss functions/lr/decay\n",
        "- lr finder [link](https://medium.com/octavian-ai/how-to-use-the-learning-rate-finder-in-tensorflow-126210de9489)\n",
        "\n",
        "NETWORKS:\n",
        "\n",
        "DONE:\n",
        "- FFNN (2,3 layers, with/without L2, with/without dropout)\n",
        "- CNN (2,3 layers, with/without L2, with/without dropout, paper networks)\n",
        "- LSTM (2 layers)\n",
        "- GRU (1 layer)\n",
        "\n",
        "DOING / to test:\n",
        "- CNN - test epoch\n",
        "- LSTM - test lr, dropout\n",
        "- RNN - test lr, dropout\n",
        "- CNN + LSTM\n",
        "- AE CNN/LSTM\n",
        "\n",
        "TO DO:\n",
        "- AE as over with previous models as classifier\n",
        "\n",
        "AUGMENTATION:\n",
        "- random shuffle"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_swJwzcxFYUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi | grep T4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install telepot\n",
        "clear_output()\n",
        "\n",
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,6)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, LSTM, GRU\n",
        "from tensorflow.keras.layers import TimeDistributed, RepeatVector, UpSampling1D, UpSampling2D, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau, LambdaCallback\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tg_callback import TelegramCallback\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxB85blVHppF",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Start from previously preprocessed data, altrady splitted in train and test parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD72ldeCpHVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_decode = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling'\n",
        "}\n",
        "num_classes = len(map_decode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJYO52hC3cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-train-test-body-framed-aug-rot-per-norm.h5','r') as h5f:\n",
        "    X_train = h5f['X_train'][:] # IMU data w.r.t body frame\n",
        "    X_test  = h5f['X_test'][:]  # activities (labels)\n",
        "    Y_train = h5f['Y_train'][:]\n",
        "    Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "num_data = len(X_train)\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "# categorical structures are needed for the loss function to work properly\n",
        "# original test classes are needed for prediction steps\n",
        "Y_train_orig = Y_train.copy()\n",
        "Y_test_orig  = Y_test.copy()\n",
        "Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lxhfx_7Hw4u",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation\n",
        "\n",
        "Precision, recall and F1 score are implemented referring to Tensorflow backend and are passed as custom metrics to track during training and evaluation of models.\n",
        "\n",
        "We further define three metrics to save best models:\n",
        "\n",
        "$sm1 = accuracy+precision+recall \\quad sm2 = \\frac{accuracy+precision+recall}{loss} \\quad sm3 = \\frac{accuracy}{loss}$\n",
        "\n",
        "where $accuracy$ is `keras.metrics.categorical_accuracy` and $loss$ is `keras.losses.categorical_crossentropy`.\n",
        "\n",
        "The `run_model` function takes care of bootstrap, training and evaluation processes for a given Keras model and configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpa4dexi1Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"F1 score, based on precision and recall metrics.\"\"\"\n",
        "    prc = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prc*rec)/(prc+rec+K.epsilon()))\n",
        "\n",
        "# -----\n",
        "\n",
        "def sum_metric_1(y_true, y_pred):\n",
        "    # accuracy+precision+recall\n",
        "    accu = K.cast(K.equal(K.argmax(y_true, axis=-1),\n",
        "                          K.argmax(y_pred, axis=-1)),\n",
        "                  K.floatx()) # from Keras source code\n",
        "    prc = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return accu + prc + rec\n",
        "\n",
        "def sum_metric_2(y_true, y_pred):\n",
        "    apr = sum_metric_1(y_true, y_pred)\n",
        "    loss = K.mean(K.categorical_crossentropy(y_true, y_pred))\n",
        "    return apr / loss\n",
        "\n",
        "def sum_metric_3(y_true, y_pred):\n",
        "    accu = K.cast(K.equal(K.argmax(y_true, axis=-1),\n",
        "                          K.argmax(y_pred, axis=-1)), K.floatx())\n",
        "    loss = K.mean(K.categorical_crossentropy(y_true, y_pred))\n",
        "    return accu / loss\n",
        "    \n",
        "def save_high_acc_low_loss(epoch,logs):\n",
        "    global output_dir\n",
        "    global max_accu\n",
        "    global min_loss\n",
        "        \n",
        "    vaccu = logs['val_acc']\n",
        "    vloss = logs['val_loss']\n",
        "    \n",
        "    if vaccu < max_accu and vaccu > max_accu - 0.01 and vloss < min_loss:\n",
        "        model.save(os.path.join(out_folder, 'model-bestal.h5'))\n",
        "    \n",
        "    if vaccu > max_accu:\n",
        "        max_accu = vaccu\n",
        "    if vloss < min_loss:\n",
        "        min_loss = vloss\n",
        "\n",
        "# -----\n",
        "\n",
        "def per_class_accuracy(y_true, y_preds, class_labels):\n",
        "    # for reference. confusion matrix diag is used instead\n",
        "    return [np.mean([\n",
        "            (y_true[pred_idx] == np.round(y_pred)) \n",
        "                for pred_idx, y_pred in enumerate(y_preds) \n",
        "                    if y_true[pred_idx] == int(class_label)\n",
        "        ]) for class_label in class_labels]\n",
        "\n",
        "def halfLRafterEpoch(epoch):\n",
        "    # for reference. lambda func is used instead\n",
        "    initial_lrate = 0.1\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    return initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4im4NdAR-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, config, \n",
        "              x_train = X_train, y_train = Y_train, \n",
        "              x_test = X_test, y_test = Y_test, y_test_orig = Y_test_orig):\n",
        "    \"\"\"Generic method to build a model, train and evaluate performances.\"\"\"\n",
        "\n",
        "    global out_folder\n",
        "    out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_'+model.name)\n",
        "    if not os.path.exists(out_folder):\n",
        "        os.mkdir(out_folder)\n",
        "    \n",
        "    # print and save model summary\n",
        "    print('Summary')\n",
        "    model.summary(line_length=100)\n",
        "    with open(os.path.join(out_folder, 'summary.txt'),'w') as sfile:\n",
        "        model.summary(line_length=100, print_fn=lambda x: sfile.write(x+'\\n'))\n",
        "    plot_model(model, to_file=os.path.join(out_folder, 'model.png'), show_shapes=True)\n",
        "\n",
        "    # save config\n",
        "    with open(os.path.join(out_folder, 'config.json'),'w') as cfile:\n",
        "        json.dump(config, cfile, indent=2)\n",
        "\n",
        "\n",
        "    if 'lr' not in config:\n",
        "        # use default values: ass the string\n",
        "        opt = config['optimizer']\n",
        "    else:\n",
        "        # setup optimizer with supplied parameters\n",
        "        if 'sgdm' in config['optimizer']:\n",
        "            opt = SGD(lr=config['lr'], momentum=config['momentum'], decay=config['decay'])\n",
        "\n",
        "        elif 'sgd' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = SGD(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = SGD(lr=config['lr'])\n",
        "\n",
        "        elif 'adam' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = Adam(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = Adam(lr=config['lr'])\n",
        "\n",
        "        elif 'rmsprop' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = RMSprop(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = RMSprop(lr=config['lr'])\n",
        "\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=opt,\n",
        "                  loss=config['loss'],\n",
        "                  metrics=['accuracy', precision, recall, f1, sum_metric_1, sum_metric_2, sum_metric_3])\n",
        "\n",
        "    # add requested callbacks for model, starting from checkpointing\n",
        "    callbacks = [\n",
        "        ModelCheckpoint(os.path.join(out_folder, 'model-best.h5'), \n",
        "                        monitor='val_acc', mode='max', save_best_only=True, verbose=0),\n",
        "        ModelCheckpoint(os.path.join(out_folder, 'model-bestsm1.h5'), \n",
        "                        monitor='val_sum_metric_1', mode='max', save_best_only=True, verbose=0),\n",
        "        ModelCheckpoint(os.path.join(out_folder, 'model-bestsm2.h5'), \n",
        "                        monitor='val_sum_metric_2', mode='max', save_best_only=True, verbose=0),\n",
        "        ModelCheckpoint(os.path.join(out_folder, 'model-bestsm3.h5'), \n",
        "                        monitor='val_sum_metric_3', mode='max', save_best_only=True, verbose=0),\n",
        "        LambdaCallback(on_epoch_end=save_high_acc_low_loss)\n",
        "    ]\n",
        "\n",
        "    if config['lr_step'] > 0:\n",
        "        # halves lr every lr_step epochs (starting lr = 0.01)\n",
        "        callbacks.append(LearningRateScheduler(\n",
        "            lambda epoch: 0.01 * math.pow(0.5, math.floor((1+epoch)/config['lr_step'])),\n",
        "            verbose=1))\n",
        "        \n",
        "    if config['early_stop'] > 0:\n",
        "        # stop if val_loss does has not diminished after num epochs\n",
        "        callbacks.append(EarlyStopping(patience=config['early_stop']))\n",
        "        \n",
        "    if config['tg']:\n",
        "        # telegram notification when training stops\n",
        "        callbacks.append(TelegramCallback(name=model.name))\n",
        "    \n",
        "    # train model, save final state and history\n",
        "    print('\\nTraining')\n",
        "\n",
        "    # by default, use test set also as validation set\n",
        "    x_val = x_test\n",
        "    y_val = y_test    \n",
        "    if 'use_validation' in config and config['use_validation']:\n",
        "        # generate validation set excluding it from the training set\n",
        "        x_train, x_val, y_train, y_val = \\\n",
        "            train_test_split(x_train, y_train, test_size=0.2, random_state=1, stratify=y_train)     \n",
        "    \n",
        "    # global variables to save model in LambdaCallback\n",
        "    global max_accu\n",
        "    global min_loss\n",
        "    max_accu = 0.0\n",
        "    min_loss = 1e10\n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        shuffle=config['shuffle'],\n",
        "                        epochs=config['epochs'],\n",
        "                        batch_size=config['batch_size'],\n",
        "                        callbacks=callbacks, # if len(callbacks) > 0 else None,\n",
        "                        validation_data=(x_val,y_val))\n",
        "    \n",
        "    model.save(os.path.join(out_folder, 'model-final.h5'))\n",
        "    \n",
        "    with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        hpd = pd.DataFrame(history.history)\n",
        "        json.dump(json.loads(hpd.to_json()), hfile, indent=2)\n",
        "\n",
        "        #json.dump(history.history, hfile, indent=2)\n",
        "        # native json module can't handle float32 objects\n",
        "        # pandas can and is used as a preprocessor to json module\n",
        "\n",
        "    # plot and save loss, accuracy and metrics (precision, recall, f1)\n",
        "    print('\\nLoss, accuracy and metrics plots')\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-loss')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='Training')\n",
        "    plt.plot(history.history['val_acc'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim((0,1))\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['precision'], label='Precision Tr')\n",
        "    plt.plot(history.history['val_precision'], label='Precision Val')\n",
        "    plt.plot(history.history['recall'], label='Recall Tr')\n",
        "    plt.plot(history.history['val_recall'], label='Recall Val')\n",
        "    plt.plot(history.history['f1'], label='F1 Tr')\n",
        "    plt.plot(history.history['val_f1'], label='F1 Val')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-metrics')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "        \n",
        "    # evaluate models, save results\n",
        "    for model_suffix in ['final','best','bestsm1','bestsm2','bestsm3','bestal']:\n",
        "        model_name = os.path.join(out_folder, 'model-{}.h5'.format(model_suffix))\n",
        "        if not os.path.exists(model_name): continue\n",
        "\n",
        "        model = load_model(model_name, custom_objects={'precision': precision, 'recall': recall, 'f1': f1,\n",
        "                                       'sum_metric_1': sum_metric_1, 'sum_metric_2': sum_metric_2, 'sum_metric_3': sum_metric_3})\n",
        "        \n",
        "        print(f\"\\nEvaluation of {model_suffix}\")\n",
        "        metrics = model.evaluate(x=x_test, y=y_test)\n",
        "        metrics = dict(zip(model.metrics_names, metrics)) # build a dict adding names\n",
        "        metrics['name'] = model.name\n",
        "        metrics['type'] = model_suffix\n",
        "\n",
        "        # get predictions\n",
        "        preds = model.predict(x=x_test)\n",
        "        y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "        classes_num = list(map(str,range(num_classes))) # classes list as str integers\n",
        "        classes = list(map_decode.values())\n",
        "        metrics['classes'] = classes\n",
        "\n",
        "        # build per-class metrics and confusion matrix\n",
        "        cr = classification_report(y_test_orig, y_pred, output_dict=True)\n",
        "        \n",
        "        cm = confusion_matrix(y_test_orig, y_pred)\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalization\n",
        "\n",
        "        acc_class = [cm[i,i] for i in range(num_classes)]\n",
        "        prc_class = [cr[cl]['precision'] for cl in cr if cl in classes_num] # exclude avgs\n",
        "        rec_class = [cr[cl]['recall']    for cl in cr if cl in classes_num]\n",
        "        f1_class  = [cr[cl]['f1-score']  for cl in cr if cl in classes_num]\n",
        "\n",
        "        metrics['acc-class'] = acc_class\n",
        "        metrics['precision-class'] = prc_class\n",
        "        metrics['recall-class'] = rec_class\n",
        "        metrics['f1-class'] = f1_class\n",
        "        metrics['averages'] = cr['macro avg']\n",
        "        metrics['weighted-averages'] = cr['weighted avg']\n",
        "        del metrics['averages']['support']\n",
        "        del metrics['weighted-averages']['support']\n",
        "        print()\n",
        "        pprint(metrics)\n",
        "\n",
        "        # conversion to pure python float before saving to json\n",
        "        for item in metrics:\n",
        "            if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "                metrics[item] = float(metrics[item])\n",
        "\n",
        "        # save evaluation dict, confusion matrix and its plot\n",
        "        with open(os.path.join(out_folder, f\"evaluation-{model_suffix}.json\"),'w') as efile:\n",
        "            json.dump(metrics, efile, indent=2)\n",
        "\n",
        "        np.save(os.path.join(out_folder, f\"confusion-{model_suffix}.npy\"), cm)\n",
        "\n",
        "        plt.figure()\n",
        "        sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "        plt.xlabel('Predicted class')\n",
        "        plt.ylabel('True class')\n",
        "        plt.tight_layout()\n",
        "        fname = os.path.join(out_folder, f\"plot-confusion-{model_suffix}\")\n",
        "        plt.savefig(fname+'.png')\n",
        "        plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fursw-6uk3CN",
        "colab_type": "text"
      },
      "source": [
        "## Standard models\n",
        "\n",
        "Keras models for simple or standard architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgi0iyMk5PB",
        "colab_type": "text"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYF3P-olQPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'TwoDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zKRmgLz3nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ThreeDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'ThreeDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5hS5HSSWlfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FiveDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'FiveDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBztSmjVk-Zz",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla33q7ilHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C1D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape, \n",
        "               kernel_regularizer=l2(l2_reg) if l2_reg else None), # shape == (batch, steps, channels)\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVhjcs2btcs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C2D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape,\n",
        "              kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu',\n",
        "              kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hAIEYmtnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C1D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703_dnMjuNGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C2D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL5eOuIUmLfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_Chen_model(input_shape, num_classes):\n",
        "    \"\"\"Chen and Xue, 'A DL approach to HAR based on single accelerometer'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv1D(18, 2, activation='relu', input_shape=input_shape), # depth, kernel\n",
        "        MaxPooling1D(2), # size, strides\n",
        "        \n",
        "        Conv1D(36, 2, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(24, 2, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-Chen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whtR6N9WmMYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_Rueda_model(input_shape, num_classes):\n",
        "    \"\"\"Moya Rueda et al., 'CNN for HAR using body-worn sensors'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-Rueda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcyg_0JrNArn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv2D_Bevilacqua_model(input_shape, num_classes):\n",
        "    \"\"\"Bevilacqua et al., 'HAR with CNNs'\"\"\"\n",
        "    return Sequential([\n",
        "        ZeroPadding2D((1,2), input_shape=input_shape),\n",
        "        \n",
        "        Conv2D(10, (3,5)), # depth kernel\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((3,3),(1,1)), # size strides\n",
        "        \n",
        "        Conv2D(2, (2,4)),\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2,2),(1,1)),\n",
        "        \n",
        "        Conv2D(2, (2,2)),\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((3,2),(1,2)),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(500, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(250, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(125, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv2D-Bevilacqua')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r629ddc2V-dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv2D_Ha_model(input_shape, num_classes):\n",
        "    \"\"\"Ha, Yun and Choi, 'Multi-modal CNN for AR'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv2D(32, (4,4), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((3,3),(1,1)),\n",
        "        \n",
        "        Conv2D(64, (5,5), activation='relu'),\n",
        "        MaxPooling2D((3,3),(1,1)),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv2D-Ha')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHQCDtzlAs-",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi9Y1OAqkOKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_modelv1(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(128, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        LSTM(64, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTMv1')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ANQ47wFwkLrM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_modelv0(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(128, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.4),\n",
        "        LSTM(64, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTMv0')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0aCCNFzQAs0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(128, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(64, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuKnAMXx-i1G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_modelv2(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(256, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(128, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTMv2')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTIeu3cm9rTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_modelv4(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(256, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.1),\n",
        "        LSTM(128, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTMv4')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43fq--o6xJ9Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_modelv3(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(256, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(256, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTMv3')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbLd-UMbY9NK",
        "colab_type": "text"
      },
      "source": [
        "### GRU (TODO delete this label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t7wH7sX7qcBQ",
        "colab": {}
      },
      "source": [
        "def OneGRU_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        GRU(128, input_shape=input_shape),\n",
        "        Dropout(0.2), #prev commented\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='OneGRU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSP3kr-xtwEF",
        "colab_type": "text"
      },
      "source": [
        "## Mixed models\n",
        "\n",
        "Models composed of two or more different architecture types."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqQ95bc18VVN",
        "colab_type": "text"
      },
      "source": [
        "### CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GAKogzG2hnQu",
        "colab": {}
      },
      "source": [
        "def CNN_LSTM_model(input_shape, num_classes):\n",
        "    filters = 256 #1287\n",
        "    LSTM_feat = 128 \n",
        "    #TimeDistributed: This wrapper applies a layer to every temporal slice of an input.\n",
        "    #The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.\n",
        "    return Sequential([\n",
        "        TimeDistributed(Conv1D(filters=filters, kernel_size=1, activation='relu'), input_shape=input_shape),\n",
        "        TimeDistributed(Conv1D(filters=filters, kernel_size=3, activation='relu')),\n",
        "        TimeDistributed(Dropout(0.1)),\n",
        "        TimeDistributed(MaxPooling1D(pool_size=2)),\n",
        "        TimeDistributed(Flatten()),\n",
        "        LSTM(LSTM_feat),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name ='CNN-LSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1kj8Yx5qx9t",
        "colab_type": "text"
      },
      "source": [
        "## AutoEncoders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kyZqlfrrzur",
        "colab_type": "text"
      },
      "source": [
        "### CNN AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cButa7Rr6pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', input_shape=input_shape, padding='same'),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=num_features, kernel_size=1, activation='relu') #TODO Try with softmax\n",
        "    ], name='CNN_AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx6AtMjtbMnF",
        "colab_type": "text"
      },
      "source": [
        "### LSTM AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MHyhr9q9Fgp",
        "colab": {}
      },
      "source": [
        "def LSTM_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        LSTM(128, activation='relu', input_shape=input_shape, return_sequences=True), #TODO try with 64, 32/ 128,64\n",
        "        LSTM(64, activation='relu', return_sequences=True),\n",
        "        #LSTM(64, activation='relu', return_sequences=True),\n",
        "        LSTM(128, activation='relu', return_sequences=True),\n",
        "        #LSTM(num_features, activation='relu', return_sequences=True),\n",
        "        TimeDistributed(Dense(num_features, activation='relu')) #TODO Try with softmax\n",
        "    ], name='LSTM-AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T93ulpiz1pM_",
        "colab_type": "text"
      },
      "source": [
        "### CNN + LSTM AutoEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJkAojhi1rvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN_LSTM_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        MaxPooling1D(pool_size=1),\n",
        "        LSTM(128, activation='relu', return_sequences=True),\n",
        "        TimeDistributed(Dense(num_features, activation='relu')) #TODO Try with softmax\n",
        "    ], name='CNN-LSTM-AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdruFPO0bSQH",
        "colab_type": "text"
      },
      "source": [
        "### SVM tests to be moved below or out\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wko6nGeyCxfm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DL_input = Input(input_shape)\n",
        "DL_model = DL_input\n",
        "for layer in model.layers[:3]:\n",
        "    DL_model = layer(DL_model)\n",
        "DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "DL_model.summary()\n",
        "for layer in DL_model.layers:\n",
        "    layer.trainable = False\n",
        "DL_model.summary()\n",
        "\n",
        "#DL_model.get_weights()[0]\n",
        "#model.get_weights()[0]\n",
        "\n",
        "data = DL_model.predict(x_tr, verbose = 1)\n",
        "data = data.reshape(data.shape[0],data.shape[2])\n",
        "\n",
        "flattened_y = np.repeat(Y_train_orig, 128, axis=0)\n",
        "print(flattened_y.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLaRGq9OPJiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# still to test\n",
        "from sklearn import datasets, svm\n",
        "from sklearn.kernel_approximation import Nystroem\n",
        "clf = svm.LinearSVC(verbose=True)\n",
        "feature_map_nystroem = Nystroem(gamma=.2,\n",
        "                                random_state=1,\n",
        "                                n_components=3)\n",
        "data_transformed = feature_map_nystroem.fit_transform(data)\n",
        "print(data_transformed.shape)\n",
        "clf.fit(data_transformed, flattened_y)\n",
        "\n",
        "clf.score(data_transformed, flattened_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0515VyZShYr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "b = data.as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwsXnnBaRunU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import svm\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "#TO BE TRIED\n",
        "classifier=svm.SVC() #or LinearSVC\n",
        "\n",
        "parameters=[{'kernel': ['rbf'], 'gamma': [0.001, 0.0001], 'C': [1, 10, 100, 1000]}, {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
        "\n",
        "modelCV=GridSearchCV(classifier,parameters,n_jobs=-1,cv=4,verbose=4)\n",
        "modelCV.fit(data,flattened_y)\n",
        "\n",
        "\n",
        "# STILL TO BE ADAPTED\n",
        "from sklearn.metrics import accuracy_score\n",
        "ypred=model.predict(xtest)\n",
        "accuracy=accuracy_score(ytest,ypred)\n",
        "print('Best Parameters: '+ str(modelCV.best_params_))\n",
        "print('Accuracy Score: '+ str(accuracy*100) + ' %')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9h3ylBSbUHz",
        "colab_type": "text"
      },
      "source": [
        "## TODO ENCODER + PREVIOUS CLASSIFICATION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuzqoxhheZu",
        "colab_type": "text"
      },
      "source": [
        "## Tests\n",
        "\n",
        "Here models are trained according to selected configuration on the dataset split.\n",
        "\n",
        "The configuration is a dictionary that allow to set the model's parameters and callbacks:\n",
        "\n",
        "- `optimizer` (str): the selected optimizer for training. One of [`sgd`, `sgdm`, `adam`, `rmsprop`].\n",
        "- `lr` (float): the learning rate. If omitted, standard values are used for the optimizer.\n",
        "- `decay` (float): learning rate decay. Valid for all optimizers with a supplied `lr`.\n",
        "- `momentum` (float): gradient momentum. Only valid for `sgdm` optimizer.\n",
        "- `loss` (str): type of loss to minimize.\n",
        "\n",
        "- `epochs` (int) and `batch_size` (int).\n",
        "- `use_validation` (bool): whether to derive validation set from training set instead of using test set to validate training.\n",
        "- `shuffle` (bool): whether to shuffle data in batches or keep the same retrieval order.\n",
        "\n",
        "- `lr_step` (int): epochs after which the learning rate is halved. Set to 0 to disable.\n",
        "- `early_stop` (int): number of epochs after which to stop training if validation loss does not decrease anymore. Set to 0 to disable.\n",
        "- `tg` (bool): whether to enable Telegram notification when training finishes.\n",
        "\n",
        "Models that contain Dense or convolutional layers may use L2 regularization with the optional parameter `l2_reg` to the model function.\n",
        "\n",
        "Some models may apply dropout if `dropout_rate` is set, some have it enabled by default as part of the network structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSTL8Vn9WMX",
        "colab_type": "text"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1NUZsSC017",
        "colab_type": "text"
      },
      "source": [
        "Using two dense layers leads to variable accuracy in the range [72.5%, 87.5%] with growing loss if regularization is not applied.\n",
        "\n",
        "Regularization and dropout lower accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K68uw52ZI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = TwoDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 60,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbODHqzsFeyU",
        "colab_type": "text"
      },
      "source": [
        "Adding one dense layer, there is no substantial changes: regularization stabilizes validation loss but lowers the accuracy by 5% from 90% to 85%.\n",
        "\n",
        "Dropout is not effective in bounding loss and has final accuracy similar to using regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAj2q2r9mzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = ThreeDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 30,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugRm6bzMbXuD",
        "colab_type": "text"
      },
      "source": [
        "Using five dense layers the network is too deep for the assigned task and it does not match interesting results.\n",
        "\n",
        "Network reaches 90% accuracy with visible overfit, and applying dropout or regularization lowers accuracy to 85 and 75% resp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDlvJqCXuE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = FiveDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 300,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXWiR979ZqM",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1lpUevlAqBy",
        "colab_type": "text"
      },
      "source": [
        "Using 1 convolutional and 1 dense layer we obtain poor results: overfitting and growing validation loss.\n",
        "\n",
        "Accuracy between 85 and 90%. Regularization helps but not too much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKaBl4gUvIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWRhxuw_BzOm",
        "colab_type": "text"
      },
      "source": [
        "Using 1 conv. and 2 dense layers, we achieve 95% accuracy. Regularization controls loss growing but lowers accuracy to 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpeEIf2y6mac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv6cqQMJEIJk",
        "colab_type": "text"
      },
      "source": [
        "Two conv. layers and one final dense layer: of the four configuration, applying only dropout is the best setup and leads to 96% accuracy.\n",
        "\n",
        "Adding regularization is not helpful. Standard model reaches 94% accuracy with growing loss.\n",
        "\n",
        "_Selected as one of the best CNN model for the paper._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJWTb8a8qBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "# selected for report\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4JGMlTQJi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this contains only the selected configuration\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19m6MYe6Qzzs",
        "colab_type": "text"
      },
      "source": [
        "Two conv. layers and two dense layers: same conclusions as in previous setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6l8wwe-Jn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8XrykLTCen",
        "colab_type": "text"
      },
      "source": [
        "Model from Chen's paper: uses only accelerometer data and overfits after 150 epochs, achieving no more than 75% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bz3lTkajWpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150, # tuned\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "X_tr = X_train.copy()\n",
        "X_tr = X_tr[:,:,:3] # only take accelerometer data\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = X_te[:,:,:3]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_tr.shape[1], X_tr.shape[2])\n",
        "model = Conv1D_Chen_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHB5IimIT6ih",
        "colab_type": "text"
      },
      "source": [
        "Model from Moya-Rueda's paper: very bad performance (no more than 65% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtprVlP-puVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    #'optimizer': 'adam',\n",
        "    'optimizer': 'rmsprop',\n",
        "    'lr': 0.01,\n",
        "    'decay': 0.95,\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 100,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 25,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_Rueda_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEdaYoZUsy4",
        "colab_type": "text"
      },
      "source": [
        "Model from Bevilacqua's paper: achieves $\\approx$ 86% accuracy using 2D convolutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om1h7FO7dfhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = X_train.copy()\n",
        "X_tr = np.swapaxes(X_tr,1,2)\n",
        "X_tr = X_tr[...,None] # add last dimension\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = np.swapaxes(X_te,1,2)\n",
        "X_te = X_te[...,None]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_train.shape[2], X_train.shape[1], 1)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150, # tuned. visible overfit if continuing\n",
        "    'batch_size': 1024,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Bevilacqua_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200, # tuned. stable\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Bevilacqua_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1mSywdVWUV",
        "colab_type": "text"
      },
      "source": [
        "Model from Ha's paper: achieves $\\approx$ 95% accuracy padding columns to separate signals data from different sensors.\n",
        "\n",
        "_Best CNN model, selected for paper._ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEgx6oAXjYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = X_train.copy()\n",
        "X_tr = np.swapaxes(X_tr,1,2)\n",
        "# pad three 'cols' of zeros between each sensor data\n",
        "# > ax ay az 0 0 0 gx gy gz 0 0 0 mx my mz \n",
        "for index in [9,6,3]:\n",
        "    for rep in range(3):\n",
        "        X_tr = np.insert(X_tr, index, 0, axis=1)\n",
        "X_tr = X_tr[...,None] # add dim\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = np.swapaxes(X_te,1,2)\n",
        "for index in [9,6,3]:\n",
        "    for rep in range(3):\n",
        "        X_te = np.insert(X_te, index, 0, axis=1)\n",
        "X_te = X_te[...,None]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_tr.shape[1], X_tr.shape[2],1)\n",
        "\n",
        "\"\"\"\n",
        "# no validation version\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100, # tuned. slightly overfitting but stable val. accuracy\n",
        "    'batch_size': 1024,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Ha_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te) # TUNED\n",
        "\"\"\"\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200, # tuned. slightly overfitting but stable val. accuracy\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Ha_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sYZFUn9cJx",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "es-1YYhBkgEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_modelv1(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvjkROn3khcn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_modelv0(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXvRjzsTmpRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZhuP6-1FZx7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_modelv2(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiOXyFBpySkP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_modelv3(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nqgT8HXD9wqd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_modelv4(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42xHqw0CkkWS",
        "colab_type": "text"
      },
      "source": [
        "### GRU (TODO delete this label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp-G0Bw3bIU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drop 0.2 softmax - bs 100 (vs 200)\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 250,\n",
        "    'batch_size': 100,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[-1])\n",
        "model = OneGRU_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_giGbFS-yFJ",
        "colab_type": "text"
      },
      "source": [
        "### CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ilyM0LMRh8W8",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False,\n",
        "}\n",
        "\n",
        "folds, fold_len = 4, 32 # 4*32 = 128\n",
        "\n",
        "input_shape = (folds, fold_len, X_train.shape[-1])\n",
        "model = CNN_LSTM_model(input_shape, num_classes)\n",
        "run_model(model, config,\n",
        "          x_train = X_train.reshape(X_train.shape[0], folds, fold_len, X_train.shape[-1]),\n",
        "          x_test = X_test.reshape(X_test.shape[0], folds, fold_len, X_train.shape[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RaWnK24XcpA5",
        "colab_type": "text"
      },
      "source": [
        "## TODO provare con reshape [0], 128*9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzS5YvFWsmPF",
        "colab_type": "text"
      },
      "source": [
        "### CNN AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tRvYKRfspwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 20,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "x_tr = x_tr.reshape(-1,1,9)\n",
        "x_te = x_te.reshape(-1,1,9)\n",
        "y_tr = y_tr.reshape(-1,1,9)\n",
        "y_te = x_te.reshape(-1,1,9)\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = CNN_AE_model(input_shape, X_train.shape[2])\n",
        "run_model(model, config, x_train=x_tr,  y_train=y_tr, x_test=x_te, y_test=y_te, y_test_orig = x_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teKbzKvWdo3c",
        "colab_type": "text"
      },
      "source": [
        "### LSTM AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNUc0ZdC9La6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 20,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "x_tr = x_tr.reshape(-1,1,9)\n",
        "x_te = x_te.reshape(-1,1,9)\n",
        "y_tr = y_tr.reshape(-1,1,9)\n",
        "y_te = x_te.reshape(-1,1,9)\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = LSTM_AE_model(input_shape, X_train.shape[2])\n",
        "run_model(model, config, x_train=x_tr,  y_train=y_tr, x_test=x_te, y_test=y_te, y_test_orig = x_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEzpxCYN5IaJ",
        "colab_type": "text"
      },
      "source": [
        "### CNN + LSTM AutoEncoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwrncMI213a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 20,\n",
        "    'batch_size': 100,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], X_train.shape[2]))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], X_test.shape[2]))\n",
        "x_tr = x_tr.reshape(-1,1,9)\n",
        "x_te = x_te.reshape(-1,1,9)\n",
        "y_tr = y_tr.reshape(-1,1,9)\n",
        "y_te = x_te.reshape(-1,1,9)\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = CNN_LSTM_AE_model(input_shape, X_train.shape[2])\n",
        "run_model(model, config, x_train=x_tr,  y_train=y_tr, x_test=x_te, y_test=y_te, y_test_orig = x_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX4VjdNYYTtk",
        "colab_type": "text"
      },
      "source": [
        "## Tests in pure Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1AW3gZsiW2",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvv75Z2iY-q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model definition\n",
        "\n",
        "features = 32 # number of hidden layer's features\n",
        "\n",
        "#batch = 1500 # TODO unused vars\n",
        "#n_iters = 300\n",
        "#tot_iters = Y_train.shape[0] * n_iters\n",
        "#disp_iter = 1000\n",
        "\n",
        "w = {\n",
        "    'h' : tf.Variable(tf.random_normal([X_train.shape[2], features])),\n",
        "    'o' : tf.Variable(tf.random_normal([features, Y_train.shape[1]], mean=1.0))\n",
        "}\n",
        "b = {\n",
        "    'h' : tf.Variable(tf.random_normal([features])),\n",
        "    'o' : tf.Variable(tf.random_normal([Y_train.shape[1]]))\n",
        "}\n",
        "\n",
        "def LSTM(X, w, b):\n",
        "    # input processing\n",
        "    X = tf.transpose(X,[1,0,2])         # (batch_size, steps, input)\n",
        "    X = tf.reshape(X, [-1, X.shape[2]]) # (steps*batch, n_initial_\"features\")\n",
        "\n",
        "    X = tf.nn.relu(tf.matmul(X, w['h']) + b['h'])\n",
        "    X = tf.split(X, X_train.shape[1])\n",
        "    \n",
        "    # model\n",
        "    l_1 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)\n",
        "    l_2 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)    \n",
        "    lstm = tf.contrib.rnn.MultiRNNCell([l_1,l_2], state_is_tuple=True)    \n",
        "    \n",
        "    # output\n",
        "    out, state = tf.contrib.rnn.static_rnn(lstm, X, dtype=tf.float32)\n",
        "    \n",
        "    return tf.matmul(out[-1], w['o']) + b['o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqoOJlQZAjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a dataset object on input\n",
        "ds_obj = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32), Y_train.astype(np.float32))).repeat().batch(300)\n",
        "iter = ds_obj.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "prediction = LSTM(x, w, b)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y,1)),tf.float32))\n",
        "\n",
        "# losses, optimizer\n",
        "lr = 0.0025\n",
        "lambda_l = 0.0015\n",
        "\n",
        "l2_norm = lambda_l * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "softmax_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) + l2_norm\n",
        "adam = tf.train.AdamOptimizer(learning_rate=lr).minimize(softmax_cost)\n",
        "\n",
        "# run training\n",
        "test_log  = {'loss':[], 'acc':[]}\n",
        "train_log = {'loss':[], 'acc':[]}\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(1000): #epochs\n",
        "        _, l, a = sess.run([adam, softmax_cost, accuracy])\n",
        "        train_log['loss'].append(l)\n",
        "        train_log['acc'].append(a)\n",
        "        \n",
        "        l,a = sess.run([softmax_cost, accuracy], feed_dict={x:X_test.astype(np.float32), y:Y_test.astype(np.float32)})\n",
        "        test_log['loss'].append(l)\n",
        "        test_log['acc'].append(a)\n",
        "        #print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "        #      \"Batch Loss = {}\".format(l) + \\\n",
        "        #      \", Accuracy = {}\".format(a))\n",
        "print('Reached {}'.format(max(test_log['acc'])))\n",
        "\n",
        "# save stuff and plots\n",
        "out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_LSTM-TF')\n",
        "if not os.path.exists(out_folder):\n",
        "    os.mkdir(out_folder)\n",
        "\n",
        "with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "    json.dump({'training':train_log, 'validation':test_log}, hfile, indent=2)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['loss'], label='Training')\n",
        "plt.plot( test_log['loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-loss')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['acc'], label='Training')\n",
        "plt.plot( test_log['acc'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
