{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Human Activity Recognition using Inertial sensors and Neural Networks\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 2 - DL models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vo-eQWFE-aB",
        "colab_type": "text"
      },
      "source": [
        "TEST TO DO:\n",
        "- augmented\n",
        "- 70/30\n",
        "- not-normalized\n",
        "\n",
        "TO DO:\n",
        "- bound on plot's y\n",
        "- test various loss functions/lr/decay\n",
        "- lr finder [link](https://medium.com/octavian-ai/how-to-use-the-learning-rate-finder-in-tensorflow-126210de9489)\n",
        "\n",
        "NETWORKS:\n",
        "\n",
        "DONE:\n",
        "- FFNN (2,3 layers, with/without L2, with/without dropout)\n",
        "- CNN (2,3 layers, with/without L2, with/without dropout)\n",
        "- LSTM (2 layers)\n",
        "- GRU (1 layer)\n",
        "\n",
        "DOING:\n",
        "- more CNNs\n",
        "- LSTM\n",
        "- RNN\n",
        "\n",
        "TO DO:\n",
        "- CNN + LSTM\n",
        "- RF???\n",
        "- AE\n",
        "- Well known networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_swJwzcxFYUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install telepot\n",
        "clear_output()\n",
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,6)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, GRU\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tg_callback import TelegramCallback\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxB85blVHppF",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Start from previously preprocessed data, altrady splitted in train and test parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD72ldeCpHVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_decode = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling'\n",
        "}\n",
        "num_classes = len(map_decode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJYO52hC3cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-train-test-body-framed-aug-norm.h5','r') as h5f:\n",
        "    X_train = h5f['X_train'][:] # IMU data w.r.t body frame\n",
        "    X_test  = h5f['X_test'][:]  # activities (labels)\n",
        "    Y_train = h5f['Y_train'][:]\n",
        "    Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "num_data = len(X_train)\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "# categorical structures are needed for the loss function to work properly\n",
        "# original test classes are needed for prediction steps\n",
        "Y_test_orig  = Y_test.copy()\n",
        "Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lxhfx_7Hw4u",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation\n",
        "\n",
        "Precision, recall and F1 score are implemented in Tensorflow and are passed as metrics to track during training and evaluation of models.\n",
        "\n",
        "The `run_model` function takes care of bootstrap, training and evaluation processes for a given Keras model and configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpa4dexi1Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"F1 score, based on precision and recall metrics.\"\"\"\n",
        "    prc = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prc*rec)/(prc+rec+K.epsilon()))\n",
        "\n",
        "def per_class_accuracy(y_true, y_preds, class_labels):\n",
        "    # for reference. confusion matrix diag is used instead\n",
        "    return [np.mean([\n",
        "            (y_true[pred_idx] == np.round(y_pred)) \n",
        "                for pred_idx, y_pred in enumerate(y_preds) \n",
        "                    if y_true[pred_idx] == int(class_label)\n",
        "        ]) for class_label in class_labels]\n",
        "\n",
        "def halfLRafterEpoch(epoch):\n",
        "    # for reference. lambda func is used instead\n",
        "    initial_lrate = 0.1\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    return initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4im4NdAR-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, config):\n",
        "    \"\"\"Generic method to build a model, train and evaluate performances.\"\"\"\n",
        "\n",
        "    out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_'+model.name)\n",
        "    if not os.path.exists(out_folder):\n",
        "        os.mkdir(out_folder)\n",
        "    \n",
        "    # print and save model summary\n",
        "    print('Summary')\n",
        "    model.summary()\n",
        "    with open(os.path.join(out_folder, 'summary.txt'),'w') as sfile:\n",
        "        model.summary(print_fn=lambda x: sfile.write(x+'\\n'))\n",
        "    plot_model(model, to_file=os.path.join(out_folder, 'model.png'), show_shapes=True)\n",
        "\n",
        "    # save config\n",
        "    with open(os.path.join(out_folder, 'config.json'),'w') as cfile:\n",
        "        json.dump(config, cfile, indent=2)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=config['optimizer'],\n",
        "                  loss=config['loss'],\n",
        "                  metrics=['accuracy', precision, recall, f1])\n",
        "\n",
        "    # add requested callbacks for model\n",
        "    callbacks = []\n",
        "    \n",
        "    # ModelCheckpoint may go here\n",
        "    \n",
        "    if config['lr_step'] > 0:\n",
        "        # halves lr every lr_step epochs (default lr = 0.1)\n",
        "        callbacks.append(LearningRateScheduler(\n",
        "            lambda epoch: 0.1 * math.pow(0.5, math.floor((1+epoch)/config['lr_step'])),\n",
        "            verbose=1))\n",
        "        \n",
        "    if config['early_stop'] > 0:\n",
        "        # stop if val_loss does has not diminished after num epochs\n",
        "        callbacks.append(EarlyStopping(patience=config['early_stop']))\n",
        "        \n",
        "    if config['tg']:\n",
        "        # telegram notification when training stops\n",
        "        callbacks.append(TelegramCallback(name=model.name))\n",
        "    \n",
        "    # train model, save final state and history\n",
        "    print('\\nTraining')\n",
        "    history = model.fit(x=X_train, y=Y_train,\n",
        "                        shuffle=config['shuffle'],\n",
        "                        epochs=config['epochs'],\n",
        "                        batch_size=config['batch_size'],\n",
        "                        callbacks=callbacks if len(callbacks) > 0 else None,\n",
        "                        validation_data=(X_test,Y_test))\n",
        "    \n",
        "    model.save(os.path.join(out_folder, 'model.h5'))\n",
        "    \n",
        "    with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        hpd = pd.DataFrame(history.history)\n",
        "        json.dump(json.loads(hpd.to_json()), hfile, indent=2)\n",
        "\n",
        "        #json.dump(history.history, hfile, indent=2)\n",
        "        # native json module can't handle float32 objects\n",
        "        # pandas can and is used as a preprocessor to json module\n",
        "    \n",
        "    # evaluate model, save results\n",
        "    print('\\nEvaluation')\n",
        "    metrics = model.evaluate(x=X_test, y=Y_test)\n",
        "    metrics = dict(zip(model.metrics_names, metrics)) # build a dict adding names\n",
        "    \n",
        "    # get predictions\n",
        "    preds = model.predict(x=X_test)\n",
        "    Y_pred = np.argmax(preds, axis=1)\n",
        "    \n",
        "    classes_num = list(map(str,range(num_classes))) # classes list as str integers\n",
        "    classes = list(map_decode.values())\n",
        "    metrics['classes'] = classes\n",
        "    \n",
        "    # build per-class metrics and confusion matrix\n",
        "    \n",
        "    cr = classification_report(Y_test_orig, Y_pred, output_dict=True)\n",
        "    \n",
        "    cm = confusion_matrix(Y_test_orig, Y_pred)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalization\n",
        "\n",
        "    acc_class = [cm[i,i] for i in range(num_classes)]\n",
        "    prc_class = [cr[cl]['precision'] for cl in cr if cl in classes_num] # exclude avgs\n",
        "    rec_class = [cr[cl]['recall']    for cl in cr if cl in classes_num]\n",
        "    f1_class  = [cr[cl]['f1-score']  for cl in cr if cl in classes_num]\n",
        "    \n",
        "    metrics['acc-class'] = acc_class\n",
        "    metrics['precision-class'] = prc_class\n",
        "    metrics['recall-class'] = rec_class\n",
        "    metrics['f1-class'] = f1_class\n",
        "    metrics['averages'] = cr['macro avg']\n",
        "    metrics['weighted-averages'] = cr['weighted avg']\n",
        "    del metrics['averages']['support']\n",
        "    del metrics['weighted-averages']['support']\n",
        "    print()\n",
        "    pprint(metrics)\n",
        "\n",
        "    # conversion to pure python float before saving to json\n",
        "    for item in metrics:\n",
        "        if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "            metrics[item] = float(metrics[item])\n",
        "            \n",
        "    with open(os.path.join(out_folder, 'evaluation.json'),'w') as efile:\n",
        "        json.dump(metrics, efile, indent=2)\n",
        "        \n",
        "    np.save(os.path.join(out_folder, 'confusion.npy'), cm)\n",
        "\n",
        "    # plot and save loss, accuracy and metrics (precision, recall, f1)\n",
        "    print('\\nLoss, accuracy, metrics and cm plots')\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-loss')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='Training')\n",
        "    plt.plot(history.history['val_acc'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(history.history['precision'], label='Precision Tr')\n",
        "    plt.plot(history.history['val_precision'], label='Precision Val')\n",
        "    plt.plot(history.history['recall'], label='Recall Tr')\n",
        "    plt.plot(history.history['val_recall'], label='Recall Val')\n",
        "    plt.plot(history.history['f1'], label='F1 Tr')\n",
        "    plt.plot(history.history['val_f1'], label='F1 Val')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-metrics')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    \n",
        "    plt.figure()\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.ylabel('True class')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-confusion')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fursw-6uk3CN",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "Here we layout the Keras models of the analyzed architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgi0iyMk5PB",
        "colab_type": "text"
      },
      "source": [
        "### Feed-forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYF3P-olQPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'TwoDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu',\n",
        "                    kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    \n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zKRmgLz3nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ThreeDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'ThreeDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu',\n",
        "                    kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    \n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "                  \n",
        "    model.add(Dense(256, activation='relu',\n",
        "                    kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    \n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "                      \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBztSmjVk-Zz",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla33q7ilHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C1D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape, \n",
        "               kernel_regularizer=l2(l2_reg) if l2_reg else None), # shape == (batch, steps, channels)\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVhjcs2btcs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C2D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape,\n",
        "              kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu',\n",
        "              kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hAIEYmtnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C1D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703_dnMjuNGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C2D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHQCDtzlAs-",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJBFATRAl-U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(32, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        LSTM(32, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIW5dDBAoPmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OneGRU_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        GRU(32, input_shape=input_shape),\n",
        "        Dense(num_classes)\n",
        "    ], name='OneGRU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuzqoxhheZu",
        "colab_type": "text"
      },
      "source": [
        "## Tests\n",
        "\n",
        "Here models are trained according to selected configuration on the dataset split.\n",
        "\n",
        "The configuration is a dictionary that allow to set the model's parameters and callbacks:\n",
        "\n",
        "- `optimizer`: the selected optimizer for training\n",
        "- `loss`: type of loss to minimize\n",
        "- `epochs` and `batch_size`\n",
        "- `shuffle`: whether to shuffle data in batches or keep the same order\n",
        "- `lr_step`: epochs after which the learning rate is halved. Set to 0 to disable.\n",
        "(UNIMPLEMENTED: set to 'auto' to reduce the lr when val loss does not decrease.)\n",
        "- `early_stop`: number of epochs after which to stop training if validation loss does not decrease anymore. Set to 0 to disable.\n",
        "- `tg`: whether to enable Telegram notification when training finishes\n",
        "\n",
        "Models that contain Dense or convolutional layers may use L2 regularization with the optional parameter `l2_reg`.\n",
        "\n",
        "Some models apply dropout if `dropout_rate` is set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSTL8Vn9WMX",
        "colab_type": "text"
      },
      "source": [
        "### Feed-forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K68uw52ZI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 10,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = TwoDense_model(input_shape, num_classes)\n",
        "#model = TwoDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "#model = TwoDense_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "#model = TwoDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAj2q2r9mzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 25,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = ThreeDense_model(input_shape, num_classes)\n",
        "#model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "#model = ThreeDense_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "#model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXWiR979ZqM",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKaBl4gUvIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes)\n",
        "#model = Conv1D_1C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpeEIf2y6mac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes)\n",
        "#model = Conv1D_1C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJWTb8a8qBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes)\n",
        "#model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "#model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "#model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6l8wwe-Jn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes)\n",
        "#model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "#model = Conv1D_2C2D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "#model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sYZFUn9cJx",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXvRjzsTmpRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb4aTEc1oYK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mae',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[-1])\n",
        "model = OneGRU_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX4VjdNYYTtk",
        "colab_type": "text"
      },
      "source": [
        "## Tests in pure Tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm1AW3gZsiW2",
        "colab_type": "text"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvv75Z2iY-q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model definition\n",
        "\n",
        "features = 32 # number of hidden layer's features\n",
        "\n",
        "#batch = 1500 # TODO unused vars\n",
        "#n_iters = 300\n",
        "#tot_iters = Y_train.shape[0] * n_iters\n",
        "#disp_iter = 1000\n",
        "\n",
        "w = {\n",
        "    'h' : tf.Variable(tf.random_normal([X_train.shape[2], features])),\n",
        "    'o' : tf.Variable(tf.random_normal([features, Y_train.shape[1]], mean=1.0))\n",
        "}\n",
        "b = {\n",
        "    'h' : tf.Variable(tf.random_normal([features])),\n",
        "    'o' : tf.Variable(tf.random_normal([Y_train.shape[1]]))\n",
        "}\n",
        "\n",
        "def LSTM(X, w, b):\n",
        "    # input processing\n",
        "    X = tf.transpose(X,[1,0,2])         # (batch_size, steps, input)\n",
        "    X = tf.reshape(X, [-1, X.shape[2]]) # (steps*batch, n_initial_\"features\")\n",
        "\n",
        "    X = tf.nn.relu(tf.matmul(X, w['h']) + b['h'])\n",
        "    X = tf.split(X, X_train.shape[1])\n",
        "    \n",
        "    # model\n",
        "    l_1 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)\n",
        "    l_2 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)    \n",
        "    lstm = tf.contrib.rnn.MultiRNNCell([l_1,l_2], state_is_tuple=True)    \n",
        "    \n",
        "    # output\n",
        "    out, state = tf.contrib.rnn.static_rnn(lstm, X, dtype=tf.float32)\n",
        "    \n",
        "    return tf.matmul(out[-1], w['o']) + b['o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqoOJlQZAjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# copy input\n",
        "X_train_2 = X_train.astype(np.float32)\n",
        "Y_train_2 = Y_train.astype(np.float32)\n",
        "X_test_2 = X_test.astype(np.float32)\n",
        "Y_test_2 = Y_test.astype(np.float32)\n",
        "\n",
        "# define a dataset object on input\n",
        "ds_obj = tf.data.Dataset.from_tensor_slices((X_train_2, Y_train_2)).repeat().batch(300)\n",
        "iter = ds_obj.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "prediction = LSTM(x, w, b)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y,1)),tf.float32))\n",
        "\n",
        "# losses, optimizer\n",
        "lr = 0.0025\n",
        "lambda_l = 0.0015\n",
        "\n",
        "l2_norm = lambda_l * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "softmax_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) + l2_norm\n",
        "adam = tf.train.AdamOptimizer(learning_rate=lr).minimize(softmax_cost)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpiaUo-VzGIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# run training\n",
        "test_log  = {'loss':[], 'acc':[]}\n",
        "train_log = {'loss':[], 'acc':[]}\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(1000): #epochs\n",
        "        _, l, a = sess.run([adam, softmax_cost, accuracy])\n",
        "        train_log['loss'].append(l)\n",
        "        train_log['acc'].append(a)\n",
        "        \n",
        "        l,a = sess.run([softmax_cost, accuracy])\n",
        "        test_log['loss'].append(l)\n",
        "        test_log['acc'].append(a)\n",
        "        #print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "        #      \"Batch Loss = {}\".format(l) + \\\n",
        "        #      \", Accuracy = {}\".format(a))\n",
        "print('Reached {}'.format(max(test_log['acc'])))\n",
        "\n",
        "# save stuff and plots\n",
        "out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_LSTM-TF')\n",
        "if not os.path.exists(out_folder):\n",
        "    os.mkdir(out_folder)\n",
        "\n",
        "with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "    json.dump({'training':train_log, 'validation':test_log}, hfile, indent=2)\n",
        "    \n",
        "plt.figure()\n",
        "plt.plot(train_log['loss'], label='Training')\n",
        "plt.plot( test_log['loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-loss')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['acc'], label='Training')\n",
        "plt.plot( test_log['acc'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
