{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Human Activity Recognition using Inertial sensors and Neural Networks\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 2 - DL models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Vo-eQWFE-aB",
        "colab_type": "text"
      },
      "source": [
        "TEST TO DO:\n",
        "- augmented\n",
        "- 70/30\n",
        "- not-normalized\n",
        "\n",
        "TO DO:\n",
        "- bound on plot's y\n",
        "- save checkpoints and resume model\n",
        "- test various loss functions/lr/decay\n",
        "- regularization in models\n",
        "\n",
        "NETWORKS:\n",
        "\n",
        "DONE:\n",
        "- FFNN (2,3 layers with/without dropout)\n",
        "- CNN (2,3 layers, with/without dropout)\n",
        "- LSTM (2 layers)\n",
        "- GRU (1 layer)\n",
        "\n",
        "DOING:\n",
        "- more CNNs\n",
        "- LSTM\n",
        "- RNN\n",
        "\n",
        "TO DO:\n",
        "- CNN + LSTM\n",
        "- RF???\n",
        "- AE\n",
        "- Well known networks\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_swJwzcxFYUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,6)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Conv1D, MaxPooling1D, Dropout\n",
        "#from tensorflow.keras.layers import Conv2D, ZeroPadding2D, AveragePooling2D, MaxPooling2D, Dropout, GlobalMaxPooling2D, GlobalAveragePooling2D\n",
        "from tensorflow.keras.layers import LSTM, GRU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxB85blVHppF",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Start from previously preprocessed data, altrady splitted in train and test parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD72ldeCpHVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_decode = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling'\n",
        "}\n",
        "num_classes = len(map_decode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJYO52hC3cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-train-test-body-framed-norm.h5','r') as h5f:\n",
        "    X_train = h5f['X_train'][:] # IMU data w.r.t body frame\n",
        "    X_test  = h5f['X_test'][:]  # activities (labels)\n",
        "    Y_train = h5f['Y_train'][:]\n",
        "    Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "num_data = len(X_train)\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "# categorical structures are needed for the loss function to work properly\n",
        "# original test classes are needed for prediction steps\n",
        "Y_test_orig  = Y_test.copy()\n",
        "Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lxhfx_7Hw4u",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation\n",
        "\n",
        "Precision, recall and F1 score are implemented in Tensorflow and are passed as metrics to track during training and evaluation of models.\n",
        "\n",
        "The `run_model` function takes care of bootstrap, training and evaluation processes for a given Keras model and configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpa4dexi1Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"F1 score, based on precision and recall metrics.\"\"\"\n",
        "    prc = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prc*rec)/(prc+rec+K.epsilon()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4im4NdAR-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, config):\n",
        "    \"\"\"Generic method to build a model, train and evaluate performances.\"\"\"\n",
        "\n",
        "    out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_'+model.name)\n",
        "    if not os.path.exists(out_folder):\n",
        "        os.mkdir(out_folder)\n",
        "    \n",
        "    # print and save model summary\n",
        "    print('Summary')\n",
        "    model.summary()\n",
        "    with open(os.path.join(out_folder, 'summary.txt'),'w') as sfile:\n",
        "        model.summary(print_fn=lambda x: sfile.write(x+'\\n'))\n",
        "\n",
        "    # save config\n",
        "    with open(os.path.join(out_folder, 'config.json'),'w') as cfile:\n",
        "        json.dump(config, cfile, indent=2)\n",
        "\n",
        "    # compile model\n",
        "    model.compile(optimizer=config['optimizer'], loss=config['loss'], metrics=['accuracy',precision,recall,f1])\n",
        "\n",
        "    # train model, save final state and history\n",
        "    print('\\nTraining')\n",
        "    history = model.fit(x=X_train, y=Y_train, shuffle=config['shuffle'],\n",
        "                        epochs=config['epochs'], batch_size=config['batch_size'], validation_data=(X_test,Y_test))\n",
        "    model.save(os.path.join(out_folder, 'model.h5'))\n",
        "    with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        hpd = pd.DataFrame(history.history)\n",
        "        json.dump(json.loads(hpd.to_json()), hfile, indent=2)\n",
        "\n",
        "        #json.dump(history.history, hfile, indent=2)\n",
        "        # native json module can't handle float32 objects\n",
        "        # pandas can and is used as a preprocessor to json module\n",
        "    \n",
        "    # evaluate model, save results\n",
        "    print('\\nEvaluation')\n",
        "    metrics = model.evaluate(x=X_test, y=Y_test)\n",
        "    metrics = dict(zip(model.metrics_names, metrics)) # build a dict adding names\n",
        "    \n",
        "    # get predictions\n",
        "    preds = model.predict(x=X_test)\n",
        "    Y_pred = np.argmax(preds, axis=1)\n",
        "    \n",
        "    classes_num = list(map(str,range(num_classes))) # classes list as str integers\n",
        "    classes = list(map_decode.values())\n",
        "    metrics['classes'] = classes\n",
        "    \n",
        "    # build per-class metrics\n",
        "    cr = classification_report(Y_test_orig, Y_pred, output_dict=True)\n",
        "\n",
        "    prc_class = [cr[cl]['precision'] for cl in cr if cl in classes_num] # exclude avgs\n",
        "    rec_class = [cr[cl]['recall']    for cl in cr if cl in classes_num]\n",
        "    f1_class  = [cr[cl]['f1-score']  for cl in cr if cl in classes_num]\n",
        "    \n",
        "    metrics['precision-class'] = prc_class\n",
        "    metrics['recall-class'] = rec_class\n",
        "    metrics['f1-class'] = f1_class\n",
        "    metrics['averages'] = cr['macro avg']\n",
        "    metrics['weighted-averages'] = cr['weighted avg']\n",
        "    del metrics['averages']['support']\n",
        "    del metrics['weighted-averages']['support']\n",
        "    print()\n",
        "    pprint(metrics)\n",
        "\n",
        "    # conversion to pure python float before saving to json\n",
        "    for item in metrics:\n",
        "        if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "            metrics[item] = float(metrics[item])\n",
        "            \n",
        "    with open(os.path.join(out_folder, 'evaluation.json'),'w') as efile:\n",
        "        json.dump(metrics, efile, indent=2)\n",
        "        \n",
        "    # build confusion matrix\n",
        "    cm = confusion_matrix(Y_test_orig, Y_pred)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalization\n",
        "    np.save(os.path.join(out_folder, 'confusion.npy'), cm)\n",
        "\n",
        "    # plot and save loss, accuracy and metrics (precision, recall, f1)\n",
        "    print('\\nLoss, accuracy, metrics and cm plots')\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-loss')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='Training')\n",
        "    plt.plot(history.history['val_acc'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.plot(history.history['precision'], label='Precision Tr')\n",
        "    plt.plot(history.history['val_precision'], label='Precision Val')\n",
        "    plt.plot(history.history['recall'], label='Recall Tr')\n",
        "    plt.plot(history.history['val_recall'], label='Recall Val')\n",
        "    plt.plot(history.history['f1'], label='F1 Tr')\n",
        "    plt.plot(history.history['val_f1'], label='F1 Val')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Metrics')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-metrics')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    \n",
        "    plt.figure()\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.ylabel('True class')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-confusion')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fursw-6uk3CN",
        "colab_type": "text"
      },
      "source": [
        "## Models\n",
        "\n",
        "Here we layout the Keras models of the analyzed architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgi0iyMk5PB",
        "colab_type": "text"
      },
      "source": [
        "### Feed-forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYF3P-olQPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoDense_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoDense')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teRDueoY8E4f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoDense_dropout_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoDense-Dropout')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zKRmgLz3nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ThreeDense_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='ThreeDense')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ljBH4TeHz9y7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ThreeDense_dropout_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Flatten(input_shape=input_shape),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='ThreeDense-Dropout')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBztSmjVk-Zz",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla33q7ilHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C1D_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape), # shape == (batch, steps, channels)\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-1C1D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVhjcs2btcs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C2D_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-1C2D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hAIEYmtnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C1D_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-2C1D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ew-qf2c9t_XI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C1D_dropout_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.3),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.3),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-2C1D-Dropout')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703_dnMjuNGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C2D_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-2C2D')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "76PArdebuP0i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C2D_dropout_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        Conv1D(32, 5, input_shape=input_shape),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.3),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        Dropout(0.3),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.3),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-2C2D-Dropout')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHQCDtzlAs-",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJBFATRAl-U7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(32, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        LSTM(32, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIW5dDBAoPmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def OneGRU_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        GRU(32, input_shape=input_shape),\n",
        "        Dense(num_classes)\n",
        "    ], name='OneGRU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuzqoxhheZu",
        "colab_type": "text"
      },
      "source": [
        "## Tests\n",
        "\n",
        "Here models are trained according to config files on the dataset split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSTL8Vn9WMX",
        "colab_type": "text"
      },
      "source": [
        "### Feed-forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K68uw52ZI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 5,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = TwoDense_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3txXPTj9mDN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 25,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = TwoDense_dropout_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAj2q2r9mzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 25,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = ThreeDense_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPIz2jLB9nZJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = ThreeDense_dropout_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXWiR979ZqM",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKaBl4gUvIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpeEIf2y6mac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJWTb8a8qBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_lPBEbH8qv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C1D_dropout_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6l8wwe-Jn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s0tKZtVb-MEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'shuffle': True\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2]) # valid for 1D models only\n",
        "model = Conv1D_2C2D_dropout_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sYZFUn9cJx",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXvRjzsTmpRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sb4aTEc1oYK4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mae',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[-1])\n",
        "model = OneGRU_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX4VjdNYYTtk",
        "colab_type": "text"
      },
      "source": [
        "# LSTM in Tensorflow\n",
        "\n",
        "**NOTE** not yet reviewed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E_JIdOMYEyj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "features = 32 #number of hidden layer's features\n",
        "classes = 6 # number of final classes\n",
        "\n",
        "lr = 0.0025\n",
        "lambda_l = 0.0015\n",
        "batch = 1500\n",
        "n_iters = 300\n",
        "tot_iters = Y_train.shape[0] * n_iters\n",
        "disp_iter = 1000\n",
        "\n",
        "\n",
        "w = {\n",
        "    'h' : tf.Variable(tf.random_normal([X_train.shape[2], features])),\n",
        "    'o' : tf.Variable(tf.random_normal([features, Y_train.shape[1]], mean=1.0))\n",
        "}\n",
        "b = {\n",
        "    'h' : tf.Variable(tf.random_normal([features])),\n",
        "    'o' : tf.Variable(tf.random_normal([Y_train.shape[1]]))\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvv75Z2iY-q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LSTM(X, w, b):\n",
        "    X = tf.transpose(X,[1,0,2]) #(batch_size, steps, input)\n",
        "    X = tf.reshape(X, [-1, X.shape[2]])  #(steps*batch, n_initial_\"features\")\n",
        "\n",
        "    X = tf.nn.relu(tf.matmul(X, w['h']) + b['h'])\n",
        "    X = tf.split(X, X_train.shape[1])\n",
        "    \n",
        "    #define LSTM\n",
        "    l_1 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)\n",
        "    l_2 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)    \n",
        "    lstm = tf.contrib.rnn.MultiRNNCell([l_1,l_2], state_is_tuple=True)    \n",
        "    \n",
        "    #output\n",
        "    out, state = tf.contrib.rnn.static_rnn(lstm, X, dtype=tf.float32)\n",
        "    \n",
        "    return tf.matmul(out[-1], w['o'])+b['o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqoOJlQZAjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train_2 = X_train.astype(np.float32)\n",
        "Y_train_2 = Y_train.astype(np.float32)\n",
        "X_test_2 = X_test.astype(np.float32)\n",
        "Y_test_2 = Y_test.astype(np.float32)\n",
        "\n",
        "tmp = tf.data.Dataset.from_tensor_slices((X_train_2, Y_train_2)).repeat().batch(300)\n",
        "\n",
        "iter = tmp.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "prediction = LSTM(x, w, b)\n",
        "l2_norm = lambda_l * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "\n",
        "softmax_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction))+l2_norm\n",
        "adam = tf.train.AdamOptimizer(learning_rate=lr).minimize(softmax_cost)\n",
        "\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y,1)),tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpiaUo-VzGIu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test = {'loss':[], 'acc':[]}\n",
        "train = {'loss':[], 'acc':[]}\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(1000): #epochs\n",
        "        _, l, a = sess.run([adam, softmax_cost, accuracy])\n",
        "        train['loss'].append(l)\n",
        "        train['acc'].append(a)\n",
        "        if i%1 == 0:\n",
        "            l,a = sess.run([softmax_cost, accuracy])\n",
        "            test['loss'].append(l)\n",
        "            test['acc'].append(a)\n",
        "            #print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "            #      \"Batch Loss = {}\".format(l) + \\\n",
        "            #      \", Accuracy = {}\".format(a))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjMOm_VJBx4a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(max(test['acc']))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
