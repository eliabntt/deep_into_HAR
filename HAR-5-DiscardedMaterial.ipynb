{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR-5-DiscardedMaterial.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Going deep into Human Activity Recognition\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "## Part 5 - Area 51\n",
        "\n",
        "In this _Discarded Material_ part, there is reference methods and code that may be useful in the future, but are not used in the main parts of this work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sp3W3AfC00sc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import logging\n",
        "logging.getLogger('tensorflow').disabled = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FWJT4Fh10eqQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def per_class_accuracy(y_true, y_preds, class_labels):\n",
        "    # for reference. confusion matrix diag is used instead\n",
        "    return [np.mean([\n",
        "            (y_true[pred_idx] == np.round(y_pred)) \n",
        "                for pred_idx, y_pred in enumerate(y_preds) \n",
        "                    if y_true[pred_idx] == int(class_label)\n",
        "        ]) for class_label in class_labels]\n",
        "\n",
        "def halfLRafterEpoch(epoch):\n",
        "    # for reference. lambda func is used instead\n",
        "    initial_lrate = 0.1\n",
        "    drop_rate = 0.5\n",
        "    epochs_drop = 10.0\n",
        "    return initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d07UpGJ41Co7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        hpd = pd.DataFrame(history.history)\n",
        "        json.dump(json.loads(hpd.to_json()), hfile, indent=2)\n",
        "\n",
        "        #json.dump(history.history, hfile, indent=2)\n",
        "        # native json module can't handle float32 objects\n",
        "        # pandas can and is used as a preprocessor to json module"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKEF8bhZ2fwb",
        "colab_type": "text"
      },
      "source": [
        "Checkpoint saving"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptgp6Jv42gWd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-raw.h5','w') as h5f:\n",
        "    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "    h5f.create_dataset('attitudes',  data=attitude_mat)\n",
        "    h5f.create_dataset('activities', data=activities)\n",
        "\n",
        "# optional reload if messing up below\n",
        "with h5py.File('dataset/ARS-raw.h5','r') as h5f:\n",
        "    imu_sensor = h5f['imu_sensor'][:]\n",
        "    attitude_mat = h5f['attitudes'][:]\n",
        "    activities = h5f['activities'][:]\n",
        "\n",
        "with h5py.File('dataset/ARS.h5','w') as h5f:\n",
        "    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "    h5f.create_dataset('imu_body', data=imu_body)\n",
        "    h5f.create_dataset('attitudes', data=attitude_mat)\n",
        "    h5f.create_dataset('activities', data=activities)\n",
        "\n",
        "with h5py.File('dataset/ARS-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('imu_sensor', data=imu_sensor_framed)\n",
        "    h5f.create_dataset('imu_body',   data=imu_body_framed)\n",
        "    h5f.create_dataset('activities', data=activities_sensor_framed)\n",
        "    #h5f.create_dataset('activities_body', data=activities_body_framed) # useless duplicate of prev item\n",
        "\n",
        "# optional reload if messing up below\n",
        "with h5py.File('dataset/ARS-framed.h5','r') as h5f:\n",
        "    imu_sensor_framed = h5f['imu_sensor'][:]\n",
        "    imu_body_framed = h5f['imu_body'][:]\n",
        "    activities_sensor_framed = h5f['activities'][:]\n",
        "    activities_body_framed = activities_sensor_framed.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NzaDTAGYZ_k",
        "colab_type": "text"
      },
      "source": [
        "## Computing weighted accuracies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKxwao9rYeQb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_names = {\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-rot-per-norm.h5' : 'BAHC',   # manual aug\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-rot-per.h5' : 'BAHCNN',\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-norm.h5' : 'BADA',           # adasyn\n",
        "    'ARS-train-test-body-framed-aug-onlytrain.h5' : 'BADANN',\n",
        "    'ARS-train-test-body-framed-norm.h5' : 'BNOR',                         # not augmented\n",
        "    'ARS-train-test-body-framed.h5' : 'BFRA',                              # not normalized  \n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-rot-per-norm.h5' : 'SAHC',\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-rot-per.h5' : 'SAHCNN',\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-norm.h5' : 'SADA',\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain.h5' : 'SADANN',\n",
        "    'ARS-train-test-sensor-framed-norm.h5' : 'SNOR',\n",
        "    'ARS-train-test-sensor-framed.h5' : 'SFRA'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43YUsAbUYeHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_folders(odir='output'):\n",
        "    folders = []\n",
        "    for d1 in [f for f in os.listdir(os.path.join(odir)) if os.path.isdir(os.path.join(odir, f))]:\n",
        "        for d2 in [f for f in os.listdir(os.path.join(odir,d1)) if os.path.isdir(os.path.join(odir, d1, f))]:\n",
        "            for d3 in [f for f in os.listdir(os.path.join(odir,d1,d2)) if os.path.isdir(os.path.join(odir, d1, d2, f))]:\n",
        "                #print(f\"{d1} <> {d2} <> {d3}\")\n",
        "                folders.append(os.path.join(odir,d1,d2,d3))\n",
        "    return folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-JkU8lcYd5b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_classes = 7\n",
        "folders = get_folders()\n",
        "for ds in dataset_names:\n",
        "    print(f\"Doing {dataset_names[ds]}\")\n",
        "    with h5py.File(f\"dataset/{ds}\",'r') as h5f:\n",
        "        #X_train = h5f['X_train'][:] # IMU data\n",
        "        X_test  = h5f['X_test'][:]  # activities\n",
        "        #Y_train = h5f['Y_train'][:]\n",
        "        Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "    samples = []\n",
        "    for activity in range(num_classes):\n",
        "        samples.append(len(X_test[Y_test == activity]))\n",
        "    print(f\"Samples: {samples}\")\n",
        "    #samples_w = [ val/sum(samples) for val in samples ]\n",
        "    #pprint(samples_w)\n",
        "    \n",
        "    int_folders = [f for f in folders if dataset_names[ds] in f and dataset_names[ds]+'NN' not in f and 'train' not in f]\n",
        "    for fld in int_folders:\n",
        "        print(f\"  Doing {fld}\")\n",
        "        for eval_file in [f for f in os.listdir(fld) if 'evaluation' in f]:\n",
        "            print(f\"    Doing {eval_file}:\\t\",end='')\n",
        "            with open(os.path.join(fld,eval_file),'r') as ef:\n",
        "                efj = json.load(ef)\n",
        "            acc = efj['acc-class']\n",
        "            summed = 0\n",
        "            for i in range(num_classes):\n",
        "                summed += acc[i]*samples[i]\n",
        "            weighted_a = summed / sum(samples)\n",
        "\n",
        "            print(f\"{weighted_a} while acc in json is {efj['acc']}\")\n",
        "            #efj['weighted-averages']['accuracy'] = weighted_a\n",
        "            #with open(os.path.join(fld,eval_file+'2'),'w') as ef:\n",
        "            #    json.dump(efj, ef, indent=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRDq4IfC1ska",
        "colab_type": "text"
      },
      "source": [
        "## LSTM in pure Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SVoYOai91u3K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model definition\n",
        "\n",
        "features = 32 # number of hidden layer's features\n",
        "\n",
        "#batch = 1500 # TODO unused vars\n",
        "#n_iters = 300\n",
        "#tot_iters = Y_train.shape[0] * n_iters\n",
        "#disp_iter = 1000\n",
        "\n",
        "w = {\n",
        "    'h' : tf.Variable(tf.random_normal([X_train.shape[2], features])),\n",
        "    'o' : tf.Variable(tf.random_normal([features, Y_train.shape[1]], mean=1.0))\n",
        "}\n",
        "b = {\n",
        "    'h' : tf.Variable(tf.random_normal([features])),\n",
        "    'o' : tf.Variable(tf.random_normal([Y_train.shape[1]]))\n",
        "}\n",
        "\n",
        "def LSTM(X, w, b):\n",
        "    # input processing\n",
        "    X = tf.transpose(X,[1,0,2])         # (batch_size, steps, input)\n",
        "    X = tf.reshape(X, [-1, X.shape[2]]) # (steps*batch, n_initial_\"features\")\n",
        "\n",
        "    X = tf.nn.relu(tf.matmul(X, w['h']) + b['h'])\n",
        "    X = tf.split(X, X_train.shape[1])\n",
        "    \n",
        "    # model\n",
        "    l_1 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)\n",
        "    l_2 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)    \n",
        "    lstm = tf.contrib.rnn.MultiRNNCell([l_1,l_2], state_is_tuple=True)    \n",
        "    \n",
        "    # output\n",
        "    out, state = tf.contrib.rnn.static_rnn(lstm, X, dtype=tf.float32)\n",
        "    \n",
        "    return tf.matmul(out[-1], w['o']) + b['o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nr6ZFREy1ysj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a dataset object on input\n",
        "ds_obj = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32), Y_train.astype(np.float32))).repeat().batch(300)\n",
        "iter = ds_obj.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "prediction = LSTM(x, w, b)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y,1)),tf.float32))\n",
        "\n",
        "# losses, optimizer\n",
        "lr = 0.0025\n",
        "lambda_l = 0.0015\n",
        "\n",
        "l2_norm = lambda_l * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "softmax_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) + l2_norm\n",
        "adam = tf.train.AdamOptimizer(learning_rate=lr).minimize(softmax_cost)\n",
        "\n",
        "# run training\n",
        "test_log  = {'loss':[], 'acc':[]}\n",
        "train_log = {'loss':[], 'acc':[]}\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(1000): #epochs\n",
        "        _, l, a = sess.run([adam, softmax_cost, accuracy])\n",
        "        train_log['loss'].append(l)\n",
        "        train_log['acc'].append(a)\n",
        "        \n",
        "        l,a = sess.run([softmax_cost, accuracy], feed_dict={x:X_test.astype(np.float32), y:Y_test.astype(np.float32)})\n",
        "        test_log['loss'].append(l)\n",
        "        test_log['acc'].append(a)\n",
        "        #print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "        #      \"Batch Loss = {}\".format(l) + \\\n",
        "        #      \", Accuracy = {}\".format(a))\n",
        "print('Reached {}'.format(max(test_log['acc'])))\n",
        "\n",
        "# save stuff and plots\n",
        "out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_LSTM-TF')\n",
        "if not os.path.exists(out_folder):\n",
        "    os.mkdir(out_folder)\n",
        "\n",
        "with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "    json.dump({'training':train_log, 'validation':test_log}, hfile, indent=2)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['loss'], label='Training')\n",
        "plt.plot( test_log['loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-loss')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['acc'], label='Training')\n",
        "plt.plot( test_log['acc'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIuL-vTym9Rx",
        "colab_type": "text"
      },
      "source": [
        "## Plots"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pfbjUZnYm-Ff",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "names = ['F   ','AL  ','AOL ','APR ','APRL']\n",
        "for t in container:\n",
        "    log = f\"{t}\\n\"\n",
        "    for i,ef in enumerate(container[t]['eval']):\n",
        "        log += f\"{names[i]} L {ef['loss']:.4f} A {ef['acc']:.4f} \"\n",
        "        log += f\"P {ef['precision']:.4f} R {ef['recall']:.4f} F1 {ef['f1']:.4f}\\n\"\n",
        "    print(log)\n",
        "    display(Image(container[t]['plot-accu'],width=420))\n",
        "    display(Image(container[t]['plot-loss'],width=420))\n",
        "    #display(Image(container[t]['plot-metr'],width=420))\n",
        "    display(Image(container[t]['plot-conf1'],width=420))\n",
        "    print('\\n')\n",
        "    display(Image(container[t]['plot-conf2'],width=600))\n",
        "    display(Image(container[t]['plot-conf3'],width=600))\n",
        "    print()\n",
        "    display(Image(container[t]['plot-conf4'],width=600))\n",
        "    display(Image(container[t]['plot-conf5'],width=600))\n",
        "    print('\\n')\n",
        "HTML('<style>.display_data { display: inline; } .output_image { display: inline; }</style>') # very good for side-to-side iamges in colab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2sPAi_dnIW2",
        "colab_type": "text"
      },
      "source": [
        "Classwise for best dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tNZcdXEnCn4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_dataset = 'SADA' # sensor, adasyn, normalized\n",
        "\n",
        "best_evals = np.array(best_evals)\n",
        "selected = best_evals[best_evals[:,0] == best_dataset]\n",
        "pprint(selected.tolist())\n",
        "print()\n",
        "\n",
        "for item in selected:\n",
        "    ds,net,_,metric,_,_ = item\n",
        "    corr = container_single[f\"{ds}_{net}\"]\n",
        "    \n",
        "    cm = corr['plots'][f\"confusion-best-{metric}.png\"]\n",
        "    corr_eval = corr['eval'][f\"best-{metric}\"]\n",
        "    cwa,cwf = corr_eval['acc-class'], corr_eval['f1-class']\n",
        "    \n",
        "    print(f\"{ds} {net} {metric}\\n{cwa}\\n{cwf}\\n\")\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.bar(range(len(cwa)),cwa)\n",
        "    plt.title('Accuracy')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "    plt.figure()\n",
        "    plt.bar(range(len(cwf)),cwf)\n",
        "    plt.title('F1')\n",
        "    plt.tight_layout()\n",
        "    print()\n",
        "HTML('<style>.display_data { display: inline; } .output_image { display: inline; }</style>')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mqjmI_V7nNB0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# norm vs not norm\n",
        "df4 = dfAccRev.copy()[['BAHC','BAHCNN',   'BADA','BADANN',   'BNOR','BFRA',   'SAHC','SAHCNN',   'SADA','SADANN',   'SNOR','SFRA']]\n",
        "display(df4)\n",
        "\n",
        "# manual vs adasyn vs not aug.\n",
        "df5 = dfAccRev.copy()[['BAHC','BADA','BNOR',   'SAHC','SADA','SNOR']]\n",
        "display(df5)\n",
        "\n",
        "# body vs sensor\n",
        "df4 = dfAccRev.copy()[['BAHC','SAHC',   'BADA','SADA',   'BNOR','SNOR',   'BFRA','SFRA']]\n",
        "display(df4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vy8jVSB7nPQ0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df2 = dfAcc.copy()\n",
        "df2.loc['mean'] = df2.mean() # adds mean row at the end\n",
        "display(df2)\n",
        "\n",
        "df3 = dfF1s.copy()\n",
        "df3.loc['mean'] = df3.mean()\n",
        "display(df3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjOkztI5nVMZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "body, sensor = 'BADA','SADA' # ['BADA','BAHC'],['SADA','SAHC']\n",
        "body_tests, sensor_tests = {},{}\n",
        "for test in container_single:\n",
        "    ds, net = test.split('_')\n",
        "    if ds == body:\n",
        "        body_tests[net] = container_single[test]\n",
        "    elif ds == sensor:\n",
        "        sensor_tests[net] = container_single[test]\n",
        "assert(body_tests.keys() == sensor_tests.keys())\n",
        "print(body_tests.keys())\n",
        "\n",
        "arr = []\n",
        "for key in body_tests:\n",
        "    b = [body, key]\n",
        "    for i in body_tests[key]['eval']['best-a']['acc-class']:\n",
        "        b.append(i)\n",
        "    b += [\n",
        "        body_tests[key]['eval']['best-a']['acc'],\n",
        "        body_tests[key]['eval']['best-a']['precision'],\n",
        "        body_tests[key]['eval']['best-a']['averages']['precision'],\n",
        "        body_tests[key]['eval']['best-a']['recall'],\n",
        "        body_tests[key]['eval']['best-a']['averages']['recall'],\n",
        "        body_tests[key]['eval']['best-a']['f1']\n",
        "    ]\n",
        "    s = [sensor,key]\n",
        "    for i in sensor_tests[key]['eval']['best-a']['acc-class']:\n",
        "        s.append(i)\n",
        "    s+= [\n",
        "        sensor_tests[key]['eval']['best-a']['acc'],\n",
        "        sensor_tests[key]['eval']['best-a']['precision'],\n",
        "        sensor_tests[key]['eval']['best-a']['averages']['precision'],\n",
        "        sensor_tests[key]['eval']['best-a']['recall'],\n",
        "        sensor_tests[key]['eval']['best-a']['averages']['recall'],\n",
        "        sensor_tests[key]['eval']['best-a']['f1']\n",
        "    ]\n",
        "    arr.append(b)\n",
        "    arr.append(s)\n",
        "df = pd.DataFrame(arr, columns=['ds','net','c1','c2','c3','c4','c5','c6','c7','a','p','wp','r','wr','f1'])\n",
        "df.pivot(index='ds', columns='net',values=['c1','c2','c3','c4','c5','c6','c7','a','p','wp','r','wr','f1'])"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
