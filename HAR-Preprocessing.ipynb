{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR-Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Human Activity Recognition using Inertial sensors and Neural Networks\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 1 - Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "!ls /content/drive/My\\ Drive/hda-project\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transforms3d\n",
        "clear_output()\n",
        "\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from transforms3d.axangles import axangle2mat\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (16,10)\n",
        "mpl.rcParams['axes.grid'] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZs68Tz7_mG",
        "colab_type": "text"
      },
      "source": [
        "Let's start from the [original datasets](https://www.dlr.de/kn/desktopdefault.aspx/tabid-8500/14564_read-36508/) provided as MATLAB `mat` files and process data items to have the final dataset all the network models will work on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l08TcOt9h2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = [\n",
        "    'dataset/ARS_DLR_DataSet.mat',\n",
        "    'dataset/ARS_DLR_DataSet_V2.mat',\n",
        "    'dataset/ARS_DLR_Benchmark_Data_Set.mat'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfOGIDTkDXgQ",
        "colab_type": "text"
      },
      "source": [
        "Classes  to detect are reduced from the original 17 down to 8.\n",
        "\n",
        "Furthermore, transitions will not be detected, so the final number of used labels is 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovSOpId9vap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = ['RUNNING', 'WALKING', 'JUMPING','STNDING','SITTING', 'XLYINGX', 'FALLING',\n",
        "    'WALKUPS', 'WALKDWS',\n",
        "    'JUMPVRT', 'JUMPFWD', 'JUMPBCK', \n",
        "    'TRANSUP', 'TRANSDW', 'TRNSACC', 'TRNSDCC', 'TRANSIT'\n",
        "]\n",
        "\n",
        "# full map for all 17 classes\n",
        "map_encode = { label:i for i,label in enumerate(labels) }\n",
        "\n",
        "# map to squeeze down to 8 classes\n",
        "map_encode_8 = {\n",
        "    0:0,   1:1,  2:2,  3:3,  4:4,  5:5,  6:6, # untouched\n",
        "    7:1,   8:1,                  # walking up and downstairs = walking\n",
        "    9:2,  10:2, 11:2,            # jumping in place, forward and backward = jumping\n",
        "    12:7, 13:7, 14:7, 15:7, 16:7 # all transitions\n",
        "}\n",
        "\n",
        "# useful for plots\n",
        "map_decode_8 = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling',\n",
        "    7: 'transition'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4FskfwEOlL",
        "colab_type": "text"
      },
      "source": [
        "The dataset provides the list of labels (\"activities\") for each segment of tracked data, along with index bounds (start and stop) for each item of the list.\n",
        "\n",
        "Outside these ranges, data is considered to be marked as transitions between classes.\n",
        "\n",
        "This structure is flatten to a single list.\n",
        "\n",
        "**TODO** write about holes and mislabeling errors in the report "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Z9H1Tv-W2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_labels(labels, bounds):\n",
        "    \"\"\"Builds a single labels array from labels and bounds.\"\"\"\n",
        "    start = bounds[0::2]-1 # even positions\n",
        "    stop  = bounds[1::2]   # -1: numbering starts from 1\n",
        "    # start is included, stop is excluded\n",
        "    \n",
        "    res = np.ones(bounds[-1], dtype=np.uint8) * map_encode['TRANSIT']\n",
        "    for i, lab in enumerate(labels):\n",
        "        if i+1 < len(stop):\n",
        "            if stop[i] > start[i+1]+1:\n",
        "                # advance next start, mislabeling error\n",
        "                print('Time error: {} > {}'.format(stop[i],start[i+1]))\n",
        "                start[i+1] = stop[i]\n",
        "        res[start[i] : stop[i]] = lab\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVcomuAZEi2r",
        "colab_type": "text"
      },
      "source": [
        "Datasets contain IMU measurements referred to the sensor frame, but also provides the attitude/cosine matrix to express the measurements w.r.t. the body frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsd0NVgF-xHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_body_frame(imu_data, attitude_matrix):\n",
        "    \"\"\"Converts sensor frames in a data item to body frames through the attitude matrix.\"\"\"\n",
        "    C = attitude_matrix[1:].reshape(3,3).T\n",
        "\n",
        "    result = imu_data.copy()\n",
        "    result[1:4]  = np.dot(C, imu_data[1:4].T)  # acc\n",
        "    result[4:7]  = np.dot(C, imu_data[4:7].T)  # gyro\n",
        "    result[7:10] = np.dot(C, imu_data[7:10].T) # mag\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Ladk5mE9EP",
        "colab_type": "text"
      },
      "source": [
        "For each test in a dataset, extract the relevant data and then flat the labels to a single long list.\n",
        "\n",
        "The rest of the processing is postponed to operate on the full dataset instead of working on single tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke3Npi6a_u0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_single_test(dataset, key):\n",
        "    imu_data, attitude_matrices, activities, activities_bounds = dataset[key][0]\n",
        "\n",
        "    # throwing away useless nested arrays\n",
        "    activities = np.array([ act[0] for act in activities[0] ])\n",
        "    activities_bounds = activities_bounds[0]\n",
        "    \n",
        "    # integrity checks on time and length\n",
        "    assert([ imu_data[i][0] == attitude_matrices[i][0] for i in range(len(imu_data)) ])\n",
        "    assert(len(activities_bounds) == 2*len(activities))\n",
        "\n",
        "    # change labels to int numbers\n",
        "    activities = np.array([ map_encode[a] for a in activities ])\n",
        "\n",
        "    # get a single array of labels instead of labels + bounds\n",
        "    activities_flat = flatten_labels(activities, activities_bounds)\n",
        "    #print(len(activities_flat[activities_flat == 0]))\n",
        "    assert(len(imu_data) == len(activities_flat))\n",
        "\n",
        "    return imu_data, attitude_matrices, activities_flat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLbeEpQXFdtH",
        "colab_type": "text"
      },
      "source": [
        "Numpy arrays are used to store all the collected data from every test in each dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlBfX4BBvHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imu_sensor = np.empty((0,10))\n",
        "attitude_mat = np.empty((0,10))\n",
        "activities = np.empty((0,1), dtype=np.uint8)\n",
        "\n",
        "# loop datasets\n",
        "for ds in datasets:\n",
        "    dataset = scipy.io.loadmat(ds)\n",
        "    keys = [ k for k in dataset if '__' not in k ]\n",
        "    # loop keys=tests\n",
        "    for test in keys:\n",
        "        print('Loading {}:'.format(test).ljust(52,' '), end='')\n",
        "        imu, mat, act = process_single_test(dataset, test)\n",
        "        imu_sensor = np.append(imu_sensor, imu, axis=0)\n",
        "        attitude_mat = np.append(attitude_mat, mat, axis=0)\n",
        "        activities = np.append(activities, act)\n",
        "        \n",
        "        print('{} elements'.format(imu.shape[0]).rjust(15,' '))\n",
        "\n",
        "assert(imu_sensor.shape==attitude_mat.shape)\n",
        "num_data = imu_sensor.shape[0]\n",
        "clear_output()\n",
        "print('Read {} records'.format(num_data))\n",
        "print('IMU data shape:   {}'.format(imu_sensor.shape))\n",
        "print('Attitudes shape:  {}'.format(attitude_mat.shape))\n",
        "print('Activities shape: {}'.format(activities.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2TD-DUTvpi",
        "colab_type": "text"
      },
      "source": [
        "We save this checkpoint, even if we don't use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unmK9NtcUAwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with h5py.File('dataset/ARS-raw.h5','w') as h5f:\n",
        "#    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "#    h5f.create_dataset('attitudes',  data=attitude_mat)\n",
        "#    h5f.create_dataset('activities', data=activities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or5FNfvCUMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optional reload if messing up below\n",
        "#with h5py.File('dataset/ARS-raw.h5','r') as h5f:\n",
        "#    imu_sensor = h5f['imu_sensor'][:]\n",
        "#    attitude_mat = h5f['attitudes'][:]\n",
        "#    activities = h5f['activities'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXeJ_ymQLYvn",
        "colab_type": "text"
      },
      "source": [
        "Performing some checks:\n",
        "\n",
        "- Displaying minutes for each activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coFWmHUDLdJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act,cnt = np.unique(activities, return_counts=True)\n",
        "cnt = cnt / 100 / 60 # 100 Hz, 60 seconds\n",
        "act = np.array([labels[a].lower() for a in act])\n",
        "\n",
        "act = np.append(act,'total')\n",
        "cnt = np.append(cnt, sum(cnt))\n",
        "\n",
        "df = pd.DataFrame(np.array([act,np.round(cnt)]).T, columns=['Activity','Time (min)'])\n",
        "df.index = [''] * len(df) # blank index\n",
        "display(df)\n",
        "\n",
        "#TODO other if needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_-gvcNBHKkd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "More processing on the whole dataset:\n",
        "- Reduce the number of tracked activities to 8\n",
        "- Remove of items labelled as transitions\n",
        "- Conversion of measurements to the body frame\n",
        "- Remove time columns (in both measurements and attitude matrixes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27hU5N27eRXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remap activities\n",
        "print('Old labels: {}'.format(np.unique(activities)))\n",
        "activities = np.array([ map_encode_8[act] for act in activities])\n",
        "print('New labels: {}'.format(np.unique(activities)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtvqG1aHLzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove transitions\n",
        "transit_label = map_encode_8[map_encode['TRANSIT']]\n",
        "transit_number = sum(activities == transit_label)\n",
        "print('Transit label is {}, found {} elements'.format(transit_label, transit_number))\n",
        "\n",
        "num_data = len(imu_sensor)\n",
        "print('Old num data: {}'.format(num_data))\n",
        "imu_sensor = imu_sensor[activities != transit_label]\n",
        "attitude_mat = attitude_mat[activities != transit_label]\n",
        "activities = activities[activities != transit_label]\n",
        "\n",
        "assert(imu_sensor.shape[0] == num_data-transit_number)\n",
        "assert(attitude_mat.shape[0] == num_data-transit_number)\n",
        "assert(activities.shape[0] == num_data-transit_number)\n",
        "\n",
        "num_data = len(imu_sensor)\n",
        "print('New num data: {}'.format(num_data))\n",
        "\n",
        "num_labels = len(np.unique(activities))\n",
        "print('New num labels: {}'.format(num_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ngv9Uwd8jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert from sensor frame to body frame\n",
        "imu_body = imu_sensor.copy()\n",
        "for i, imu in enumerate(imu_sensor):\n",
        "    imu_body[i] = convert_body_frame(imu, attitude_mat[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gkhe3BMd-Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove time column from data\n",
        "imu_sensor = imu_sensor[:,1:]\n",
        "imu_body = imu_body[:,1:]\n",
        "attitude_mat = attitude_mat[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGxV0TGtLgYm",
        "colab_type": "text"
      },
      "source": [
        "Performing some checks:\n",
        "\n",
        "- Displaying minutes for each activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlBD0dxyLhqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act,cnt = np.unique(activities, return_counts=True)\n",
        "cnt = cnt / 100 / 60\n",
        "act = np.array([map_decode_8[a] for a in act])\n",
        "\n",
        "act = np.append(act,'total')\n",
        "cnt = np.append(cnt, sum(cnt))\n",
        "\n",
        "df = pd.DataFrame(np.array([act,np.round(cnt)]).T, columns=['Activity','Time (min)'])\n",
        "df.index = [''] * len(df)\n",
        "display(df)\n",
        "\n",
        "#TODO other if needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlQj-FHruzD2",
        "colab_type": "text"
      },
      "source": [
        "Pre-compute and save a 80/20 train/test split of the dataset.\n",
        "\n",
        "random_state is the seed of the PRNG.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_rP0qgZuwCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = \\\n",
        "    train_test_split(imu_sensor, activities, test_size=0.2, random_state=1, stratify=activities) \n",
        "\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(len(Y_train)))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(len(Y_test)))\n",
        "\n",
        "Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "    train_test_split(imu_body, activities, test_size=0.2, random_state=1, stratify=activities) \n",
        "\n",
        "print(\"Xb_train shape: \" + str(Xb_train.shape))\n",
        "print(\"Yb_train shape: \" + str(len(Yb_train)))\n",
        "print(\"Xb_test shape:  \" + str(Xb_test.shape))\n",
        "print(\"Yb_test shape:  \" + str(len(Yb_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtESTYRbunOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS.h5','w') as h5f:\n",
        "    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "    h5f.create_dataset('imu_body', data=imu_body)\n",
        "    h5f.create_dataset('attitudes', data=attitude_mat)\n",
        "    h5f.create_dataset('activities', data=activities)\n",
        "    \n",
        "with h5py.File('dataset/ARS-train-test-sensor.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "    \n",
        "with h5py.File('dataset/ARS-train-test-body.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BJbsBq-MlC_",
        "colab_type": "text"
      },
      "source": [
        "Group by activity and organize data in overlapping windows (overlapping ratio is regulated by `stride_len`). \n",
        "\n",
        "Save the resulting dataset and a corresponding 80/20 split.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms44alUwMmYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_len = 128\n",
        "stride_len = round(window_len / 2)\n",
        "\n",
        "def framing_padding(data):\n",
        "    x, y = [], []\n",
        "\n",
        "    for activity in np.unique(activities):\n",
        "        tmp = data[activities == activity]\n",
        "        if len(tmp) % stride_len != 0:\n",
        "            # append zeroes to fill the window, if necessary\n",
        "            windows_inside = math.ceil( (len(tmp) - window_len) / stride_len )\n",
        "            windows_space = windows_inside * stride_len\n",
        "            rest = windows_space - len(tmp) + window_len\n",
        "            #print('Act. {}, appending {} rows of zeros'.format(activity,rest))\n",
        "\n",
        "            tmp = np.append(tmp, np.zeros((rest,data.shape[1])), axis=0)\n",
        "            #tmp = np.append(tmp, [[0]*9]*int((math.floor((len(tmp)-window_len)/stride_len) + 1)*stride_len-len(tmp)+window_len), axis = 0)\n",
        "\n",
        "        # exlude unnecessary padded windows\n",
        "        for i in range(0, len(tmp)-window_len, stride_len):\n",
        "            x.extend([tmp[i:i+window_len]])\n",
        "            y.extend([activity])\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    assert(x.shape[0] == len(y))\n",
        "    return x,y\n",
        "\n",
        "imu_sensor_framed, activities_sensor_framed = framing_padding(imu_sensor)\n",
        "imu_body_framed, activities_body_framed = framing_padding(imu_body)\n",
        "\n",
        "assert(np.array_equal(activities_sensor_framed, activities_body_framed))\n",
        "\n",
        "print(\"IMU_sensor shape: \" + str(imu_sensor_framed.shape))\n",
        "print(\"IMU body shape:   \" + str(imu_body_framed.shape))\n",
        "print(\"Activities shape: \" + str(len(activities_sensor_framed)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXz4oqm2UpNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = \\\n",
        "    train_test_split(imu_sensor_framed, activities_sensor_framed, test_size=0.2, random_state=1, stratify=activities_sensor_framed) \n",
        "\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(len(Y_train)))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(len(Y_test)))\n",
        "\n",
        "Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "    train_test_split(imu_body_framed, activities_body_framed, test_size=0.2, random_state=1, stratify=activities_body_framed) \n",
        "\n",
        "print(\"Xb_train shape: \" + str(Xb_train.shape))\n",
        "print(\"Yb_train shape: \" + str(len(Yb_train)))\n",
        "print(\"Xb_test shape:  \" + str(Xb_test.shape))\n",
        "print(\"Yb_test shape:  \" + str(len(Yb_test)))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p87LKo2sNo_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('imu_sensor', data=imu_sensor_framed)\n",
        "    h5f.create_dataset('imu_body',   data=imu_body_framed)\n",
        "    h5f.create_dataset('activities', data=activities_sensor_framed)\n",
        "    #h5f.create_dataset('activities_body', data=activities_body_framed)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-sensor-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-body-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQjCrfvvQkut",
        "colab_type": "text"
      },
      "source": [
        "Data normalization using training set's mean and std. May improve results (or may not, this is a thing to test)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kD60s-HQmUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    tmp_test = X_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    X_train[:,:,i] = np.reshape((tmp_train - mean)/std, (X_train.shape[0], X_train.shape[1]))\n",
        "    X_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (X_test.shape[0], X_test.shape[1]))\n",
        "\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    tmp_test = Xb_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    Xb_train[:,:,i] = np.reshape((tmp_train - mean)/std, (Xb_train.shape[0], Xb_train.shape[1]))\n",
        "    Xb_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (Xb_test.shape[0], Xb_test.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ZzP38vZLdRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# checks\n",
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    print(np.mean(tmp_train), end=' -- ')\n",
        "    print(np.std(tmp_train))\n",
        "\n",
        "print()\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    print(np.mean(tmp_train), end=' -- ')\n",
        "    print(np.std(tmp_train))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj3CHzJCMbZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-train-test-sensor-framed-norm.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-body-framed-norm.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3hygy5loNMny",
        "colab_type": "text"
      },
      "source": [
        "Augmentation of data:\n",
        "- By applying a rotation of a random angle against a random axis, \n",
        "- By permutation of data in a window (currently unimplemented)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WegMdmyBSMD5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_random(data):\n",
        "    # axis and angle of the rotation\n",
        "    axis = np.random.uniform(low=-1, high=1, size = 3)\n",
        "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "        \n",
        "    \n",
        "    res = data.copy()\n",
        "    for i in range(0,len(data),3):\n",
        "        res[i:i+3] = np.matmul(data[i:i+3], axangle2mat(axis,angle))\n",
        "\n",
        "    return res\n",
        "\n",
        "def rotate_values(values):\n",
        "    for i in range(values.shape[0]):\n",
        "        for j in range(values.shape[1]):\n",
        "            values[i,j] = rotate_random(values[i,j])\n",
        "    return values\n",
        "\n",
        "def permute_values(values):\n",
        "    # currently not implemented\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cd0PcnBrNLQj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TODO check if only for training or for whole dataset\n",
        "# TODO do this for both sensor and body\n",
        "permute = False\n",
        "rotate = True\n",
        "augm_factor = 0.35\n",
        "\n",
        "count = []\n",
        "for activity in np.unique(Y_test):\n",
        "    count.append(len(x[y == activity]))\n",
        "percentage_before = count/np.sum(count,axis=0)\n",
        "most_repr = int(max(count)*augm_factor)\n",
        "\n",
        "x_add = []\n",
        "y_add = []\n",
        "for activity in range(len(count)):\n",
        "    augment_number = most_repr - count[activity]\n",
        "    if augment_number > 0:\n",
        "        print(f\"Class {activity} has {count[activity]} entries and needs to be augmented with {augment_number} values\")\n",
        "        values = x[np.array(y) == activity][np.random.choice(range(count[activity]),size=augment_number)]\n",
        "        if rotate:\n",
        "            values = rotate_values(values)\n",
        "        if permute:\n",
        "            permute_values(values)\n",
        "        x_add.extend(values)\n",
        "        print(np.array(x_add).shape)\n",
        "        y_add.extend([activity]*values.shape[0])\n",
        "            \n",
        "    else:\n",
        "        print(f\"Class {activity} has enough values\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4PC9-kbBdYkg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#X_train, X_test, Y_train, Y_test = \\\n",
        "#   train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)\n",
        "#Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "#   train_test_split(x, y, test_size=0.2, random_state=1, stratify=y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeVyW34xuy7o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with h5py.File('dataset/ARS-train-test-sensor-framed-norm-augm.h5','w') as h5f:\n",
        "#    h5f.create_dataset('X_train', data=X_train)\n",
        "#    h5f.create_dataset('X_test',  data=X_test)\n",
        "#    h5f.create_dataset('Y_train', data=Y_train)\n",
        "#    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "#with h5py.File('dataset/ARS-train-test-body-framed-norm-augm.h5','w') as h5f:\n",
        "#    h5f.create_dataset('X_train', data=Xb_train)\n",
        "#    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "#    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "#    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XpW5WzsQodc",
        "colab_type": "text"
      },
      "source": [
        "Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqjGncLWQplM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#TODO"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncBaXDbos436",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git status"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5auHrfHQVi83",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git add HAR-Preprocessing.ipynb;"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3OVQNyusvBEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git config --global user.name 'Elia Bonetto'; git config user.email 'eliabntt94@gmail.com'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0E_ym-T1VqSD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git commit -m \"Framing\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7IUJptgV7Kp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git push origin master"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
