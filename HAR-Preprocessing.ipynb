{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR-Preprocessing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Going deep into Human Activity Recognition\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 1 - Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install transforms3d\n",
        "clear_output()\n",
        "\n",
        "from collections import Counter\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import random\n",
        "\n",
        "from transforms3d.axangles import axangle2mat\n",
        "\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from imblearn.over_sampling import ADASYN \n",
        "from sklearn.model_selection import train_test_split\n",
        "import skimage\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rcParams['figure.figsize'] = (16,10)\n",
        "mpl.rcParams['axes.grid'] = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZs68Tz7_mG",
        "colab_type": "text"
      },
      "source": [
        "## Loading\n",
        "\n",
        "Let's start from the [original datasets](https://www.dlr.de/kn/desktopdefault.aspx/tabid-8500/14564_read-36508/) provided as MATLAB `mat` files and process data items to have the final dataset all the network models will work on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9l08TcOt9h2y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datasets = [\n",
        "    'dataset/ARS_DLR_DataSet.mat',\n",
        "    'dataset/ARS_DLR_DataSet_V2.mat',\n",
        "    'dataset/ARS_DLR_Benchmark_Data_Set.mat'\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MfOGIDTkDXgQ",
        "colab_type": "text"
      },
      "source": [
        "Classes  to detect are reduced from the original 17 down to 8.\n",
        "\n",
        "Furthermore, transitions will not be detected, so the final number of used labels is 7."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lovSOpId9vap",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "labels = [\n",
        "    'RUNNING', 'WALKING', 'JUMPING','STNDING','SITTING', 'XLYINGX', 'FALLING',\n",
        "    'WALKUPS', 'WALKDWS',\n",
        "    'JUMPVRT', 'JUMPFWD', 'JUMPBCK', \n",
        "    'TRANSUP', 'TRANSDW', 'TRNSACC', 'TRNSDCC', 'TRANSIT'\n",
        "]\n",
        "\n",
        "# full map for all 17 classes\n",
        "map_encode = { label:i for i,label in enumerate(labels) }\n",
        "\n",
        "# map to squeeze down to 8 classes\n",
        "map_encode_8 = {\n",
        "    0:0,   1:1,  2:2,  3:3,  4:4,  5:5,  6:6, # untouched\n",
        "    7:1,   8:1,                  # walking up and downstairs = walking\n",
        "    9:2,  10:2, 11:2,            # jumping in place, forward and backward = jumping\n",
        "    12:7, 13:7, 14:7, 15:7, 16:7 # all transitions\n",
        "}\n",
        "\n",
        "# useful for plots\n",
        "map_decode_8 = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling',\n",
        "    7: 'transition'\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kB4FskfwEOlL",
        "colab_type": "text"
      },
      "source": [
        "The dataset provides the list of labels (\"activities\") for each segment of tracked data, along with index bounds (start and stop) for each item of the list.\n",
        "\n",
        "Outside these ranges, data is considered to be marked as transitions between classes.\n",
        "\n",
        "This structure is flatten to a single list.\n",
        "\n",
        "**TODO** write about holes and mislabeling errors in the report "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5Z9H1Tv-W2L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def flatten_labels(labels, bounds):\n",
        "    \"\"\"Builds a single labels array from labels and bounds.\"\"\"\n",
        "    start = bounds[0::2]-1 # even positions\n",
        "    stop  = bounds[1::2]   # -1: numbering starts from 1\n",
        "    # start is included, stop is excluded\n",
        "    \n",
        "    res = np.ones(bounds[-1], dtype=np.uint8) * map_encode['TRANSIT']\n",
        "    for i, lab in enumerate(labels):\n",
        "        if i+1 < len(stop):\n",
        "            if stop[i] > start[i+1]+1:\n",
        "                # advance next start, mislabeling error\n",
        "                print('Time error: {} > {}'.format(stop[i],start[i+1]), end=' ')\n",
        "                start[i+1] = stop[i]\n",
        "        res[start[i] : stop[i]] = lab\n",
        "    return res"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVcomuAZEi2r",
        "colab_type": "text"
      },
      "source": [
        "Datasets contain IMU measurements referred to the sensor frame, but also provides the attitude/cosine matrix to express the measurements w.r.t. the body frame."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zsd0NVgF-xHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert_body_frame(imu_data, attitude_matrix):\n",
        "    \"\"\"Converts sensor frames in a data item to body frames through the attitude matrix.\"\"\"\n",
        "    C = attitude_matrix[1:].reshape(3,3).T\n",
        "\n",
        "    result = imu_data.copy()\n",
        "    result[1:4]  = np.dot(C, imu_data[1:4].T)  # acc\n",
        "    result[4:7]  = np.dot(C, imu_data[4:7].T)  # gyro\n",
        "    result[7:10] = np.dot(C, imu_data[7:10].T) # mag\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0Ladk5mE9EP",
        "colab_type": "text"
      },
      "source": [
        "For each test in a dataset, extract the relevant data and then flat the labels to a single long list.\n",
        "\n",
        "The rest of the processing is postponed to operate on the full dataset instead of working on single tests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke3Npi6a_u0E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def process_single_test(dataset, key):\n",
        "    imu_data, attitude_matrices, activities, activities_bounds = dataset[key][0]\n",
        "\n",
        "    # throwing away useless nested arrays\n",
        "    activities = np.array([ act[0] for act in activities[0] ])\n",
        "    activities_bounds = activities_bounds[0]\n",
        "    \n",
        "    # integrity checks on time and length\n",
        "    assert([ imu_data[i][0] == attitude_matrices[i][0] for i in range(len(imu_data)) ])\n",
        "    assert(len(activities_bounds) == 2*len(activities))\n",
        "\n",
        "    # change labels to int numbers\n",
        "    activities = np.array([ map_encode[a] for a in activities ])\n",
        "\n",
        "    # get a single array of labels instead of labels + bounds\n",
        "    activities_flat = flatten_labels(activities, activities_bounds)\n",
        "    assert(len(imu_data) == len(activities_flat))\n",
        "\n",
        "    info_dict = {\n",
        "        'name': key,\n",
        "        'act_list': [ map_decode_8[map_encode_8[i]] for i in activities ],\n",
        "        'act': [ (int(a),int(i),int(f)) for a,i,f in zip(activities, activities_bounds[0::2], activities_bounds[1::2]) ] }\n",
        "    \n",
        "    return imu_data, attitude_matrices, activities_flat, info_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLbeEpQXFdtH",
        "colab_type": "text"
      },
      "source": [
        "Numpy arrays are used to store all the collected data from every test in each dataset.\n",
        "\n",
        "For some relevant tests (those with a good mix of activities) plots of the (normalized) magnitude of IMU vectors are saved."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gKlBfX4BBvHq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "imu_sensor = np.empty((0,10))\n",
        "attitude_mat = np.empty((0,10))\n",
        "activities = np.empty((0,1), dtype=np.uint8)\n",
        "info_list = [] # just for keeping info about single tests\n",
        "\n",
        "tests_to_plot = [\n",
        "    'ARS_Cristina_Test_JmpFall_Sensor_Right',\n",
        "    'ARS_Hanno_Test_JmpFall_Sensor_Right',\n",
        "    'ARS_Maria_Test_JmpFall_Sensor_Left',\n",
        "    'ARS_Paula_Benchmark_Sensor_Left',\n",
        "    'ARS_Sinja_Benchmark_Sensor_Left',\n",
        "    'ARS_Emil_Benchmark_Sensor_Right'\n",
        "]\n",
        "\n",
        "# loop datasets\n",
        "for ds in datasets:\n",
        "    dataset = scipy.io.loadmat(ds)\n",
        "    keys = [ k for k in dataset if '__' not in k ]\n",
        "    # loop keys=tests\n",
        "    for test in keys:\n",
        "        print('Loading {}:'.format(test).ljust(52,' '), end='')\n",
        "        imu, mat, act, info = process_single_test(dataset, test)\n",
        "        imu_sensor = np.append(imu_sensor, imu, axis=0)\n",
        "        attitude_mat = np.append(attitude_mat, mat, axis=0)\n",
        "        activities = np.append(activities, act)\n",
        "        \n",
        "        info['start'] = int(imu_sensor.shape[0] - imu.shape[0])\n",
        "        info['end'] = int(imu_sensor.shape[0] - 1)\n",
        "        info_list.append(info)\n",
        "        print('{} elements'.format(imu.shape[0]).rjust(15,' '), end='')\n",
        "        \n",
        "        if test in tests_to_plot:\n",
        "            print('\\tSaving magnitude plots...', end='')\n",
        "            x = range(imu.shape[0])\n",
        "            acc = np.linalg.norm(imu[:,1:4],  axis=1) # vector magnitude\n",
        "            gyr = np.linalg.norm(imu[:,4:7],  axis=1)\n",
        "            mag = np.linalg.norm(imu[:,7:10], axis=1)\n",
        "            acc /= acc.max() # normalization\n",
        "            gyr /= gyr.max()\n",
        "            mag /= mag.max()\n",
        "            \n",
        "            act2 = [ map_encode_8[a] for a in act ]\n",
        "            cmap8 = mpl.cm.get_cmap('tab10', 8)\n",
        "            fig, (pa,pg,pm) = plt.subplots(3, 1, sharex=True)\n",
        "            #fig.suptitle(test, y=0.95, fontsize=16) # no title: will go in figure's caption\n",
        "            pa.scatter(x, acc, s=1, c=act2, cmap=cmap8)\n",
        "            pg.scatter(x, gyr, s=1, c=act2, cmap=cmap8)\n",
        "            pm.scatter(x, mag, s=1, c=act2, cmap=cmap8)\n",
        "            pa.set_ylabel('Accelerometer')\n",
        "            pg.set_ylabel('Gyroscope')\n",
        "            pm.set_ylabel('Magnetometer')\n",
        "            pm.set_xlabel('Time')\n",
        "            \n",
        "            #cb = plt.colorbar(sc) # sc = first plt.scatter\n",
        "            #cb.set_ticks(np.arange(8) + 0.5)\n",
        "            #cmap_labels = list(map_decode_8.values())\n",
        "            #cb.set_ticklabels(cmap_labels)#.append(' '))\n",
        "            \n",
        "            #fig.tight_layout(rect=[0, 0, 1, 0.93])\n",
        "            fig.tight_layout()\n",
        "            fname = os.path.join('.', 'images', test)\n",
        "            fig.savefig(fname+'.png')\n",
        "            fig.savefig(fname+'.pdf', format='pdf')\n",
        "            plt.close()\n",
        "        print()\n",
        "        \n",
        "assert(imu_sensor.shape==attitude_mat.shape)\n",
        "num_data = imu_sensor.shape[0]\n",
        "\n",
        "# save info file for reference\n",
        "with open('dataset/info.json','w') as info_file:\n",
        "    json.dump(info_list, info_file, indent=2)\n",
        "\n",
        "clear_output()\n",
        "print('Read {} records'.format(num_data))\n",
        "print('IMU data shape:   {}'.format(imu_sensor.shape))\n",
        "print('Attitudes shape:  {}'.format(attitude_mat.shape))\n",
        "print('Activities shape: {}'.format(activities.shape))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0K15js_bM1K",
        "colab_type": "text"
      },
      "source": [
        "Save label's color palette, useful in report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxSyiunRUY7w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cm = mpl.cm.get_cmap('tab10',8)\n",
        "\n",
        "colors = np.array(cm.colors)\n",
        "colors255 = np.round(colors*255)[:,:-1].astype(np.uint8) # no alpha channel\n",
        "\n",
        "np.savetxt(\"dataset/colors.csv\",    colors,    delimiter=',', fmt='%.5f')\n",
        "np.savetxt(\"dataset/colors255.csv\", colors255, delimiter=',', fmt='%d')\n",
        "\n",
        "indices = np.arange(8).reshape(1,8)\n",
        "arr = colors255[indices]\n",
        "arr = np.repeat(arr, 500, axis=0)\n",
        "arr = np.repeat(arr, 300, axis=1)\n",
        "\n",
        "skimage.io.imsave('dataset/colors.jpg', arr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nt2TD-DUTvpi",
        "colab_type": "text"
      },
      "source": [
        "Save this checkpoint, even if it will not be used."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unmK9NtcUAwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with h5py.File('dataset/ARS-raw.h5','w') as h5f:\n",
        "#    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "#    h5f.create_dataset('attitudes',  data=attitude_mat)\n",
        "#    h5f.create_dataset('activities', data=activities)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "or5FNfvCUMik",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optional reload if messing up below\n",
        "#with h5py.File('dataset/ARS-raw.h5','r') as h5f:\n",
        "#    imu_sensor = h5f['imu_sensor'][:]\n",
        "#    attitude_mat = h5f['attitudes'][:]\n",
        "#    activities = h5f['activities'][:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iXeJ_ymQLYvn",
        "colab_type": "text"
      },
      "source": [
        "Performing some checks:\n",
        "\n",
        "- Displaying minutes for each activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coFWmHUDLdJJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act,cnt = np.unique(activities, return_counts=True)\n",
        "cnt = cnt / 100 / 60 # 100 Hz, 60 seconds\n",
        "act = np.array([labels[a].lower() for a in act])\n",
        "\n",
        "act = np.append(act,'total')\n",
        "cnt = np.append(cnt, sum(cnt))\n",
        "\n",
        "df = pd.DataFrame(np.array([act,np.round(cnt)]).T, columns=['Activity','Time (min)'])\n",
        "df.index = [''] * len(df) # blank index\n",
        "display(df)\n",
        "\n",
        "#TODO other if needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_-gvcNBHKkd",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## More processing\n",
        "\n",
        "Starting from the whole dataset:\n",
        "- Reduce the number of tracked activities to 8\n",
        "- Remove of items labelled as transitions\n",
        "- Conversion of measurements to the body frame\n",
        "- Remove time columns (in both measurements and attitude matrixes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "27hU5N27eRXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remap activities\n",
        "print('Old labels: {}'.format(np.unique(activities)))\n",
        "activities = np.array([ map_encode_8[act] for act in activities])\n",
        "print('New labels: {}'.format(np.unique(activities)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqcmtN6erf1J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# save times as latex table for report\n",
        "act,cnt = np.unique(activities, return_counts=True)\n",
        "cnt = cnt / 100 / 60\n",
        "act = np.array([map_decode_8[a] for a in act])\n",
        "\n",
        "act = np.append(act,'total')\n",
        "cnt = np.append(cnt, sum(cnt))\n",
        "\n",
        "df = pd.DataFrame(np.array([act,np.round(cnt)]).T, columns=['Activity','Time (min)'])\n",
        "df.index = [''] * len(df)\n",
        "\n",
        "with open('output/act-times.tex','w') as tfile:\n",
        "    tfile.write(df.to_latex(index=False))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKtvqG1aHLzP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove transitions\n",
        "transit_label = map_encode_8[map_encode['TRANSIT']]\n",
        "transit_number = sum(activities == transit_label)\n",
        "print('Transit label is {}, found {} elements'.format(transit_label, transit_number))\n",
        "\n",
        "num_data = len(imu_sensor)\n",
        "print('Old num data: {}'.format(num_data))\n",
        "imu_sensor = imu_sensor[activities != transit_label]\n",
        "attitude_mat = attitude_mat[activities != transit_label]\n",
        "activities = activities[activities != transit_label]\n",
        "\n",
        "assert(imu_sensor.shape[0] == num_data-transit_number)\n",
        "assert(attitude_mat.shape[0] == num_data-transit_number)\n",
        "assert(activities.shape[0] == num_data-transit_number)\n",
        "\n",
        "num_data = len(imu_sensor)\n",
        "print('New num data: {}'.format(num_data))\n",
        "\n",
        "num_labels = len(np.unique(activities))\n",
        "print('New num labels: {}'.format(num_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ngv9Uwd8jo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert from sensor frame to body frame\n",
        "imu_body = imu_sensor.copy()\n",
        "for i, imu in enumerate(imu_sensor):\n",
        "    imu_body[i] = convert_body_frame(imu, attitude_mat[i])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gkhe3BMd-Se",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove time column from data\n",
        "imu_sensor = imu_sensor[:,1:]\n",
        "imu_body = imu_body[:,1:]\n",
        "attitude_mat = attitude_mat[:,1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGxV0TGtLgYm",
        "colab_type": "text"
      },
      "source": [
        "Performing some checks:\n",
        "\n",
        "- Displaying minutes for each activity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QlBD0dxyLhqH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "act,cnt = np.unique(activities, return_counts=True)\n",
        "cnt = cnt / 100 / 60\n",
        "act = np.array([map_decode_8[a] for a in act])\n",
        "\n",
        "act = np.append(act,'total')\n",
        "cnt = np.append(cnt, sum(cnt))\n",
        "\n",
        "df = pd.DataFrame(np.array([act,np.round(cnt)]).T, columns=['Activity','Time (min)'])\n",
        "df.index = [''] * len(df)\n",
        "display(df)\n",
        "\n",
        "#TODO other if needed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UlQj-FHruzD2",
        "colab_type": "text"
      },
      "source": [
        "Pre-compute and save a train/test split of the dataset.\n",
        "\n",
        "`random_state` is the seed of the PRNG, the percentage is set using `SPLIT_TEST_PERC` variable.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y_rP0qgZuwCM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SPLIT_TEST_PERC = 0.3\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = \\\n",
        "    train_test_split(imu_sensor, activities, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities) \n",
        "\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(len(Y_train)))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(len(Y_test)))\n",
        "\n",
        "Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "    train_test_split(imu_body, activities, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities) \n",
        "\n",
        "print(\"Xb_train shape: \" + str(Xb_train.shape))\n",
        "print(\"Yb_train shape: \" + str(len(Yb_train)))\n",
        "print(\"Xb_test shape:  \" + str(Xb_test.shape))\n",
        "print(\"Yb_test shape:  \" + str(len(Yb_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rtESTYRbunOG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with h5py.File('dataset/ARS.h5','w') as h5f:\n",
        "#    h5f.create_dataset('imu_sensor', data=imu_sensor)\n",
        "#    h5f.create_dataset('imu_body', data=imu_body)\n",
        "#    h5f.create_dataset('attitudes', data=attitude_mat)\n",
        "#    h5f.create_dataset('activities', data=activities)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-sensor.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "    \n",
        "with h5py.File('dataset/ARS-train-test-body.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BJbsBq-MlC_",
        "colab_type": "text"
      },
      "source": [
        "## Framing\n",
        "\n",
        "Group by activity and organize data in overlapping windows (overlapping ratio is regulated by `stride_len`). \n",
        "\n",
        "Save the resulting dataset and a corresponding split."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms44alUwMmYL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_len = 128\n",
        "stride_len = round(window_len / 2)\n",
        "\n",
        "def framing_padding(data):\n",
        "    x, y = [], []\n",
        "\n",
        "    for activity in np.unique(activities):\n",
        "        tmp = data[activities == activity]\n",
        "        if len(tmp) % stride_len != 0:\n",
        "            # append zeroes to fill the window, if necessary\n",
        "            windows_inside = math.ceil( (len(tmp) - window_len) / stride_len )\n",
        "            windows_space = windows_inside * stride_len\n",
        "            rest = windows_space - len(tmp) + window_len\n",
        "            #print('Act. {}, appending {} rows of zeros'.format(activity,rest))\n",
        "\n",
        "            tmp = np.append(tmp, np.zeros((rest,data.shape[1])), axis=0)\n",
        "            #tmp = np.append(tmp, [[0]*9]*int((math.floor((len(tmp)-window_len)/stride_len) + 1)*stride_len-len(tmp)+window_len), axis = 0)\n",
        "\n",
        "        # exlude unnecessary padded windows\n",
        "        for i in range(0, len(tmp)-window_len, stride_len):\n",
        "            x.extend([tmp[i:i+window_len]])\n",
        "            y.extend([activity])\n",
        "\n",
        "    x = np.array(x)\n",
        "    y = np.array(y)\n",
        "    assert(x.shape[0] == len(y))\n",
        "    return x,y\n",
        "\n",
        "imu_sensor_framed, activities_sensor_framed = framing_padding(imu_sensor)\n",
        "imu_body_framed, activities_body_framed = framing_padding(imu_body)\n",
        "\n",
        "assert(np.array_equal(activities_sensor_framed, activities_body_framed))\n",
        "\n",
        "print(\"IMU_sensor shape: \" + str(imu_sensor_framed.shape))\n",
        "print(\"IMU body shape:   \" + str(imu_body_framed.shape))\n",
        "print(\"Activities shape: \" + str(len(activities_sensor_framed)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXz4oqm2UpNd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = \\\n",
        "    train_test_split(imu_sensor_framed, activities_sensor_framed, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_sensor_framed) \n",
        "\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(len(Y_train)))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(len(Y_test)))\n",
        "\n",
        "Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "    train_test_split(imu_body_framed, activities_body_framed, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_body_framed) \n",
        "\n",
        "print(\"Xb_train shape: \" + str(Xb_train.shape))\n",
        "print(\"Yb_train shape: \" + str(len(Yb_train)))\n",
        "print(\"Xb_test shape:  \" + str(Xb_test.shape))\n",
        "print(\"Yb_test shape:  \" + str(len(Yb_test)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p87LKo2sNo_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#with h5py.File('dataset/ARS-framed.h5','w') as h5f:\n",
        "#    h5f.create_dataset('imu_sensor', data=imu_sensor_framed)\n",
        "#    h5f.create_dataset('imu_body',   data=imu_body_framed)\n",
        "#    h5f.create_dataset('activities', data=activities_sensor_framed)\n",
        "    #h5f.create_dataset('activities_body', data=activities_body_framed) # useless duplicate of prev item\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-sensor-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-body-framed.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6N76xO1wpQh_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# optional reload if messing up below\n",
        "#with h5py.File('dataset/ARS-framed.h5','r') as h5f:\n",
        "#    imu_sensor_framed = h5f['imu_sensor'][:]\n",
        "#    imu_body_framed = h5f['imu_body'][:]\n",
        "#    activities_sensor_framed = h5f['activities'][:]\n",
        "#    activities_body_framed = activities_sensor_framed.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAdjXcS4EsfV",
        "colab_type": "text"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "Data normalization is performed using training set's mean and std.\n",
        "\n",
        "Improves accuracy results: to be applied upon latest splits."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTMjk3xREzmo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    tmp_test = X_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    X_train[:,:,i] = np.reshape((tmp_train - mean)/std, (X_train.shape[0], X_train.shape[1]))\n",
        "    X_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (X_test.shape[0], X_test.shape[1]))\n",
        "\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    tmp_test = Xb_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    Xb_train[:,:,i] = np.reshape((tmp_train - mean)/std, (Xb_train.shape[0], Xb_train.shape[1]))\n",
        "    Xb_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (Xb_test.shape[0], Xb_test.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfCO8OWYEzYd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with h5py.File('dataset/ARS-train-test-sensor-framed-norm.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File('dataset/ARS-train-test-body-framed-norm.h5','w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97qj75NLE81t",
        "colab_type": "text"
      },
      "source": [
        "Checks: display mean and std for each class in training (should be near 0 and 1)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg_lJqrkEzIk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')\n",
        "\n",
        "print()\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnRFt7ML-WW-",
        "colab_type": "text"
      },
      "source": [
        "## Augmentation\n",
        "\n",
        "Data augmentation is performed using the Adaptive Syntetic (ADASYN) sampling algorithm, since this dataset is not balanced.\n",
        "\n",
        "More information on the algorithm can be found in the paper [(link)](https://ieeexplore.ieee.org/document/4633969): \n",
        " \n",
        "    Haibo He, Yang Bai, Edwardo A. Garcia, and Shutao Li. \u201cADASYN: Adaptive synthetic sampling approach for imbalanced learning,\u201d In IEEE International Joint Conference on Neural Networks, pp. 1322-1328, 2008.\n",
        "\n",
        "We use the implementation included in the [imbalanced-learn](https://imbalanced-learn.readthedocs.io) Python package.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YQ5CznScAo5V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADASYN accepts 2d arrays, so we squeeze last two dimensions\n",
        "imu_sensor_2d = imu_sensor_framed.reshape((imu_sensor_framed.shape[0], imu_sensor_framed.shape[2]*imu_sensor_framed.shape[1]))\n",
        "imu_body_2d = imu_body_framed.reshape((imu_body_framed.shape[0], imu_body_framed.shape[2]*imu_body_framed.shape[1]))\n",
        "\n",
        "print(imu_sensor_framed.shape, end=' > ')\n",
        "print(imu_sensor_2d.shape)\n",
        "print(imu_body_framed.shape, end=' > ')\n",
        "print(imu_body_2d.shape)\n",
        "\n",
        "# to have comparable results to manual augmentation written below, try to keep proportions between classes\n",
        "\"\"\"\n",
        "Class 0: 1409 entries, 2552 values to add, totalling 3961.\n",
        "Class 1: 6762 entries, OK\n",
        "Class 2: 709 entries, 3252 values to add, totalling 3961.\n",
        "Class 3: 11319 entries, OK\n",
        "Class 4: 5518 entries, OK\n",
        "Class 5: 2644 entries, 1317 values to add, totalling 3961.\n",
        "Class 6: 184 entries, 3777 values to add, totalling 3961.\n",
        "\"\"\"\n",
        "\n",
        "sample_num_dict = {\n",
        "    0: 3961,\n",
        "    1: 6762,  # not augmented\n",
        "    2: 3961,\n",
        "    3: 11319, # not augmented\n",
        "    4: 5518,  # not augmented\n",
        "    5: 3961,\n",
        "    6: 3961\n",
        "}\n",
        "\n",
        "ada = ADASYN(random_state=8, n_jobs=2, sampling_strategy=sample_num_dict)\n",
        "imu_sensor_framed_aug, activities_sensor_framed_aug = ada.fit_resample(imu_sensor_2d, activities_sensor_framed)\n",
        "print(f'Resampled composition: {Counter(activities_sensor_framed_aug)}')\n",
        "\n",
        "ada = ADASYN(random_state=8, n_jobs=2, sampling_strategy=sample_num_dict)\n",
        "imu_body_framed_aug, activities_body_framed_aug = ada.fit_resample(imu_body_2d, activities_body_framed)\n",
        "print(f'Resampled composition: {Counter(activities_body_framed_aug)}')\n",
        "\n",
        "# back to three dimensions arrays\n",
        "imu_sensor_framed_aug = imu_sensor_framed_aug.reshape((-1,imu_sensor_framed.shape[1],imu_sensor_framed.shape[2]))\n",
        "imu_body_framed_aug = imu_body_framed_aug.reshape((-1,imu_body_framed.shape[1],imu_body_framed.shape[2]))\n",
        "\n",
        "print(imu_sensor_framed_aug.shape)\n",
        "print(imu_body_framed_aug.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFXa3ChGBzw-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = \\\n",
        "    train_test_split(imu_sensor_framed_aug, activities_sensor_framed_aug, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_sensor_framed_aug)\n",
        "Xb_train, Xb_test, Yb_train, Yb_test = \\\n",
        "    train_test_split(imu_body_framed_aug,   activities_body_framed_aug,   test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_body_framed_aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9RN0OE2B1fb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "name = 'dataset/ARS-train-test-{}-framed-aug.h5'\n",
        "\n",
        "with h5py.File(name.format('sensor'),'w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File(name.format('body'),'w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XfxLG8Rr7f83",
        "colab_type": "text"
      },
      "source": [
        "## Manual Augmentation\n",
        "\n",
        "Instead of relying on an algorithm to augment data, this can be performed manually:\n",
        "\n",
        "- By applying a rotation of a random angle against a random axis (_not_ applied to classes _standing_ and _lying_, worse performance), \n",
        "- By permutation of data in a window.\n",
        "\n",
        "One or both operations may be carried out.\n",
        "\n",
        "To enable and configure the feature set the global switches below accordingly (or skip executing all next cells)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwzzSIqi1hy1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "augment = True\n",
        "rotation  = True\n",
        "permutation = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbNIguAc7luF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def rotate_random(data):\n",
        "    # axis and angle of the rotation\n",
        "    axis = np.random.uniform(low=-1, high=1, size = 3)\n",
        "    angle = np.random.uniform(low=-np.pi, high=np.pi)\n",
        "        \n",
        "    res = data.copy()\n",
        "    for i in range(0,len(data),3):\n",
        "        res[i:i+3] = np.matmul(data[i:i+3], axangle2mat(axis,angle))\n",
        "    return res\n",
        "\n",
        "def rotate(values):\n",
        "    for i in range(values.shape[0]):\n",
        "        for j in range(values.shape[1]):\n",
        "            values[i,j] = rotate_random(values[i,j])\n",
        "    return values\n",
        "\n",
        "def chunks(l, n):\n",
        "    for i in range(0, len(l), n):\n",
        "        yield l[i:i+n]\n",
        "        \n",
        "def permute(values):\n",
        "    values = list(chunks(values,8))  #8*16=128\n",
        "    random.shuffle(values)\n",
        "    values = [i for crop_seq in values for i in crop_seq]\n",
        "    return np.array(values)\n",
        "   \n",
        "def augmenting(x, y, rotation=True, permutation=True):\n",
        "    np.random.seed(8) # to have the same choices for both sensor and body-related datasets\n",
        "    augm_factor = 0.35\n",
        "\n",
        "    count = []\n",
        "    for activity in np.unique(y):\n",
        "        count.append(len(x[y == activity]))\n",
        "    percentage_before = count/np.sum(count,axis=0)\n",
        "    most_repr = int(max(count)*augm_factor)\n",
        "\n",
        "    x_add = []\n",
        "    y_add = []\n",
        "    for activity in range(len(count)):\n",
        "        augment_number = most_repr - count[activity]\n",
        "        if augment_number > 0:\n",
        "            print(f\"Class {activity}: {count[activity]} entries, {augment_number} values to add, totalling {count[activity]+augment_number}.\")\n",
        "            values = x[np.array(y) == activity][np.random.choice(range(count[activity]),size=augment_number)]\n",
        "            if rotation and activity not in [4,5]: # exclude sitting, lying from random rotations\n",
        "                values = rotate(values)\n",
        "            if permutation:\n",
        "                values = permute(values)\n",
        "            x_add.extend(values)\n",
        "            y_add.extend([activity]*values.shape[0])\n",
        "        else:\n",
        "            print(f\"Class {activity}: {count[activity]} entries, OK\")\n",
        "    return np.array(x_add), np.array(y_add)\n",
        "\n",
        "if augment:\n",
        "    # get data to add\n",
        "    imu_sensor_toadd, activities_sensor_toadd = augmenting(imu_sensor_framed, activities_sensor_framed, rotation, permutation)\n",
        "    print()\n",
        "    imu_body_toadd, activities_body_toadd = augmenting(imu_body_framed, activities_body_framed, rotation, permutation)\n",
        "\n",
        "    # add to existing data\n",
        "    imu_sensor_framed_aug = np.concatenate((imu_sensor_framed, imu_sensor_toadd), axis=0)\n",
        "    imu_body_framed_aug   = np.concatenate((imu_body_framed,   imu_body_toadd),   axis=0)\n",
        "    activities_sensor_framed_aug = np.concatenate((activities_sensor_framed, activities_sensor_toadd), axis=0)\n",
        "    activities_body_framed_aug   = np.concatenate((activities_body_framed,   activities_body_toadd),   axis=0)\n",
        "\n",
        "    assert(len(imu_sensor_framed_aug) == len(imu_sensor_framed) + len(imu_sensor_toadd))\n",
        "    assert(len(imu_body_framed_aug)   == len(imu_body_framed)   + len(imu_body_toadd))\n",
        "    assert(len(activities_sensor_framed_aug) == len(activities_sensor_framed) + len(activities_sensor_toadd))\n",
        "    assert(len(activities_body_framed_aug)   == len(activities_body_framed)   + len(activities_body_toadd))\n",
        "    \n",
        "    print(f'Resampled composition: {Counter(activities_sensor_framed_aug)}')\n",
        "    print(f'Resampled composition: {Counter(activities_body_framed_aug)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzSKlKFG7osu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if augment:\n",
        "    X_trainM, X_testM, Y_trainM, Y_testM = \\\n",
        "        train_test_split(imu_sensor_framed_aug, activities_sensor_framed_aug, test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_sensor_framed_aug)\n",
        "    Xb_trainM, Xb_testM, Yb_trainM, Yb_testM = \\\n",
        "        train_test_split(imu_body_framed_aug,   activities_body_framed_aug,   test_size=SPLIT_TEST_PERC, random_state=1, stratify=activities_body_framed_aug)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLgAC_t-7rHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if augment:\n",
        "    name = 'dataset/ARS-train-test-{}-framed-aug'\n",
        "    if rotation:    name += '-rot'\n",
        "    if permutation: name += '-per'\n",
        "    name += '.h5'\n",
        "    \n",
        "    with h5py.File(name.format('sensor'),'w') as h5f:\n",
        "        h5f.create_dataset('X_train', data=X_trainM)\n",
        "        h5f.create_dataset('X_test',  data=X_testM)\n",
        "        h5f.create_dataset('Y_train', data=Y_trainM)\n",
        "        h5f.create_dataset('Y_test',  data=Y_testM)\n",
        "\n",
        "    with h5py.File(name.format('body'),'w') as h5f:\n",
        "        h5f.create_dataset('X_train', data=Xb_trainM)\n",
        "        h5f.create_dataset('X_test',  data=Xb_testM)\n",
        "        h5f.create_dataset('Y_train', data=Yb_trainM)\n",
        "        h5f.create_dataset('Y_test',  data=Yb_testM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fQjCrfvvQkut",
        "colab_type": "text"
      },
      "source": [
        "## Normalization\n",
        "\n",
        "Reapplied to augmented data in the same way it was done before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5kD60s-HQmUI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADASYN part\n",
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    tmp_test = X_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    X_train[:,:,i] = np.reshape((tmp_train - mean)/std, (X_train.shape[0], X_train.shape[1]))\n",
        "    X_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (X_test.shape[0], X_test.shape[1]))\n",
        "\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    tmp_test = Xb_test[:,:,i].flatten()\n",
        "    mean = np.mean(tmp_train)\n",
        "    std = np.std(tmp_train)\n",
        "    Xb_train[:,:,i] = np.reshape((tmp_train - mean)/std, (Xb_train.shape[0], Xb_train.shape[1]))\n",
        "    Xb_test[:,:,i] =  np.reshape((tmp_test - mean)/std, (Xb_test.shape[0], Xb_test.shape[1]))\n",
        "\n",
        "# Manual part\n",
        "if augment:\n",
        "    for i in range(X_trainM.shape[-1]):\n",
        "        tmp_train = X_trainM[:,:,i].flatten()\n",
        "        tmp_test = X_testM[:,:,i].flatten()\n",
        "        mean = np.mean(tmp_train)\n",
        "        std = np.std(tmp_train)\n",
        "        X_trainM[:,:,i] = np.reshape((tmp_train - mean)/std, (X_trainM.shape[0], X_trainM.shape[1]))\n",
        "        X_testM[:,:,i] =  np.reshape((tmp_test - mean)/std, (X_testM.shape[0], X_testM.shape[1]))\n",
        "\n",
        "    for i in range(Xb_trainM.shape[-1]):\n",
        "        tmp_train = Xb_trainM[:,:,i].flatten()\n",
        "        tmp_test = Xb_testM[:,:,i].flatten()\n",
        "        mean = np.mean(tmp_train)\n",
        "        std = np.std(tmp_train)\n",
        "        Xb_trainM[:,:,i] = np.reshape((tmp_train - mean)/std, (Xb_trainM.shape[0], Xb_trainM.shape[1]))\n",
        "        Xb_testM[:,:,i] =  np.reshape((tmp_test - mean)/std, (Xb_testM.shape[0], Xb_testM.shape[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qj3CHzJCMbZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ADASYN part\n",
        "name = 'dataset/ARS-train-test-{}-framed-aug-norm.h5'\n",
        "\n",
        "with h5py.File(name.format('sensor'),'w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=X_train)\n",
        "    h5f.create_dataset('X_test',  data=X_test)\n",
        "    h5f.create_dataset('Y_train', data=Y_train)\n",
        "    h5f.create_dataset('Y_test',  data=Y_test)\n",
        "\n",
        "with h5py.File(name.format('body'),'w') as h5f:\n",
        "    h5f.create_dataset('X_train', data=Xb_train)\n",
        "    h5f.create_dataset('X_test',  data=Xb_test)\n",
        "    h5f.create_dataset('Y_train', data=Yb_train)\n",
        "    h5f.create_dataset('Y_test',  data=Yb_test)\n",
        "\n",
        "# Manual part\n",
        "if augment:\n",
        "    name = 'dataset/ARS-train-test-{}-framed-aug'\n",
        "    if augment and rotation:    name += '-rot'\n",
        "    if augment and permutation: name += '-per'\n",
        "    name += '-norm.h5'\n",
        "\n",
        "    with h5py.File(name.format('sensor'),'w') as h5f:\n",
        "        h5f.create_dataset('X_train', data=X_trainM)\n",
        "        h5f.create_dataset('X_test',  data=X_testM)\n",
        "        h5f.create_dataset('Y_train', data=Y_trainM)\n",
        "        h5f.create_dataset('Y_test',  data=Y_testM)\n",
        "\n",
        "    with h5py.File(name.format('body'),'w') as h5f:\n",
        "        h5f.create_dataset('X_train', data=Xb_trainM)\n",
        "        h5f.create_dataset('X_test',  data=Xb_testM)\n",
        "        h5f.create_dataset('Y_train', data=Yb_trainM)\n",
        "        h5f.create_dataset('Y_test',  data=Yb_testM)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1XpW5WzsQodc",
        "colab_type": "text"
      },
      "source": [
        "Checks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xqjGncLWQplM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(X_train.shape[-1]):\n",
        "    tmp_train = X_train[:,:,i].flatten()\n",
        "    print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')\n",
        "\n",
        "print()\n",
        "for i in range(Xb_train.shape[-1]):\n",
        "    tmp_train = Xb_train[:,:,i].flatten()\n",
        "    print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')\n",
        "    \n",
        "if augment:\n",
        "    print('\\n')\n",
        "    for i in range(X_trainM.shape[-1]):\n",
        "        tmp_train = X_trainM[:,:,i].flatten()\n",
        "        print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')\n",
        "\n",
        "    print()\n",
        "    for i in range(Xb_trainM.shape[-1]):\n",
        "        tmp_train = Xb_trainM[:,:,i].flatten()\n",
        "        print(f'{np.mean(tmp_train)}    \\t{np.std(tmp_train)}')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
