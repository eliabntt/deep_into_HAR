{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "HAR-3-SVM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "qY_Pl0khOtkt"
      },
      "source": [
        "# Going deep into Human Activity Recognition\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 3 - SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TeKKPUPPF1PO",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "f_j884TpGNrb",
        "colab": {}
      },
      "source": [
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,6)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, LSTM, GRU\n",
        "from tensorflow.keras.layers import TimeDistributed, RepeatVector, UpSampling1D, UpSampling2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "#import logging\n",
        "#logging.getLogger('tensorflow').disabled = True\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rxB85blVHppF"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Start from previously preprocessed data, altrady splitted in train and test parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xD72ldeCpHVi",
        "colab": {}
      },
      "source": [
        "map_decode = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling'\n",
        "}\n",
        "num_classes = len(map_decode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaCisoY-EW7t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval(model, x_test, y_test, y_test_orig, model_suffix, out_folder='./output', num_classes=7):\n",
        "    print(f\"\\nEvaluation of {model_suffix}\")\n",
        "    metrics = {}\n",
        "    \n",
        "    # get predictions\n",
        "    preds = model.predict(x_test)\n",
        "    y_pred = preds #np.argmax(preds)\n",
        "\n",
        "    classes_num = list(map(str,range(num_classes))) # classes list as str integers\n",
        "    classes = list(map_decode.values())\n",
        "    metrics['classes'] = classes\n",
        "\n",
        "    # build per-class metrics and confusion matrix\n",
        "    cr = classification_report(y_test_orig, y_pred, output_dict=True)\n",
        "\n",
        "    cm = confusion_matrix(y_test_orig, y_pred)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalization\n",
        "\n",
        "    acc_class = [cm[i,i] for i in range(num_classes)]\n",
        "    prc_class = [cr[cl]['precision'] for cl in cr if cl in classes_num] # exclude avgs\n",
        "    rec_class = [cr[cl]['recall']    for cl in cr if cl in classes_num]\n",
        "    f1_class  = [cr[cl]['f1-score']  for cl in cr if cl in classes_num]\n",
        "\n",
        "    metrics['acc-class'] = acc_class\n",
        "    metrics['precision-class'] = prc_class\n",
        "    metrics['recall-class'] = rec_class\n",
        "    metrics['f1-class'] = f1_class\n",
        "    metrics['averages'] = cr['macro avg']\n",
        "    metrics['weighted-averages'] = cr['weighted avg']\n",
        "    del metrics['averages']['support']\n",
        "    del metrics['weighted-averages']['support']\n",
        "    print()\n",
        "    pprint(metrics)\n",
        "\n",
        "    # conversion to pure python float before saving to json\n",
        "    for item in metrics:\n",
        "        if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "            metrics[item] = float(metrics[item])\n",
        "\n",
        "    # save evaluation dict, confusion matrix and its plot\n",
        "    os.makedirs(os.path.join(out_folder,model_suffix), exist_ok=True)\n",
        "    with open(os.path.join(out_folder, model_suffix, f\"evaluation-{model_suffix}.json\"),'w') as efile:\n",
        "        json.dump(metrics, efile, indent=2)\n",
        "\n",
        "    np.save(os.path.join(out_folder, model_suffix, f\"confusion-{model_suffix}.npy\"), cm)\n",
        "\n",
        "    plt.figure()\n",
        "    sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.ylabel('True class')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, model_suffix, f\"plot-confusion-{model_suffix}\")\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xrMJXPD3EW71",
        "colab_type": "code",
        "outputId": "b3d76543-88c8-4d56-eb6a-29172084ec30",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.metrics import f1_score, hinge_loss,confusion_matrix\n",
        "import parfit.parfit as pf\n",
        "\n",
        "dic = {\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-rot-per-norm.h5':'SAHC', # manual aug  \n",
        "    'ARS-train-test-body-framed-aug-onlytrain-rot-per-norm.h5':'BAHC',   # manual aug  \n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-norm.h5':'SADA',         # adasyn\n",
        "    'ARS-train-test-sensor-framed-norm.h5':'SNOR',                       # not augmented\n",
        "    'ARS-train-test-body-framed-norm.h5':'BNOR',                         # not augmented\n",
        "    'ARS-train-test-sensor-framed.h5':'SFRA'                             # not normalized\n",
        "}\n",
        "\n",
        "for i in dic.keys():\n",
        "    print(\"---------------------------------\")\n",
        "    print(i)\n",
        "    print(\"---------------------------------\")\n",
        "    with h5py.File(f'dataset/{i}','r') as h5f:\n",
        "        X_train = h5f['X_train'][:] # IMU data w.r.t body frame\n",
        "        X_test  = h5f['X_test'][:]  # activities (labels)\n",
        "        Y_train = h5f['Y_train'][:]\n",
        "        Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "    num_data = len(X_train)\n",
        "\n",
        "    # categorical structures are needed for the loss function to work properly\n",
        "    # original test classes are needed for prediction steps\n",
        "    Y_test_orig  = Y_test.copy()\n",
        "    Y_train_orig = Y_train.copy()\n",
        "    Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "    Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8)\n",
        "\n",
        "    x_tr = X_train.reshape((X_train.shape[0], X_train.shape[2]*X_train.shape[1]))\n",
        "    x_te = X_test.reshape((X_test.shape[0], X_test.shape[2]*X_test.shape[1]))\n",
        "    y_tr = X_train.reshape((X_train.shape[0], X_train.shape[2]*X_train.shape[1]))\n",
        "    y_te = X_test.reshape((X_test.shape[0], X_test.shape[2]*X_test.shape[1]))\n",
        "    x_tr = x_tr.reshape(-1,1,9)\n",
        "    x_te = x_te.reshape(-1,1,9)\n",
        "    y_tr = y_tr.reshape(-1,1,9)\n",
        "    y_te = x_te.reshape(-1,1,9)\n",
        "    input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "    \n",
        "    from tensorflow.keras.models import Model, Sequential, load_model\n",
        "    for j in ['cnn','lstm','mixed']:\n",
        "        print(f\"-----------{j}------------\")\n",
        "        if not os.path.exists(f'/home/eliab/Desktop/{dic[i]}/model-best-a-{j}.h5'):\n",
        "            continue\n",
        "        model = load_model(f'/home/eliab/Desktop/{dic[i]}/model-best-a-{j}.h5')\n",
        "\n",
        "        DL_input = Input(input_shape)\n",
        "        DL_model = DL_input\n",
        "        for layer in model.layers[:2]:\n",
        "            DL_model = layer(DL_model)\n",
        "        DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "        for layer in DL_model.layers:\n",
        "            layer.trainable = False\n",
        "        data = DL_model.predict(x_tr, verbose = 0)\n",
        "        data = data.reshape(X_train.shape[0],-1)\n",
        "        data_te = DL_model.predict(x_te, verbose = 0)\n",
        "        data_te = data_te.reshape(X_test.shape[0], -1)\n",
        "        \n",
        "        print(\"SVM-l2\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
        "              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "              verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'SVM-l2-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "        print(\"SVM-l1\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "            early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "            l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
        "            n_iter_no_change=5, n_jobs=-1, penalty='l1', power_t=0.5,\n",
        "            random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "            verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'SVM-l1-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "        print(\"SVM-elasticnet\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "              l1_ratio=0.15, learning_rate='optimal', loss='hinge', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=-1, penalty='elasticnet', power_t=0.5,\n",
        "              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "              verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'SVM-elasticnet-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "        print(\"Log-l2\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=-1, penalty='l2', power_t=0.5,\n",
        "              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "              verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'Log-l2-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "        print(\"Log-l1\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=-1, penalty='l1', power_t=0.5,\n",
        "              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "              verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'Log-l1-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "        print(\"Log-elasticnet\")\n",
        "        clf = SGDClassifier(alpha=0.0001, average=False, class_weight=None,\n",
        "              early_stopping=False, epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
        "              l1_ratio=0.15, learning_rate='optimal', loss='log', max_iter=1000,\n",
        "              n_iter_no_change=5, n_jobs=-1, penalty='elasticnet', power_t=0.5,\n",
        "              random_state=42, shuffle=True, tol=0.001, validation_fraction=0.1,\n",
        "              verbose=0, warm_start=False)\n",
        "        clf.fit(data, Y_train_orig) \n",
        "        print(clf.score(data, Y_train_orig))\n",
        "        print(clf.score(data_te, Y_test_orig))\n",
        "        eval(clf, data_te, Y_test, Y_test_orig, model_suffix=f'Log-elasticnet-{dic[i]}-{j}')\n",
        "        print(\"\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\")\n",
        "        \n",
        "    print(\"---------------------------------\")\n",
        "    print(\"-------------END-----------------\")\n",
        "    print(\"---------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------\n",
            "ARS-train-test-sensor-framed-aug-onlytrain-rot-per-norm.h5\n",
            "---------------------------------\n",
            "-----------cnn------------\n",
            "SVM-l2\n",
            "0.8287638984462714\n",
            "0.7880663241475946\n",
            "\n",
            "Evaluation of SVM-l2-SAHC-cnn\n",
            "\n",
            "{'acc-class': [0.8226950354609929,\n",
            "               0.9127649088220798,\n",
            "               0.25821596244131456,\n",
            "               0.917550058892815,\n",
            "               0.35468277945619336,\n",
            "               0.9886506935687264,\n",
            "               0.12727272727272726],\n",
            " 'averages': {'f1-score': 0.6326900784993125,\n",
            "              'precision': 0.6874029248656205,\n",
            "              'recall': 0.6259760237021214},\n",
            " 'classes': ['running',\n",
            "             'walking',\n",
            "             'jumping',\n",
            "             'standing',\n",
            "             'sitting',\n",
            "             'lying',\n",
            "             'falling'],\n",
            " 'f1-class': [0.760655737704918,\n",
            "              0.8717345257707696,\n",
            "              0.35143769968051125,\n",
            "              0.82086406743941,\n",
            "              0.4970364098221846,\n",
            "              0.9781659388646288,\n",
            "              0.14893617021276595],\n",
            " 'precision-class': [0.7073170731707317,\n",
            "                     0.8342342342342343,\n",
            "                     0.55,\n",
            "                     0.7426120114394662,\n",
            "                     0.8302687411598303,\n",
            "                     0.9679012345679012,\n",
            "                     0.1794871794871795],\n",
            " 'recall-class': [0.8226950354609929,\n",
            "                  0.9127649088220798,\n",
            "                  0.25821596244131456,\n",
            "                  0.917550058892815,\n",
            "                  0.35468277945619336,\n",
            "                  0.9886506935687264,\n",
            "                  0.12727272727272726],\n",
            " 'weighted-averages': {'f1-score': 0.7659376074300941,\n",
            "                       'precision': 0.7919697588235133,\n",
            "                       'recall': 0.7880663241475946}}\n",
            "\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n",
            "SVM-l1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K6QhHvuREW8D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
