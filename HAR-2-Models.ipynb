{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HAR-2-Models.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qY_Pl0khOtkt",
        "colab_type": "text"
      },
      "source": [
        "# Going deep into Human Activity Recognition\n",
        "\n",
        "**Elia Bonetto, Filippo Rigotto.**\n",
        "\n",
        "Department of Information Engineering, University of Padova, Italy.\n",
        "\n",
        "Human Data Analytics, a.y. 2018/2019\n",
        "\n",
        "## Part 2 - Training of DL models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_swJwzcxFYUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!nvidia-smi # watch for T4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TeKKPUPPF1PO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import Image, clear_output\n",
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "clear_output()\n",
        "os.chdir(\"/content/drive/My Drive/hda-project\")\n",
        "#!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_j884TpGNrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install telepot\n",
        "clear_output()\n",
        "\n",
        "from pprint import pprint\n",
        "import json\n",
        "from datetime import datetime\n",
        "import pytz\n",
        "\n",
        "import math\n",
        "import h5py\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import scipy.io\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.precision',3)\n",
        "pd.set_option('display.float_format', '{:0.3f}'.format)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline\n",
        "mpl.rcParams['figure.figsize'] = (10,6)\n",
        "mpl.rcParams['axes.grid'] = True\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Input, Dense, Activation, BatchNormalization, Flatten, Dropout\n",
        "from tensorflow.keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, LSTM, GRU, Bidirectional\n",
        "from tensorflow.keras.layers import Bidirectional, TimeDistributed, RepeatVector, UpSampling1D, UpSampling2D, ZeroPadding2D\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.models import Model, Sequential, load_model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, LambdaCallback\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "from tg_callback import TelegramCallback\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "K.set_image_data_format('channels_last')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxB85blVHppF",
        "colab_type": "text"
      },
      "source": [
        "## Data loading\n",
        "\n",
        "Start from previously preprocessed data, altrady splitted in train and test parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xD72ldeCpHVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "map_decode = {\n",
        "    0: 'running',\n",
        "    1: 'walking',\n",
        "    2: 'jumping',\n",
        "    3: 'standing',\n",
        "    4: 'sitting',\n",
        "    5: 'lying',\n",
        "    6: 'falling'\n",
        "}\n",
        "num_classes = len(map_decode)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCJYO52hC3cS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this dataset will be used for all preliminary tests below\n",
        "with h5py.File('dataset/ARS-train-test-body-framed-aug-onlytrain-rot-per-norm.h5','r') as h5f:\n",
        "    X_train = h5f['X_train'][:] # IMU data w.r.t body frame\n",
        "    X_test  = h5f['X_test'][:]  # activities (labels)\n",
        "    Y_train = h5f['Y_train'][:]\n",
        "    Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "num_data = len(X_train)\n",
        "print(\"X_train shape: \" + str(X_train.shape))\n",
        "print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "print(\"X_test shape:  \" + str(X_test.shape))\n",
        "print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "# categorical structures are needed for the loss function to work properly\n",
        "# original test classes are needed for prediction steps\n",
        "Y_train_orig = Y_train.copy()\n",
        "Y_test_orig  = Y_test.copy()\n",
        "Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Lxhfx_7Hw4u",
        "colab_type": "text"
      },
      "source": [
        "## Training and evaluation\n",
        "\n",
        "Precision, recall and F1 score are implemented referring to Tensorflow backend and are passed as custom metrics to track during training and evaluation of models.\n",
        "\n",
        "Other than monitoring accuracy, we further define three metrics:\n",
        "\n",
        "$m_2 = \\frac{accuracy}{loss} \\qquad m_3 = accuracy+precision+recall \\qquad m_4 = \\frac{accuracy+precision+recall}{loss}$\n",
        "\n",
        "All of these are referred to test-set values, and models are saved when these quantities reach a new maximum.\n",
        "\n",
        "The `run_model` function takes care of bootstrap, training and evaluation processes for a given Keras model and configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttpa4dexi1Rw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def recall(y_true, y_pred):\n",
        "    \"\"\"Recall metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    \"\"\"Precision metric, batch-wise average.\"\"\"\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "def f1(y_true, y_pred):\n",
        "    \"\"\"F1 score, based on precision and recall metrics.\"\"\"\n",
        "    prc = precision(y_true, y_pred)\n",
        "    rec = recall(y_true, y_pred)\n",
        "    return 2*((prc*rec)/(prc+rec+K.epsilon()))\n",
        "\n",
        "def save_custom(epoch, logs):\n",
        "    \"\"\"Callback method to save models on metrics change.\"\"\"\n",
        "    global output_dir\n",
        "    global max_accu\n",
        "    global min_loss\n",
        "    global max_accu_over_loss\n",
        "    global max_sum_apr\n",
        "    global max_sum_aprol\n",
        "    global epochs\n",
        "    global is_ae\n",
        "    \n",
        "    vaccu = logs['val_acc']\n",
        "    vloss = logs['val_loss']\n",
        "    if not is_ae:\n",
        "        vprec = logs['val_precision']\n",
        "        vrec  = logs['val_recall']\n",
        "\n",
        "    if not is_ae:\n",
        "        if (vaccu + vprec + vrec) > max_sum_apr:\n",
        "            max_sum_apr = (vaccu + vprec + vrec)\n",
        "            model.save(os.path.join(out_folder, 'model-best-apr.h5'))\n",
        "            epochs['apr'] = epoch # save to log\n",
        "        \n",
        "        if (vaccu + vprec + vrec) / vloss > max_sum_aprol:\n",
        "            max_sum_aprol = (vaccu + vprec + vrec) / vloss\n",
        "            model.save(os.path.join(out_folder, 'model-best-aprol.h5'))\n",
        "            epochs['aprol'] = epoch\n",
        "\n",
        "    if vaccu < max_accu and vaccu > max_accu - 0.01 and vloss < min_loss:\n",
        "        model.save(os.path.join(out_folder, 'model-best-al.h5'))\n",
        "        epochs['al'] = epoch\n",
        "    \n",
        "    if vaccu / vloss > max_accu_over_loss:\n",
        "        max_accu_over_loss = vaccu/vloss\n",
        "        model.save(os.path.join(out_folder, 'model-best-aol.h5'))\n",
        "        epochs['aol'] = epoch\n",
        "\n",
        "    if vaccu > max_accu:\n",
        "        max_accu = vaccu\n",
        "        model.save(os.path.join(out_folder, 'model-best-a.h5'))\n",
        "        epochs['a'] = epoch\n",
        "    if vloss < min_loss:\n",
        "        min_loss = vloss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B4im4NdAR-Wf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def run_model(model, config, \n",
        "              x_train = X_train, y_train = Y_train, \n",
        "              x_test = X_test, y_test = Y_test, y_test_orig = Y_test_orig, is_autoencoder=False, folder_suffix=None):\n",
        "    \"\"\"Generic method to build a model, train and evaluate performance.\"\"\"\n",
        "\n",
        "    global out_folder\n",
        "    out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+ (('_'+folder_suffix) if folder_suffix else '') +'_'+model.name)\n",
        "    if not os.path.exists(out_folder):\n",
        "        os.mkdir(out_folder)\n",
        "    \n",
        "    # print and save model summary\n",
        "    print('Summary')\n",
        "    model.summary(line_length=100)\n",
        "    with open(os.path.join(out_folder, 'summary.txt'),'w') as sfile:\n",
        "        model.summary(line_length=100, print_fn=lambda x: sfile.write(x+'\\n'))\n",
        "    plot_model(model, to_file=os.path.join(out_folder, 'model.png'), show_shapes=True)\n",
        "\n",
        "    # save config\n",
        "    with open(os.path.join(out_folder, 'config.json'),'w') as cfile:\n",
        "        json.dump(config, cfile, indent=2)\n",
        "\n",
        "\n",
        "    if 'lr' not in config:\n",
        "        # use default values: ass the string\n",
        "        opt = config['optimizer']\n",
        "    else:\n",
        "        # setup optimizer with supplied parameters\n",
        "        if 'sgdm' in config['optimizer']:\n",
        "            opt = SGD(lr=config['lr'], momentum=config['momentum'], decay=config['decay'])\n",
        "\n",
        "        elif 'sgd' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = SGD(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = SGD(lr=config['lr'])\n",
        "\n",
        "        elif 'adam' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = Adam(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = Adam(lr=config['lr'])\n",
        "\n",
        "        elif 'rmsprop' in config['optimizer']:\n",
        "            if 'decay' in config:\n",
        "                opt = RMSprop(lr=config['lr'], decay=config['decay'])\n",
        "            else:\n",
        "                opt = RMSprop(lr=config['lr'])\n",
        "\n",
        "\n",
        "    # compile model\n",
        "    if is_autoencoder:\n",
        "        model.compile(optimizer=opt,\n",
        "                      loss=config['loss'],\n",
        "                      metrics=['accuracy'])\n",
        "    else:\n",
        "        model.compile(optimizer=opt,\n",
        "                  loss=config['loss'],\n",
        "                  metrics=['accuracy', precision, recall, f1])\n",
        "\n",
        "    # add requested callbacks for model, starting from checkpointing\n",
        "    callbacks = [ LambdaCallback(on_epoch_end=save_custom) ]\n",
        "\n",
        "    if config['lr_step'] > 0:\n",
        "        # halves lr every lr_step epochs (starting lr = 0.01)\n",
        "        callbacks.append(LearningRateScheduler(\n",
        "            lambda epoch: 0.01 * math.pow(0.5, math.floor((1+epoch)/config['lr_step'])),\n",
        "            verbose=1))\n",
        "        \n",
        "    if config['early_stop'] > 0:\n",
        "        # stop if val_loss does has not diminished after num epochs\n",
        "        callbacks.append(EarlyStopping(patience=config['early_stop']))\n",
        "        \n",
        "    if config['tg']:\n",
        "        # telegram notification when training stops\n",
        "        callbacks.append(TelegramCallback(name=model.name))\n",
        "    \n",
        "    # train model, save final state and history\n",
        "    print('\\nTraining')\n",
        "\n",
        "    # by default, use test set also as validation set\n",
        "    x_val = x_test\n",
        "    y_val = y_test    \n",
        "    if 'use_validation' in config and config['use_validation']:\n",
        "        # generate validation set excluding it from the training set\n",
        "        x_train, x_val, y_train, y_val = \\\n",
        "            train_test_split(x_train, y_train, test_size=0.2, random_state=1, stratify=y_train)     \n",
        "    \n",
        "    # global variables to save model in LambdaCallback\n",
        "    global max_accu\n",
        "    global min_loss\n",
        "    global max_accu_over_loss\n",
        "    global max_sum_apr\n",
        "    global max_sum_aprol\n",
        "    global epochs\n",
        "    global is_ae\n",
        "    is_ae = is_autoencoder\n",
        "    max_accu = 0.0\n",
        "    min_loss = 1e10\n",
        "    max_accu_over_loss = 0.0\n",
        "    max_sum_apr = 0.0\n",
        "    max_sum_aprol = 0.0\n",
        "    if is_ae:\n",
        "        epochs = {'a':0, 'al':0, 'aol':0}\n",
        "    else:\n",
        "        epochs = {'a':0, 'al':0, 'aol':0, 'apr':0, 'aprol':0}\n",
        "    \n",
        "    history = model.fit(x=x_train, y=y_train,\n",
        "                        shuffle=config['shuffle'],\n",
        "                        epochs=config['epochs'],\n",
        "                        batch_size=config['batch_size'],\n",
        "                        callbacks=callbacks, # if len(callbacks) > 0 else None,\n",
        "                        validation_data=(x_val,y_val))\n",
        "    \n",
        "    model.save(os.path.join(out_folder, 'model-final.h5'))\n",
        "    \n",
        "    with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "        hpd = pd.DataFrame(history.history)\n",
        "        json.dump(json.loads(hpd.to_json()), hfile, indent=2)\n",
        "\n",
        "    with open(os.path.join(out_folder, 'epochs.json'),'w') as hfile:\n",
        "        json.dump(epochs, hfile, indent=2)\n",
        "\n",
        "    # plot and save loss, accuracy and metrics (precision, recall, f1)\n",
        "    print('\\nLoss, accuracy and metrics plots')\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['loss'], label='Training')\n",
        "    plt.plot(history.history['val_loss'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-loss')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    plt.close()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(history.history['acc'], label='Training')\n",
        "    plt.plot(history.history['val_acc'], label='Validation')\n",
        "    plt.legend()\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.ylim((0,1))\n",
        "    plt.tight_layout()\n",
        "    fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "    plt.savefig(fname+'.png')\n",
        "    plt.savefig(fname+'.pdf', format='pdf')\n",
        "    plt.close()\n",
        "\n",
        "    if not is_ae:\n",
        "        plt.figure()\n",
        "        plt.plot(history.history['precision'], label='Precision Tr')\n",
        "        plt.plot(history.history['val_precision'], label='Precision Val')\n",
        "        plt.plot(history.history['recall'], label='Recall Tr')\n",
        "        plt.plot(history.history['val_recall'], label='Recall Val')\n",
        "        plt.plot(history.history['f1'], label='F1 Tr')\n",
        "        plt.plot(history.history['val_f1'], label='F1 Val')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Metrics')\n",
        "        plt.tight_layout()\n",
        "        fname = os.path.join(out_folder, 'plot-metrics')\n",
        "        plt.savefig(fname+'.png')\n",
        "        plt.savefig(fname+'.pdf', format='pdf')\n",
        "        plt.close()\n",
        "\n",
        "    # evaluate models, save results\n",
        "    if 'metrics_to_test' in config:\n",
        "        suffixes = ['final'] + config['metric_to_test']\n",
        "    else:\n",
        "        suffixes = ['final','best-a','best-al','best-aol','best-apr','best-aprol']\n",
        "\n",
        "    for model_suffix in suffixes:\n",
        "        model_name = os.path.join(out_folder, 'model-{}.h5'.format(model_suffix))\n",
        "        if not os.path.exists(model_name): continue\n",
        "        \n",
        "        if is_ae:\n",
        "            model = load_model(model_name)\n",
        "            print(f\"\\nEvaluation of {model_suffix}\")\n",
        "            metrics = model.evaluate(x=x_test, y=y_test)\n",
        "            metrics = dict(zip(model.metrics_names, metrics)) # build a dict adding names\n",
        "            metrics['name'] = model.name\n",
        "            metrics['type'] = model_suffix\n",
        "            \n",
        "            # conversion to pure python float before saving to json\n",
        "            for item in metrics:\n",
        "                if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "                    metrics[item] = float(metrics[item])\n",
        "            with open(os.path.join(out_folder, f\"evaluation-{model_suffix}.json\"),'w') as efile:\n",
        "                json.dump(metrics, efile, indent=2)\n",
        "            \n",
        "        else:        \n",
        "            model = load_model(model_name, custom_objects={'precision': precision, 'recall': recall, 'f1': f1})\n",
        "\n",
        "            print(f\"\\nEvaluation of {model_suffix}\")\n",
        "            metrics = model.evaluate(x=x_test, y=y_test)\n",
        "            metrics = dict(zip(model.metrics_names, metrics)) # build a dict adding names\n",
        "            metrics['name'] = model.name\n",
        "            metrics['type'] = model_suffix\n",
        "\n",
        "            # get predictions\n",
        "            preds = model.predict(x=x_test)\n",
        "            y_pred = np.argmax(preds, axis=1)\n",
        "\n",
        "            classes_num = list(map(str,range(num_classes))) # classes list as str integers\n",
        "            classes = list(map_decode.values())\n",
        "            metrics['classes'] = classes\n",
        "\n",
        "            # build per-class metrics and confusion matrix\n",
        "            cr = classification_report(y_test_orig, y_pred, output_dict=True)\n",
        "\n",
        "            cm = confusion_matrix(y_test_orig, y_pred)\n",
        "            cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] # normalization\n",
        "\n",
        "            acc_class = [cm[i,i] for i in range(num_classes)]\n",
        "            prc_class = [cr[cl]['precision'] for cl in cr if cl in classes_num] # exclude avgs\n",
        "            rec_class = [cr[cl]['recall']    for cl in cr if cl in classes_num]\n",
        "            f1_class  = [cr[cl]['f1-score']  for cl in cr if cl in classes_num]\n",
        "\n",
        "            metrics['acc-class'] = acc_class\n",
        "            metrics['precision-class'] = prc_class\n",
        "            metrics['recall-class'] = rec_class\n",
        "            metrics['f1-class'] = f1_class\n",
        "            metrics['averages'] = cr['macro avg']\n",
        "            metrics['weighted-averages'] = cr['weighted avg']\n",
        "            del metrics['averages']['support']\n",
        "            del metrics['weighted-averages']['support']\n",
        "            print()\n",
        "            pprint(metrics)\n",
        "\n",
        "            # conversion to pure python float before saving to json\n",
        "            for item in metrics:\n",
        "                if type(metrics[item]) == np.float64 or type(metrics[item]) == np.float32:\n",
        "                    metrics[item] = float(metrics[item])\n",
        "\n",
        "            # save evaluation dict, confusion matrix and its plot\n",
        "            with open(os.path.join(out_folder, f\"evaluation-{model_suffix}.json\"),'w') as efile:\n",
        "                json.dump(metrics, efile, indent=2)\n",
        "\n",
        "            np.save(os.path.join(out_folder, f\"confusion-{model_suffix}.npy\"), cm)\n",
        "\n",
        "            plt.figure()\n",
        "            sns.heatmap(cm, annot=True, cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
        "            plt.xlabel('Predicted class')\n",
        "            plt.ylabel('True class')\n",
        "            plt.tight_layout()\n",
        "            fname = os.path.join(out_folder, f\"plot-confusion-{model_suffix}\")\n",
        "            plt.savefig(fname+'.png')\n",
        "            plt.savefig(fname+'.pdf', format='pdf')\n",
        "            plt.close()\n",
        "\n",
        "    if is_ae:\n",
        "        return out_folder"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fursw-6uk3CN",
        "colab_type": "text"
      },
      "source": [
        "## Standard models\n",
        "\n",
        "Keras models for simple or standard architectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRgi0iyMk5PB",
        "colab_type": "text"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdYF3P-olQPe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TwoDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'TwoDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S6zKRmgLz3nH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ThreeDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'ThreeDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5hS5HSSWlfw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def FiveDense_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'FiveDense'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name += '-do{}'.format(dropout_rate)\n",
        "    \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Flatten(input_shape=input_shape))\n",
        "    \n",
        "    model.add(Dense(512, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(256, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(64, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(32, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    if dropout_rate: model.add(Dropout(dropout_rate))\n",
        "    \n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oBztSmjVk-Zz",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nla33q7ilHGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C1D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape, \n",
        "               kernel_regularizer=l2(l2_reg) if l2_reg else None), # shape == (batch, steps, channels)\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVhjcs2btcs7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_1C2D_model(input_shape, num_classes, l2_reg=None):\n",
        "    name = 'Conv1D-1C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, input_shape=input_shape,\n",
        "              kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        BatchNormalization(axis=1),\n",
        "        Activation('relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu', kernel_regularizer=l2(l2_reg) if l2_reg else None),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name=name)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hAIEYmtnyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# selected for report\n",
        "def Conv1D_2C1D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C1D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5, kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "703_dnMjuNGC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_2C2D_model(input_shape, num_classes, l2_reg=None, dropout_rate=None):\n",
        "    name = 'Conv1D-2C2D'\n",
        "    if l2_reg: name += '-reg{}'.format(l2_reg)\n",
        "    if dropout_rate: name +='-do{}'.format(dropout_rate)\n",
        "        \n",
        "    model = Sequential(name=name)\n",
        "    model.add(Conv1D(64, 5, input_shape=input_shape,\n",
        "                     kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Conv1D(32, 5, kernel_regularizer=l2(l2_reg) if l2_reg else None))\n",
        "    model.add(BatchNormalization(axis=1))\n",
        "    model.add(Activation('relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(MaxPooling1D(2))\n",
        "        \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(128, activation='relu'))\n",
        "    if dropout_rate:\n",
        "        model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(num_classes, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fL5eOuIUmLfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_Chen_model(input_shape, num_classes):\n",
        "    \"\"\"Chen and Xue, 'A DL approach to HAR based on single accelerometer'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv1D(18, 2, activation='relu', input_shape=input_shape), # depth, kernel\n",
        "        MaxPooling1D(2), # size, strides\n",
        "        \n",
        "        Conv1D(36, 2, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(24, 2, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-Chen')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "whtR6N9WmMYk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv1D_Rueda_model(input_shape, num_classes):\n",
        "    \"\"\"Moya Rueda et al., 'CNN for HAR using body-worn sensors'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv1D(64, 5, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        Conv1D(64, 5, activation='relu'),\n",
        "        MaxPooling1D(2),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(512, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv1D-Rueda')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hcyg_0JrNArn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv2D_Bevilacqua_model(input_shape, num_classes):\n",
        "    \"\"\"Bevilacqua et al., 'HAR with CNNs'\"\"\"\n",
        "    return Sequential([\n",
        "        ZeroPadding2D((1,2), input_shape=input_shape),\n",
        "        \n",
        "        Conv2D(10, (3,5)), # depth kernel\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((3,3),(1,1)), # size strides\n",
        "        \n",
        "        Conv2D(2, (2,4)),\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((2,2),(1,1)),\n",
        "        \n",
        "        Conv2D(2, (2,2)),\n",
        "        BatchNormalization(axis=2),\n",
        "        Activation('relu'),\n",
        "        MaxPooling2D((3,2),(1,2)),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(500, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(250, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(125, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv2D-Bevilacqua')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r629ddc2V-dl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# selected for paper\n",
        "def Conv2D_Ha_model(input_shape, num_classes):\n",
        "    \"\"\"Ha, Yun and Choi, 'Multi-modal CNN for AR'\"\"\"\n",
        "    return Sequential([\n",
        "        Conv2D(32, (4,4), activation='relu', input_shape=input_shape),\n",
        "        MaxPooling2D((3,3),(1,1)),\n",
        "        \n",
        "        Conv2D(64, (5,5), activation='relu'),\n",
        "        MaxPooling2D((3,3),(1,1)),\n",
        "        \n",
        "        Flatten(),\n",
        "        Dense(128, activation='relu'),\n",
        "        Dropout(0.5),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='Conv2D-Ha')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEHQCDtzlAs-",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZUpObK70-TSt",
        "colab": {}
      },
      "source": [
        "def TwoLSTM_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        LSTM(512, return_sequences=True,  stateful=False, batch_input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LSTM(512, return_sequences=False, stateful=False),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoLSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5S0hHzcOTPvp",
        "colab": {}
      },
      "source": [
        "def TwoGRU_model(input_shape, num_classes):\n",
        "    return Sequential([\n",
        "        GRU(512, input_shape=input_shape, return_sequences=True),\n",
        "        Dropout(0.2), #prev commented\n",
        "        GRU(512, input_shape=input_shape),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='TwoGRU')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSP3kr-xtwEF",
        "colab_type": "text"
      },
      "source": [
        "## Mixed model - CNN + LSTM\n",
        "\n",
        "Keras model made by stacking different architectures."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GAKogzG2hnQu",
        "colab": {}
      },
      "source": [
        "def CNN_LSTM_model(input_shape, num_classes):\n",
        "    filters = 256\n",
        "    LSTM_feat = 128 \n",
        "    #TimeDistributed: This wrapper applies a layer to every temporal slice of an input.\n",
        "    #The input should be at least 3D, and the dimension of index one will be considered to be the temporal dimension.\n",
        "    return Sequential([\n",
        "        TimeDistributed(Conv1D(filters=filters, kernel_size=1, activation='relu'), input_shape=input_shape),\n",
        "        TimeDistributed(Conv1D(filters=filters, kernel_size=3, activation='relu')),\n",
        "        TimeDistributed(Dropout(0.1)),\n",
        "        TimeDistributed(MaxPooling1D(pool_size=2)),\n",
        "        TimeDistributed(Flatten()),\n",
        "        LSTM(LSTM_feat),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ], name='CNN-LSTM')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1kj8Yx5qx9t",
        "colab_type": "text"
      },
      "source": [
        "## AutoEncoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4cButa7Rr6pY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', input_shape=input_shape, padding='same'),\n",
        "        Conv1D(filters=64,  kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=64,  kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', padding='same'),\n",
        "        Conv1D(filters=num_features, kernel_size=1, activation='softmax') #TODO Try with softmax\n",
        "    ], name='CNN_AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5MHyhr9q9Fgp",
        "colab": {}
      },
      "source": [
        "def LSTM_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        LSTM(128, activation='relu', input_shape=input_shape, return_sequences=True), #TODO try with 64, 32/ 128,64\n",
        "        LSTM(64,  activation='relu', return_sequences=True),\n",
        "        #LSTM(64, activation='relu', return_sequences=True),\n",
        "        LSTM(128, activation='relu', return_sequences=True),\n",
        "        #LSTM(num_features, activation='relu', return_sequences=True),\n",
        "        TimeDistributed(Dense(num_features, activation='softmax')) #TODO Try with softmax\n",
        "    ], name='LSTM-AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJkAojhi1rvi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def CNN_LSTM_AE_model(input_shape, num_features):\n",
        "    return Sequential([\n",
        "        Conv1D(filters=128, kernel_size=1, activation='relu', input_shape=input_shape),\n",
        "        Conv1D(filters=64, kernel_size=1, activation='relu'),\n",
        "        #MaxPooling1D(pool_size=1),\n",
        "        LSTM(128, activation='relu', return_sequences=True),\n",
        "        TimeDistributed(Dense(num_features, activation='softmax')) #TODO Try with softmax\n",
        "    ], name='CNN-LSTM-AE')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOuzqoxhheZu",
        "colab_type": "text"
      },
      "source": [
        "## Preliminary Tests\n",
        "\n",
        "Here each model is trained according to selected configuration on the dataset split.\n",
        "\n",
        "The goal is to have the best configuration for each network, and to select the best models for final tests.\n",
        "\n",
        "The configuration is a dictionary that allow to set the model's parameters and callbacks:\n",
        "\n",
        "- `optimizer` (str): the selected optimizer for training. One of [`sgd`, `sgdm`, `adam`, `rmsprop`].\n",
        "- `lr` (float): the learning rate. If omitted, standard values are used for the optimizer.\n",
        "- `decay` (float): learning rate decay. Valid for all optimizers with a supplied `lr`.\n",
        "- `momentum` (float): gradient momentum. Only valid for `sgdm` optimizer.\n",
        "- `loss` (str): type of loss to minimize.\n",
        "- `metrics_to_test` (arr of str): models to evaluate based on what metrics saved. Choose from [`best-al`, `best-aol`, `best-apr`, `best-aprol`]. If not present, all models are evaluated.\n",
        "\n",
        "- `epochs` (int) and `batch_size` (int).\n",
        "- `use_validation` (bool): whether to derive validation set from training set instead of using test set to validate training.\n",
        "- `shuffle` (bool): whether to shuffle data in batches or keep the same retrieval order.\n",
        "\n",
        "- `lr_step` (int): epochs after which the learning rate is halved. Set to 0 to disable.\n",
        "- `early_stop` (int): number of epochs after which to stop training if validation loss does not decrease anymore. Set to 0 to disable.\n",
        "- `tg` (bool): whether to enable Telegram notification when training finishes.\n",
        "\n",
        "Models that contain Dense or convolutional layers may use L2 regularization with the optional parameter `l2_reg` to the model function.\n",
        "\n",
        "Some models may apply dropout if `dropout_rate` is set, some have it enabled by default as part of the network structure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhSTL8Vn9WMX",
        "colab_type": "text"
      },
      "source": [
        "### Fully connected"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lu1NUZsSC017",
        "colab_type": "text"
      },
      "source": [
        "Using two dense layers leads to variable accuracy in the range [72.5%, 87.5%] with growing loss if regularization is not applied.\n",
        "\n",
        "Regularization and dropout lower accuracy values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_K68uw52ZI0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = TwoDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 60,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = TwoDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbODHqzsFeyU",
        "colab_type": "text"
      },
      "source": [
        "Adding one dense layer, there is no substantial changes: regularization stabilizes validation loss but lowers the accuracy by 5% from 90% to 85%.\n",
        "\n",
        "Dropout is not effective in bounding loss and has final accuracy similar to using regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UAj2q2r9mzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = ThreeDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 30,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = ThreeDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugRm6bzMbXuD",
        "colab_type": "text"
      },
      "source": [
        "Using five dense layers the network is too deep for the assigned task and it does not match interesting results.\n",
        "\n",
        "Network reaches 90% accuracy with visible overfit, and applying dropout or regularization lowers accuracy to 85 and 75% resp."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDDlvJqCXuE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 50,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = FiveDense_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.5)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 300,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "model = FiveDense_model(input_shape, num_classes, dropout_rate=0.5)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrXWiR979ZqM",
        "colab_type": "text"
      },
      "source": [
        "### Convolutional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1lpUevlAqBy",
        "colab_type": "text"
      },
      "source": [
        "Using 1 convolutional and 1 dense layer we obtain poor results: overfitting and growing validation loss.\n",
        "\n",
        "Accuracy between 85 and 90%. Regularization helps but not too much."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvKaBl4gUvIy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_1C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWRhxuw_BzOm",
        "colab_type": "text"
      },
      "source": [
        "Using 1 conv. and 2 dense layers, we achieve 95% accuracy. Regularization controls loss growing but lowers accuracy to 90%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GpeEIf2y6mac",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 125,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv1D_1C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv6cqQMJEIJk",
        "colab_type": "text"
      },
      "source": [
        "Two conv. layers and one final dense layer: of the four configuration, applying only dropout is the best setup and leads to 96% accuracy.\n",
        "\n",
        "Adding regularization is not helpful. Standard model reaches 94% accuracy with growing loss.\n",
        "\n",
        "_Selected as one of the best CNN models for the report._"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPJWTb8a8qBj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "# selected for report\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kq4JGMlTQJi7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this contains only the selected configuration\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 400,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)\n",
        "\n",
        "# NO validation version\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 400,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': False,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C1D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19m6MYe6Qzzs",
        "colab_type": "text"
      },
      "source": [
        "Two conv. layers and two dense layers: same conclusions as in previous setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hO6l8wwe-Jn0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200,\n",
        "    'batch_size': 32,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, dropout_rate=0.3)\n",
        "run_model(model, config)\n",
        "\n",
        "model = Conv1D_2C2D_model(input_shape, num_classes, l2_reg=0.01, dropout_rate=0.3)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SG8XrykLTCen",
        "colab_type": "text"
      },
      "source": [
        "Model from Chen's paper: uses only accelerometer data and overfits after 150 epochs, achieving no more than 75% accuracy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bz3lTkajWpc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150, # tuned\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "X_tr = X_train.copy()\n",
        "X_tr = X_tr[:,:,:3] # only take accelerometer data\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = X_te[:,:,:3]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_tr.shape[1], X_tr.shape[2])\n",
        "model = Conv1D_Chen_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHB5IimIT6ih",
        "colab_type": "text"
      },
      "source": [
        "Model from Moya-Rueda's paper: very bad performance (no more than 65% accuracy)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DtprVlP-puVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    #'optimizer': 'adam',\n",
        "    'optimizer': 'rmsprop',\n",
        "    'lr': 0.01,\n",
        "    'decay': 0.95,\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 100,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 25,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (X_train.shape[1], X_train.shape[2])\n",
        "model = Conv1D_Rueda_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEdaYoZUsy4",
        "colab_type": "text"
      },
      "source": [
        "Model from Bevilacqua's paper: achieves $\\approx$ 86% accuracy using 2D convolutions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "om1h7FO7dfhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = X_train.copy()\n",
        "X_tr = np.swapaxes(X_tr,1,2)\n",
        "X_tr = X_tr[...,None] # add last dimension\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = np.swapaxes(X_te,1,2)\n",
        "X_te = X_te[...,None]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_train.shape[2], X_train.shape[1], 1)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150, # tuned. visible overfit if continuing\n",
        "    'batch_size': 1024,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Bevilacqua_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)\n",
        "\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200, # tuned. stable\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Bevilacqua_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wf1mSywdVWUV",
        "colab_type": "text"
      },
      "source": [
        "Model from Ha's paper: achieves $\\approx$ 95% accuracy padding columns to separate signals data from different sensors.\n",
        "\n",
        "_One of the best CNN models, selected for report._ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eCEgx6oAXjYQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_tr = X_train.copy()\n",
        "X_tr = np.swapaxes(X_tr,1,2)\n",
        "# pad three 'cols' of zeros between each sensor data\n",
        "# > ax ay az 0 0 0 gx gy gz 0 0 0 mx my mz \n",
        "for index in [9,6,3]:\n",
        "    for rep in range(3):\n",
        "        X_tr = np.insert(X_tr, index, 0, axis=1)\n",
        "X_tr = X_tr[...,None] # add dim\n",
        "print(f'{X_train.shape} > {X_tr.shape}')\n",
        "\n",
        "X_te = X_test.copy()\n",
        "X_te = np.swapaxes(X_te,1,2)\n",
        "for index in [9,6,3]:\n",
        "    for rep in range(3):\n",
        "        X_te = np.insert(X_te, index, 0, axis=1)\n",
        "X_te = X_te[...,None]\n",
        "print(f'{X_test.shape} > {X_te.shape}')\n",
        "\n",
        "input_shape = (X_tr.shape[1], X_tr.shape[2],1)\n",
        "\n",
        "# no validation version\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150, # tuned at 100. slightly overfitting but stable val. accuracy\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': False,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': True\n",
        "}\n",
        "model = Conv2D_Ha_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)\n",
        "\n",
        "# validation version\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 200, # tuned. slightly overfitting but stable val. accuracy\n",
        "    'batch_size': 1024,\n",
        "    'use_validation': True,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "model = Conv2D_Ha_model(input_shape, num_classes)\n",
        "run_model(model, config, x_train = X_tr, x_test = X_te)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5sYZFUn9cJx",
        "colab_type": "text"
      },
      "source": [
        "### Recurrent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "q4t2SdFY-T2e",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 100,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[1], X_train.shape[2])\n",
        "model = TwoLSTM_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mp-G0Bw3bIU1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "input_shape = (None, X_train.shape[-1])\n",
        "model = TwoGRU_model(input_shape, num_classes)\n",
        "run_model(model, config)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_giGbFS-yFJ",
        "colab_type": "text"
      },
      "source": [
        "### CNN + LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ilyM0LMRh8W8",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "folds, fold_len = 4, 32 # 4*32 = 128\n",
        "\n",
        "input_shape = (folds, fold_len, X_train.shape[-1])\n",
        "model = CNN_LSTM_model(input_shape, num_classes)\n",
        "run_model(model, config,\n",
        "          x_train = X_train.reshape(X_train.shape[0], folds, fold_len, X_train.shape[-1]),\n",
        "          x_test = X_test.reshape(X_test.shape[0], folds, fold_len, X_test.shape[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlMGcNxUBFGU",
        "colab_type": "text"
      },
      "source": [
        "### AutoEncoders\n",
        "\n",
        "\n",
        "Workflow: train an AE > create one Input(input_shape) with input_shape of AE > add encoder part of AE > predict both train and test set  > attach network\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i3xCjGhz_Tz5",
        "colab_type": "text"
      },
      "source": [
        "#### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tRvYKRfspwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training of the AE\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 20,\n",
        "    'batch_size': 100,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "\n",
        "x_tr = x_tr.reshape(-1,1,x_tr.shape[1])\n",
        "x_te = x_te.reshape(-1,1,x_te.shape[1])\n",
        "y_tr = y_tr.reshape(-1,1,y_tr.shape[1])\n",
        "y_te = x_te.reshape(-1,1,y_te.shape[1])\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = CNN_AE_model(input_shape, x_tr.shape[2])\n",
        "\n",
        "folder = run_model(model, config, x_tr, y_tr, x_te, y_te, is_autoencoder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FGQuNTIWVFl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(f'{folder}/model-best-a.h5')\n",
        "\n",
        "# composing encoder network: start from input layer then stack only the first layers\n",
        "DL_input = Input(input_shape)\n",
        "DL_model = DL_input\n",
        "\n",
        "# keep only encoder part\n",
        "for layer in model.layers[:2]:\n",
        "    DL_model = layer(DL_model)\n",
        "DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "\n",
        "# disable gradient update\n",
        "for layer in DL_model.layers:\n",
        "    layer.trainable = False\n",
        "DL_model.summary()\n",
        "\n",
        "# using the encoder to predict data\n",
        "data_tr = DL_model.predict(x_tr, verbose = 1)\n",
        "data_tr = data_tr.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "data_te = DL_model.predict(x_te, verbose = 1)\n",
        "data_te = data_te.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
        "print(data_tr.shape)\n",
        "print(data_te.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndZMUBQwA4X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "# running CNN-LSTM model on AE's predictions\n",
        "folds, fold_len = 4, 32 # 4*32 = 128\n",
        "\n",
        "input_shape = (folds, fold_len, data_tr.shape[-1])\n",
        "model = CNN_LSTM_model(input_shape, num_classes)\n",
        "run_model(model, config,\n",
        "          x_train = data_tr.reshape(data_tr.shape[0], folds, fold_len, data_tr.shape[-1]),\n",
        "          x_test = data_te.reshape(data_te.shape[0], folds, fold_len, data_te.shape[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZnNaY-ky0gQ2",
        "colab_type": "text"
      },
      "source": [
        "#### SVM\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deSfhT5Q0gxW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import linear_model\n",
        "clf = linear_model.SGDClassifier(max_iter=10000, tol = 1e-5, n_jobs = -1)\n",
        "clf.fit(data_tr.reshape(X_train.shape[0],-1), Y_train_orig)\n",
        "clf.score(data_te.reshape(X_test.shape[0],-1), Y_test_orig)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXXlnPzEXZ6D",
        "colab_type": "text"
      },
      "source": [
        "#### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rNUc0ZdC9La6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training of the AE\n",
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 10,\n",
        "    'batch_size': 200,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "\n",
        "x_tr = x_tr.reshape(-1,1,x_tr.shape[1])\n",
        "x_te = x_te.reshape(-1,1,x_te.shape[1])\n",
        "y_tr = y_tr.reshape(-1,1,y_tr.shape[1])\n",
        "y_te = x_te.reshape(-1,1,y_te.shape[1])\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = LSTM_AE_model(input_shape, x_tr.shape[2])\n",
        "\n",
        "folder = run_model(model, config, x_tr, y_tr, x_te, y_te, is_autoencoder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoYBd6a9WVxr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(f'{folder}/model-best-a.h5')\n",
        "\n",
        "# composing encoder network: start from input layer then stack only the first layers\n",
        "DL_input = Input(input_shape)\n",
        "DL_model = DL_input\n",
        "\n",
        "# keep only encoder part\n",
        "for layer in model.layers[:2]:\n",
        "    DL_model = layer(DL_model)\n",
        "DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "\n",
        "# disable gradient update\n",
        "for layer in DL_model.layers:\n",
        "    layer.trainable = False\n",
        "DL_model.summary()\n",
        "\n",
        "# using the encoder to predict data\n",
        "data_tr = DL_model.predict(x_tr, verbose = 1)\n",
        "data_tr = data_tr.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "data_te = DL_model.predict(x_te, verbose = 1)\n",
        "data_te = data_te.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
        "print(data_tr.shape)\n",
        "print(data_te.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAW98CZjAmzK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "# running CNN-LSTM model on AE's predictions\n",
        "folds, fold_len = 4, 32 # 4*32 = 128\n",
        "\n",
        "input_shape = (folds, fold_len, data_tr.shape[-1])\n",
        "model = CNN_LSTM_model(input_shape, num_classes)\n",
        "run_model(model, config,\n",
        "          x_train = data_tr.reshape(data_tr.shape[0], folds, fold_len, data_tr.shape[-1]),\n",
        "          x_test = data_te.reshape(data_te.shape[0], folds, fold_len, data_te.shape[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjdORZgzAYwc",
        "colab_type": "text"
      },
      "source": [
        "#### CNN-LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NwrncMI213a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'mse',\n",
        "    'epochs': 10,\n",
        "    'batch_size': 100,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "x_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "x_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "y_tr = X_train.reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "y_te = X_test.reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "\n",
        "x_tr = x_tr.reshape(-1,1,x_tr.shape[1])\n",
        "x_te = x_te.reshape(-1,1,x_te.shape[1])\n",
        "y_tr = y_tr.reshape(-1,1,y_tr.shape[1])\n",
        "y_te = x_te.reshape(-1,1,y_te.shape[1])\n",
        "\n",
        "input_shape = (x_tr.shape[1], x_tr.shape[2])\n",
        "model = CNN_LSTM_AE_model(input_shape, x_tr.shape[2])\n",
        "\n",
        "folder = run_model(model, config, x_tr, y_tr, x_te, y_te, is_autoencoder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLaZy7enWWSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = load_model(f'{folder}/model-best-a.h5')\n",
        "\n",
        "# composing encoder network: start from input layer then stack only the first layers\n",
        "DL_input = Input(input_shape)\n",
        "DL_model = DL_input\n",
        "\n",
        "# keep only encoder part\n",
        "for layer in model.layers[:2]:\n",
        "    DL_model = layer(DL_model)\n",
        "DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "\n",
        "# disable gradient update\n",
        "for layer in DL_model.layers:\n",
        "    layer.trainable = False\n",
        "DL_model.summary()\n",
        "\n",
        "# using the encoder to predict data\n",
        "data_tr = DL_model.predict(x_tr, verbose = 1)\n",
        "data_tr = data_tr.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "data_te = DL_model.predict(x_te, verbose = 1)\n",
        "data_te = data_te.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
        "print(data_tr.shape)\n",
        "print(data_te.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CvJ0Nvc4Wl23",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    'optimizer': 'adam',\n",
        "    'loss': 'categorical_crossentropy',\n",
        "    'epochs': 150,\n",
        "    'batch_size': 300,\n",
        "    'shuffle': True,\n",
        "    'lr_step': 0,\n",
        "    'early_stop': 0,\n",
        "    'use_validation':False,\n",
        "    'tg': False\n",
        "}\n",
        "\n",
        "# running CNN-LSTM model on AE's predictions\n",
        "folds, fold_len = 4, 32 # 4*32 = 128\n",
        "\n",
        "input_shape = (folds, fold_len, data_tr.shape[-1])\n",
        "model = CNN_LSTM_model(input_shape, num_classes)\n",
        "run_model(model, config,\n",
        "          x_train = data_tr.reshape(data_tr.shape[0], folds, fold_len, data_tr.shape[-1]),\n",
        "          x_test = data_te.reshape(data_te.shape[0], folds, fold_len, data_te.shape[-1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVS0AwNToEGJ",
        "colab_type": "text"
      },
      "source": [
        "## Tests of accepted networks\n",
        "\n",
        "We have\n",
        "\n",
        "- 12 datasets to test (three not normalized, not augmented, adasyn, handcrafted) for both sensor and body reference frames.\n",
        "- 5 selected networks from previous sections: Conv1D-2C1D, Conv2D-Ha, TwoLSTM, TwoGRU, CNN-LSTM.\n",
        "\n",
        "A for loop trains all the standard models and the mixed model on each dataset.\n",
        "\n",
        "Another for loop trains every autoencoder and on their predictions run all the previous networks _but_ Conv2D-Ha. Also SVMs are employed here.\n",
        "\n",
        "**WARNING**: in the report, dataset abbreviations are different due to space constraints and coherence:\n",
        "- `SADANN` becomes `SADAn`\n",
        "- `SAHCNN` becomes `SAHCn`\n",
        "- `SNOR` becomes `SFRA` \n",
        "- `SFRA` becomes `SFRAn` \n",
        "\n",
        "Same for body-referenced datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dJ-zHrf5hTCX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_names = {\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-rot-per-norm.h5' : 'BAHC',   # manual aug\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-rot-per.h5' : 'BAHCNN',\n",
        "    'ARS-train-test-body-framed-aug-onlytrain-norm.h5' : 'BADA',           # adasyn\n",
        "    'ARS-train-test-body-framed-aug-onlytrain.h5' : 'BADANN',\n",
        "    'ARS-train-test-body-framed-norm.h5' : 'BNOR',                         # not augmented\n",
        "    'ARS-train-test-body-framed.h5' : 'BFRA',                              # not normalized  \n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-rot-per-norm.h5' : 'SAHC',\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-rot-per.h5' : 'SAHCNN',    # SAHCn in report\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain-norm.h5' : 'SADA',\n",
        "    'ARS-train-test-sensor-framed-aug-onlytrain.h5' : 'SADANN',            # SADAn in report\n",
        "    'ARS-train-test-sensor-framed-norm.h5' : 'SNOR',                       # SFRA  in report\n",
        "    'ARS-train-test-sensor-framed.h5' : 'SFRA'                             # SFRAn in report\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDgMx3lcME0Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# the input datasets to these methods changes in loops\n",
        "def get_Ha_datasets(X_train, X_test):\n",
        "    \"\"\"Dataset reshape for Ha model.\"\"\"\n",
        "    X_trainHa = X_train.copy()\n",
        "    X_trainHa = np.swapaxes(X_trainHa,1,2)\n",
        "    for index in [9,6,3]:\n",
        "        for rep in range(3): # pad three 'cols' of zeros between each sensor data\n",
        "            X_trainHa = np.insert(X_trainHa, index, 0, axis=1)\n",
        "    X_trainHa = X_trainHa[...,None] # add dim\n",
        "\n",
        "    X_testHa = X_test.copy()\n",
        "    X_testHa = np.swapaxes(X_testHa,1,2)\n",
        "    for index in [9,6,3]:\n",
        "        for rep in range(3):\n",
        "            X_testHa = np.insert(X_testHa, index, 0, axis=1)\n",
        "    X_testHa = X_testHa[...,None]\n",
        "    \n",
        "    return X_trainHa, X_testHa\n",
        "    \n",
        "def get_CNN_LSTM_datasets(X_train, X_test):\n",
        "    \"\"\"Dataset reshape for CNN-LSTM model.\"\"\"\n",
        "    folds, fold_len = 4, 32 # 4*32 = 128\n",
        "    X_train_CL = X_train.copy()\n",
        "    X_train_CL = X_train_CL.reshape(X_train.shape[0], folds, fold_len, X_train.shape[-1])\n",
        "\n",
        "    X_test_CL = X_test.copy()\n",
        "    X_test_CL = X_test_CL.reshape(X_test.shape[0], folds, fold_len, X_test.shape[-1])\n",
        "\n",
        "    return X_train_CL, X_test_CL"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jvm_-y_3hzMe",
        "colab_type": "text"
      },
      "source": [
        "### Standard and mixed models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBNL-u2FoIgB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in dataset_names:\n",
        "    # load standard dataset\n",
        "    with h5py.File(f\"dataset/{dataset}\",'r') as h5f:\n",
        "        X_train = h5f['X_train'][:] # IMU data\n",
        "        X_test  = h5f['X_test'][:]  # activities\n",
        "        Y_train = h5f['Y_train'][:]\n",
        "        Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "    num_classes = 7\n",
        "    num_data = len(X_train)\n",
        "    print(\"X_train shape: \" + str(X_train.shape))\n",
        "    print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "    print(\"X_test shape:  \" + str(X_test.shape))\n",
        "    print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "    Y_train_orig = Y_train.copy()\n",
        "    Y_test_orig  = Y_test.copy()\n",
        "    Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "    Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8) \n",
        "\n",
        "    \n",
        "    # --- dataset reshape for autoencoders\n",
        "    X_train_AE = X_train.copy().reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "    X_test_AE = X_test.copy().reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "    \n",
        "    X_train_AE = X_train_AE.reshape(-1,1,X_train_AE.shape[1])\n",
        "    X_test_AE = X_test_AE.reshape(-1,1,X_test_AE.shape[1])\n",
        "    \n",
        "    Y_train_AE = X_train_AE.copy() # maybe useless\n",
        "    Y_test_AE = X_test_AE.copy()\n",
        "    # ---\n",
        "    \n",
        "    # --- other datasets reshape\n",
        "    X_train_Ha, X_test_Ha = get_Ha_datasets(X_train, X_test)\n",
        "    X_train_CL, X_test_CL = get_CNN_LSTM_datasets(X_train, X_test)\n",
        "    folds, fold_len = 4, 32 # 4*32 = 128\n",
        "    # ---\n",
        "    \n",
        "\n",
        "    tests_list = [\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 400,\n",
        "                'batch_size': 32,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': Conv1D_2C1D_model( (X_train.shape[1], X_train.shape[2]), num_classes, dropout_rate=0.3)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 150,\n",
        "                'batch_size': 1024,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False  \n",
        "            },\n",
        "            'model': Conv2D_Ha_model( (X_train_Ha.shape[1], X_train_Ha.shape[2], 1), num_classes),\n",
        "            'X_train': X_train_Ha,\n",
        "            'X_test': X_test_Ha,\n",
        "            'custom_ds': 'Ha'\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 100,\n",
        "                'batch_size': 200,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': TwoLSTM_model( (None, X_train.shape[1], X_train.shape[2]), num_classes)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 150,\n",
        "                'batch_size': 200,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': TwoGRU_model( (None, X_train.shape[-1]), num_classes)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 150,\n",
        "                'batch_size': 300,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': CNN_LSTM_model( (folds, fold_len, X_train.shape[-1]), num_classes),\n",
        "            'X_train': X_train_CL,\n",
        "            'X_test': X_test_CL,\n",
        "            'custom_ds': 'CL'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # the loop\n",
        "    for dic in tests_list:\n",
        "        clear_output()\n",
        "        model = dic['model']\n",
        "        print(f\"Doing {model.name}\")\n",
        "\n",
        "        if 'X_train' in dic or 'X_test' in dic:\n",
        "            if 'Y_train' in dic or 'Y_test' in dic:\n",
        "                # model uses custom Xs and Ys\n",
        "                run_model(model, dic['config'], x_train=dic['X_train'], x_test=dic['X_test'], \\\n",
        "                          y_train=dic['Y_train'], y_test=dic['Y_test'], y_test_orig=dic['Y_test_orig'], folder_suffix=dataset_names[dataset])\n",
        "            else:\n",
        "                # model uses custom Xs\n",
        "                run_model(model, dic['config'], x_train=dic['X_train'], x_test=dic['X_test'], \\\n",
        "                          y_train=Y_train, y_test=Y_test, y_test_orig=Y_test_orig, folder_suffix=dataset_names[dataset])\n",
        "        else:\n",
        "            # model uses standard dataset\n",
        "            run_model(model, dic['config'], x_train=X_train, x_test=X_test, \\\n",
        "                      y_train=Y_train, y_test=Y_test, y_test_orig=Y_test_orig, folder_suffix=dataset_names[dataset])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2meCJKqUiZfT",
        "colab_type": "text"
      },
      "source": [
        "### AutoEncoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KNQGUcJMhrAZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for dataset in dataset_names:\n",
        "    # load standard dataset\n",
        "    with h5py.File(f\"dataset/{dataset}\",'r') as h5f:\n",
        "        X_train = h5f['X_train'][:] # IMU data\n",
        "        X_test  = h5f['X_test'][:]  # activities\n",
        "        Y_train = h5f['Y_train'][:]\n",
        "        Y_test  = h5f['Y_test'][:]\n",
        "\n",
        "    num_classes = 7\n",
        "    num_data = len(X_train)\n",
        "    print(\"X_train shape: \" + str(X_train.shape))\n",
        "    print(\"Y_train shape: \" + str(Y_train.shape))\n",
        "    print(\"X_test shape:  \" + str(X_test.shape))\n",
        "    print(\"Y_test shape:  \" + str(Y_test.shape))\n",
        "\n",
        "    Y_train_orig = Y_train.copy()\n",
        "    Y_test_orig  = Y_test.copy()\n",
        "    Y_train = to_categorical(Y_train, num_classes=num_classes, dtype=np.uint8)\n",
        "    Y_test  = to_categorical(Y_test,  num_classes=num_classes, dtype=np.uint8) \n",
        "\n",
        "    \n",
        "    # --- dataset reshape for autoencoders\n",
        "    X_train_AE = X_train.copy().reshape((X_train.shape[0]*X_train.shape[1], -1))\n",
        "    X_test_AE = X_test.copy().reshape((X_test.shape[0]*X_test.shape[1], -1))\n",
        "    \n",
        "    X_train_AE = X_train_AE.reshape(-1,1,X_train_AE.shape[1])\n",
        "    X_test_AE = X_test_AE.reshape(-1,1,X_test_AE.shape[1])\n",
        "    \n",
        "    Y_train_AE = X_train_AE.copy() # maybe useless\n",
        "    Y_test_AE = X_test_AE.copy()\n",
        "    # ---\n",
        "    \n",
        "    # --- other datasets reshape\n",
        "    X_train_CL, X_test_CL = get_CNN_LSTM_datasets(X_train, X_test)\n",
        "    folds, fold_len = 4, 32 # 4*32 = 128\n",
        "    # ---\n",
        "    \n",
        "\n",
        "    tests_ae_list = [ # autoencoders to train\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'mse',\n",
        "                'epochs': 20,\n",
        "                'batch_size': 100,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': CNN_AE_model( (X_train_AE.shape[1], X_train_AE.shape[2]), X_train_AE.shape[2]),\n",
        "            'X_train': X_train_AE,\n",
        "            'X_test': X_test_AE,\n",
        "            'Y_train': Y_train_AE, # or maybe X directly\n",
        "            'Y_test': Y_test_AE\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'mse',\n",
        "                'epochs': 10,\n",
        "                'batch_size': 200,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': LSTM_AE_model( (X_train_AE.shape[1], X_train_AE.shape[2]), X_train_AE.shape[2]),\n",
        "            'X_train': X_train_AE,\n",
        "            'X_test': X_test_AE,\n",
        "            'Y_train': Y_train_AE,\n",
        "            'Y_test': Y_test_AE\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'mse',\n",
        "                'epochs': 10,\n",
        "                'batch_size': 100,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': CNN_LSTM_AE_model( (X_train_AE.shape[1], X_train_AE.shape[2]), X_train_AE.shape[2]),\n",
        "            'X_train': X_train_AE,\n",
        "            'X_test': X_test_AE,\n",
        "            'Y_train': Y_train_AE,\n",
        "            'Y_test': Y_test_AE            \n",
        "        }        \n",
        "    ]\n",
        "\n",
        "    \n",
        "    tests_list_64 = [ # networks to attach to autoencoders\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 400,\n",
        "                'batch_size': 32,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': Conv1D_2C1D_model( (X_train.shape[1], 64), num_classes, dropout_rate=0.3)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 100,\n",
        "                'batch_size': 200,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': TwoLSTM_model( (None, X_train.shape[1], 64), num_classes)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 150,\n",
        "                'batch_size': 200,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': TwoGRU_model( (None, 64), num_classes)\n",
        "        },\n",
        "        {\n",
        "            'config': {\n",
        "                'optimizer': 'adam',\n",
        "                'loss': 'categorical_crossentropy',\n",
        "                'epochs': 150,\n",
        "                'batch_size': 300,\n",
        "                'use_validation': False,\n",
        "                'shuffle': True,\n",
        "                'lr_step': 0,\n",
        "                'early_stop': 0,\n",
        "                'tg': False\n",
        "            },\n",
        "            'model': CNN_LSTM_model( (folds, fold_len, 64), num_classes),\n",
        "            'X_train': X_train_CL,\n",
        "            'X_test': X_test_CL,\n",
        "            #'custom_ds': 'CL'\n",
        "        }\n",
        "    ]\n",
        "    \n",
        "    \n",
        "    \n",
        "    # the AE training loop\n",
        "    folders = []\n",
        "    for dic in tests_ae_list:\n",
        "        clear_output()\n",
        "        model = dic['model']\n",
        "        print(f\"Doing {model.name}\")\n",
        "        \n",
        "        # AE training\n",
        "        folder = run_model(model, dic['config'], x_train=dic['X_train'], x_test=dic['X_test'], \\\n",
        "                          y_train=dic['Y_train'], y_test=dic['Y_test'], is_autoencoder=True, folder_suffix=f\"{dataset_names[dataset]}_train\")\n",
        "        folders.append(folder)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # the networks part loop\n",
        "    for folder in folders:    \n",
        "        model_AE = load_model(f'{folder}/model-best-a.h5')\n",
        "\n",
        "        # composing encoder network\n",
        "        input_shape = (X_train_AE.shape[1], X_train_AE.shape[2])\n",
        "        DL_input = Input(input_shape)\n",
        "        DL_model = DL_input\n",
        "\n",
        "        # keep only encoder part\n",
        "        for layer in model_AE.layers[:2]:\n",
        "            DL_model = layer(DL_model)\n",
        "        DL_model = Model(inputs=DL_input, outputs=DL_model)\n",
        "\n",
        "        # disable gradient update\n",
        "        for layer in DL_model.layers:\n",
        "            layer.trainable = False\n",
        "\n",
        "        # using the encoder to predict data\n",
        "        print('Predictions')\n",
        "        data_tr = DL_model.predict(dic['X_train'], verbose = 1)\n",
        "        X_train_AE_predicted = data_tr.reshape(X_train.shape[0], X_train.shape[1], -1)\n",
        "        data_te = DL_model.predict(dic['X_test'], verbose = 1)\n",
        "        X_test_AE_predicted = data_te.reshape(X_test.shape[0], X_test.shape[1], -1)\n",
        "        \n",
        "        # recomputing reshapes for some networks\n",
        "        X_train_Ha, X_test_Ha = get_Ha_datasets(X_train_AE_predicted, X_test_AE_predicted)\n",
        "        X_train_CL, X_test_CL = get_CNN_LSTM_datasets(X_train_AE_predicted, X_test_AE_predicted)\n",
        "        \n",
        "        # the networks training and evaluation loop\n",
        "        for dic2 in tests_list_64:\n",
        "            clear_output()\n",
        "            model = dic2['model']\n",
        "            print(f\"Doing {model.name}\")\n",
        "\n",
        "            if 'X_train' in dic2 or 'X_test' in dic2:\n",
        "                # model uses custom Xs\n",
        "                if 'custom_ds' in dic2 and 'CL' in dic2['custom_ds']:\n",
        "                    run_model(model, dic2['config'], x_train=X_train_CL, x_test=X_test_CL, \\\n",
        "                              y_train=Y_train, y_test=Y_test, y_test_orig=Y_test_orig, folder_suffix=f\"{dataset_names[dataset]}_{modelAE.name}\")\n",
        "                else:\n",
        "                    # model uses standard dataset\n",
        "                    run_model(model, dic2['config'], x_train=X_train_AE_predicted, x_test=X_test_AE_predicted, \\\n",
        "                        y_train=Y_train, y_test=Y_test, y_test_orig=Y_test_orig, folder_suffix=f\"{dataset_names[dataset]}_{modelAE.name}\")\n",
        "            else:\n",
        "                # model uses standard dataset\n",
        "                run_model(model, dic2['config'], x_train=X_train_AE_predicted, x_test=X_test_AE_predicted, \\\n",
        "                      y_train=Y_train, y_test=Y_test, y_test_orig=Y_test_orig, folder_suffix=f\"{dataset_names[dataset]}_{modelAE.name}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BX4VjdNYYTtk",
        "colab_type": "text"
      },
      "source": [
        "## Extra - LSTM test in pure Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uvv75Z2iY-q8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model definition\n",
        "\n",
        "features = 32 # number of hidden layer's features\n",
        "\n",
        "#batch = 1500 # TODO unused vars\n",
        "#n_iters = 300\n",
        "#tot_iters = Y_train.shape[0] * n_iters\n",
        "#disp_iter = 1000\n",
        "\n",
        "w = {\n",
        "    'h' : tf.Variable(tf.random_normal([X_train.shape[2], features])),\n",
        "    'o' : tf.Variable(tf.random_normal([features, Y_train.shape[1]], mean=1.0))\n",
        "}\n",
        "b = {\n",
        "    'h' : tf.Variable(tf.random_normal([features])),\n",
        "    'o' : tf.Variable(tf.random_normal([Y_train.shape[1]]))\n",
        "}\n",
        "\n",
        "def LSTM(X, w, b):\n",
        "    # input processing\n",
        "    X = tf.transpose(X,[1,0,2])         # (batch_size, steps, input)\n",
        "    X = tf.reshape(X, [-1, X.shape[2]]) # (steps*batch, n_initial_\"features\")\n",
        "\n",
        "    X = tf.nn.relu(tf.matmul(X, w['h']) + b['h'])\n",
        "    X = tf.split(X, X_train.shape[1])\n",
        "    \n",
        "    # model\n",
        "    l_1 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)\n",
        "    l_2 = tf.contrib.rnn.BasicLSTMCell(features, forget_bias=1.0, state_is_tuple=True)    \n",
        "    lstm = tf.contrib.rnn.MultiRNNCell([l_1,l_2], state_is_tuple=True)    \n",
        "    \n",
        "    # output\n",
        "    out, state = tf.contrib.rnn.static_rnn(lstm, X, dtype=tf.float32)\n",
        "    \n",
        "    return tf.matmul(out[-1], w['o']) + b['o']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNqoOJlQZAjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# define a dataset object on input\n",
        "ds_obj = tf.data.Dataset.from_tensor_slices((X_train.astype(np.float32), Y_train.astype(np.float32))).repeat().batch(300)\n",
        "iter = ds_obj.make_one_shot_iterator()\n",
        "x, y = iter.get_next()\n",
        "\n",
        "prediction = LSTM(x, w, b)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(tf.argmax(prediction,1), tf.argmax(y,1)),tf.float32))\n",
        "\n",
        "# losses, optimizer\n",
        "lr = 0.0025\n",
        "lambda_l = 0.0015\n",
        "\n",
        "l2_norm = lambda_l * sum(tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables())\n",
        "softmax_cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=prediction)) + l2_norm\n",
        "adam = tf.train.AdamOptimizer(learning_rate=lr).minimize(softmax_cost)\n",
        "\n",
        "# run training\n",
        "test_log  = {'loss':[], 'acc':[]}\n",
        "train_log = {'loss':[], 'acc':[]}\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    for i in range(1000): #epochs\n",
        "        _, l, a = sess.run([adam, softmax_cost, accuracy])\n",
        "        train_log['loss'].append(l)\n",
        "        train_log['acc'].append(a)\n",
        "        \n",
        "        l,a = sess.run([softmax_cost, accuracy], feed_dict={x:X_test.astype(np.float32), y:Y_test.astype(np.float32)})\n",
        "        test_log['loss'].append(l)\n",
        "        test_log['acc'].append(a)\n",
        "        #print(\"PERFORMANCE ON TEST SET: \" + \\\n",
        "        #      \"Batch Loss = {}\".format(l) + \\\n",
        "        #      \", Accuracy = {}\".format(a))\n",
        "print('Reached {}'.format(max(test_log['acc'])))\n",
        "\n",
        "# save stuff and plots\n",
        "out_folder = os.path.join('output', datetime.now(pytz.timezone('Europe/Rome')).strftime('%y%m%d-%H%M%S')+'_LSTM-TF')\n",
        "if not os.path.exists(out_folder):\n",
        "    os.mkdir(out_folder)\n",
        "\n",
        "with open(os.path.join(out_folder, 'history.json'),'w') as hfile:\n",
        "    json.dump({'training':train_log, 'validation':test_log}, hfile, indent=2)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['loss'], label='Training')\n",
        "plt.plot( test_log['loss'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-loss')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "plt.close()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(train_log['acc'], label='Training')\n",
        "plt.plot( test_log['acc'], label='Validation')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.tight_layout()\n",
        "fname = os.path.join(out_folder, 'plot-accuracy')\n",
        "plt.savefig(fname+'.png')\n",
        "plt.savefig(fname+'.pdf', format='pdf')\n",
        "plt.close()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
